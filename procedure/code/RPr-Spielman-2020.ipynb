{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ccb0c41-8caa-4d84-835d-d297ff776126",
   "metadata": {},
   "source": [
    "# Reproduction of Spielman et al.’s 2020 Evaluation of the Social Vulnerability Index\n",
    "### Authors\n",
    "\n",
    "- Liam Smith\\*, lwsmith@middlebury.edu, @Liam-W-Smith, Middlebury College\n",
    "- Joseph Holler, josephh@middlebury.edu , @josephholler, [ORCID link](https://orcid.org/0000-0002-2381-2699), Middlebury College\n",
    "\n",
    "\\* Corresponding author and creator\n",
    "\n",
    "Version 1.1 | Created 2023-07-12 | Last Updated 2023-07-21  \n",
    "\n",
    "### Abstract\n",
    "\n",
    "This study is a *reproduction* of:\n",
    "\n",
    "> Spielman, S. E., Tuccillo, J., Folch, D. C., Schweikert, A., Davies, R., Wood, N., & Tate, E. (2020). Evaluating Social Vulnerability Indicators: Criteria and their Application to the Social Vulnerability Index. Natural Hazards, 100(1), 417–436. https://doi.org/10.1007/s11069-019-03820-z\n",
    "\n",
    "The Spielman et al. (2020) paper is in turn a replication of:\n",
    "\n",
    "> Cutter, S. L., Boruff, B. J., & Shirley, W. L. (2003). Social vulnerability to environmental hazards. Social Science Quarterly, 84(2), 242–261. https://doi.org/10.1111/1540-6237.8402002\n",
    "\n",
    "Spielman et al. (2020) developed methods to evaluate the internal consistency and construct validity of the Cutter, Boruff and Shirley (2003) Social Vulnerability Index (SoVI).\n",
    "First, they reproduce a national SoVI model and validate it against an SPSS procedure provided by the original research group (Hazards Vulnerability Research Institute at University of South Carolina).\n",
    "The original SoVI uses 42 independent z-score normalized variables from the U.S. Census, reduces the data to factors using Principal Components Analysis, selects the first eleven factors, inverts factors with inverse relationships to social vulnerability, and sums the factors together to produce a SoVI score.\n",
    "The reproduced SoVI model was slightly different than the original model due to changes in U.S. Census data, using only 28 variables.\n",
    "\n",
    "Spielman et al. modify the geographic extent of the SoVI calculation by calculating SoVI on a national extent, and then recalculating for each of ten Federal Emergency Management Agency (FEMA) regions, and again for a single state or cluster of states within each of the ten regions, resulting in 21 total indices.\n",
    "Internal consistency is assessed by calculating the spearman rank correlation coefficient of the SoVI score for counties in the state model compared to the FEMA region model and national model.\n",
    "Construct validity is assessed by summing the loadings for each input variable across the PCA factors in each model and calculating the variables sign (positive/negative) and the rank of the variable's total loading compared to the other variables.\n",
    "These signs and ranks are summarized across all 21 versions of the SoVI model with regard to the number of times the sign is different from the national model and the distributions of ranks.\n",
    "\n",
    "In this reproduction study, we attempt to reproduce identical SoVI model outputs for each of the 21 models in the original study.\n",
    "We will compare these outputs to data files in Spielman et al.'s GitHub repository.\n",
    "We will also attempt to reproduce identical results of internal consistency analysis (figure 1 and table 2) and construct validity analysis (figure 2) from Spielman et al.'s paper.\n",
    "We succeed in reproducing identical SoVI model outputs, but find slight discrepancies in our figures and tables.\n",
    "\n",
    "The code in this Jupyter notebook report is adapted from Spielman et al.'s GitHub repository.\n",
    "The original study states the intended open source permissions in the acknowledgements: \"To facilitate advances to current practice and to allow replication of our results, all of the code and data used in this analysis is open source and available at (https://github.com/geoss/sovi-validity).\n",
    "Funding was provided by the US National Science Foundation (Award No. 1333271) and the U.S. Geological Survey Land Change Science Program.\"\n",
    "\n",
    "### Keywords\n",
    "\n",
    "Social vulnerability, social indicators, Principal Component Analysis, reproducibility\n",
    "\n",
    "## Study design\n",
    "\n",
    "We computationally reproduce Spielman et al.'s original work using the code provided in their Github repository (https://github.com/geoss/sovi-validity), adapting their code to run in an updated Python environment using current package versions.\n",
    "We make all of our work available online using the HEGSRR [reproducible research compendium template](https://github.com/HEGSRR/HEGSRR-Template).\n",
    "\n",
    "The original paper was a replication study testing the sensitivity of SoVI to changes in geographic extent.\n",
    "Spielman et al. addressed the following hypotheses in their work:\n",
    "\n",
    "> OR-H1: SoVI is internally inconsistent.\n",
    "\n",
    "To address this hypothesis, Spielman et al. illustrated that SoVI is not robust to changes in geographic extent by calculating SoVI scores for ten selected states or groups of states on three geographic extents: national, FEMA region, and state(s).\n",
    "The counties within the state(s) of interest were then selected and ranked according to their SoVI score.\n",
    "OR-H1 was tested by calculating Spearman's rank correlation between the state and FEMA region models and between the state and national models.\n",
    "\n",
    "> OR-H2: SoVI is theoretically inconsistent.\n",
    "\n",
    "To address this hypothesis, Spielman et al. used the same SoVI models as described under OR-H1.\n",
    "For each model, they summed all of the PCA factors together to determine the net influence of each variable in each model.\n",
    "Then they recorded the signs of each variable and calculated the number of deviations of the ten state and FEMA region models from the national model.\n",
    "They also ranked the variables by absolute value for each model and calculated summary statistics regarding the distribution of ranks for each variable amongst all models.\n",
    "Spielman et al. did not use a particular statistical method to test OR-H2, but illustrated substantial disagreements between variable rankings and signs amongst the 21 SoVI models.\n",
    "\n",
    "For our reproduction, we address the following three hypotheses:\n",
    "\n",
    "> RPr-H1: Reproduced SoVI model scores and other reproduced output datasets are not identical to the original study SoVI model scores and provided output datasets for each of the 21 SoVI models.\n",
    "\n",
    "> RPr-H2: Reproduced figures and tables for the internal consistency analysis are not identical to the figures and tables (figure 1 and table 2) of the original study.\n",
    "\n",
    "> RPr-H3: For the theoretical consistency analysis, reproduced direction reversals and min, average, and max SoVI rank value of 28 demographic variables are not identical to the direction reversals and min, average, and max SoVI rank values shown in figure 2 of the original study.\n",
    "\n",
    "\n",
    "We answer these questions by working through Spielman et al.'s code line by line in an updated python coding environment.\n",
    "To improve reproducibility, we reorganize Spielman's repository into the Template for Reproducible and Replicable Research in Human-Environment and Geographical Sciences (doi:10.17605/OSF.IO/W29MQ) and use one Jupyter notebook for the reproduction report and code.\n",
    "We catalogue barriers to reproducibility and make improvements wherever possible.\n",
    "\n",
    "Disclaimer: we worked with the data and code before writing this report, so there is no pre-registration of the analysis plan.\n",
    "We originally intended to publish only a replication of this study; we did not anticipate publishing a reproduction until we spent some time working with the code.\n",
    "\n",
    "#### Spatio-temporal metadata\n",
    "\n",
    "- `Spatial Coverage`: United States, excluding Puerto Rico\n",
    "- `Spatial Resolution`: Counties and county equivalents\n",
    "- `Spatial Reference System`: EPSG:4269\n",
    "- `Temporal Coverage`: 2008 - 2012 (data is the 2012 5-year ACS)\n",
    "- `Temporal Resolution`: One-time measurement, does not address change over time\n",
    "\n",
    "## Materials and procedure\n",
    "\n",
    "### Computational environment\n",
    "\n",
    "Currently, we are using a [2020 MacBook Pro](https://support.apple.com/kb/SP818?locale=en_US) running on macOS Ventura 13.3.1.\n",
    "We anticipate collaborators working on the project from different computers and different operating systems, and we seek to containerize the project so that scripts can be run on many different machines.\n",
    "\n",
    "The original study used Python for their analysis, so we reproduce their results in Python, using a containerized conda environment.\n",
    "This environment consists of Python 3.9.16 and the software packages listed in [requirements.txt](../environment/requirements.txt)\n",
    "\n",
    "To set up this environment on another machine, one should install the correct version of Python and run the cell below to install the correct package versions.\n",
    "If a user wishes to create a self-contained environment, they should explore [venv](https://docs.python.org/3/library/venv.html), [conda](https://docs.conda.io/en/latest/), or [pipenv](https://pipenv.pypa.io/en/latest/) virtual environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8368471f-7098-4d60-bbc9-353e50def1d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.13\n",
      "Requirement already satisfied: pandas==1.4.4 in /Users/liamsmith/opt/anaconda3/lib/python3.9/site-packages (from -r ../environment/requirements.txt (line 1)) (1.4.4)\n",
      "Requirement already satisfied: geopandas==0.13.2 in /Users/liamsmith/opt/anaconda3/lib/python3.9/site-packages (from -r ../environment/requirements.txt (line 2)) (0.13.2)\n",
      "Requirement already satisfied: pyhere==1.0.0 in /Users/liamsmith/opt/anaconda3/lib/python3.9/site-packages (from -r ../environment/requirements.txt (line 3)) (1.0.0)\n",
      "Requirement already satisfied: scipy==1.10.1 in /Users/liamsmith/opt/anaconda3/lib/python3.9/site-packages (from -r ../environment/requirements.txt (line 4)) (1.10.1)\n",
      "Requirement already satisfied: numpy==1.21.5 in /Users/liamsmith/opt/anaconda3/lib/python3.9/site-packages (from -r ../environment/requirements.txt (line 5)) (1.21.5)\n",
      "Requirement already satisfied: MDP==3.5 in /Users/liamsmith/opt/anaconda3/lib/python3.9/site-packages (from -r ../environment/requirements.txt (line 6)) (3.5)\n",
      "Requirement already satisfied: pygris==0.1.5 in /Users/liamsmith/opt/anaconda3/lib/python3.9/site-packages (from -r ../environment/requirements.txt (line 7)) (0.1.5)\n",
      "Requirement already satisfied: libpysal==4.7.0 in /Users/liamsmith/opt/anaconda3/lib/python3.9/site-packages (from -r ../environment/requirements.txt (line 8)) (4.7.0)\n",
      "Requirement already satisfied: lxml==4.9.3 in /Users/liamsmith/opt/anaconda3/lib/python3.9/site-packages (from -r ../environment/requirements.txt (line 9)) (4.9.3)\n",
      "Requirement already satisfied: tabulate==0.9.0 in /Users/liamsmith/opt/anaconda3/lib/python3.9/site-packages (from -r ../environment/requirements.txt (line 10)) (0.9.0)\n",
      "Requirement already satisfied: matplotlib==3.7.1 in /Users/liamsmith/opt/anaconda3/lib/python3.9/site-packages (from -r ../environment/requirements.txt (line 11)) (3.7.1)\n",
      "Requirement already satisfied: mapclassify==2.5.0 in /Users/liamsmith/opt/anaconda3/lib/python3.9/site-packages (from -r ../environment/requirements.txt (line 12)) (2.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/liamsmith/opt/anaconda3/lib/python3.9/site-packages (from pandas==1.4.4->-r ../environment/requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/liamsmith/opt/anaconda3/lib/python3.9/site-packages (from pandas==1.4.4->-r ../environment/requirements.txt (line 1)) (2022.1)\n",
      "Requirement already satisfied: pyproj>=3.0.1 in /Users/liamsmith/opt/anaconda3/lib/python3.9/site-packages (from geopandas==0.13.2->-r ../environment/requirements.txt (line 2)) (3.6.0)\n",
      "Requirement already satisfied: packaging in /Users/liamsmith/opt/anaconda3/lib/python3.9/site-packages (from geopandas==0.13.2->-r ../environment/requirements.txt (line 2)) (21.3)\n",
      "Requirement already satisfied: shapely>=1.7.1 in /Users/liamsmith/opt/anaconda3/lib/python3.9/site-packages (from geopandas==0.13.2->-r ../environment/requirements.txt (line 2)) (2.0.1)\n",
      "Requirement already satisfied: fiona>=1.8.19 in /Users/liamsmith/opt/anaconda3/lib/python3.9/site-packages (from geopandas==0.13.2->-r ../environment/requirements.txt (line 2)) (1.9.4.post1)\n",
      "Requirement already satisfied: future in /Users/liamsmith/opt/anaconda3/lib/python3.9/site-packages (from MDP==3.5->-r ../environment/requirements.txt (line 6)) (0.18.2)\n",
      "Requirement already satisfied: pip in /Users/liamsmith/opt/anaconda3/lib/python3.9/site-packages (from pygris==0.1.5->-r ../environment/requirements.txt (line 7)) (22.2.2)\n",
      "Requirement already satisfied: appdirs in /Users/liamsmith/opt/anaconda3/lib/python3.9/site-packages (from pygris==0.1.5->-r ../environment/requirements.txt (line 7)) (1.4.4)\n",
      "Requirement already satisfied: rtree in /Users/liamsmith/opt/anaconda3/lib/python3.9/site-packages (from pygris==0.1.5->-r ../environment/requirements.txt (line 7)) (0.9.7)\n",
      "Requirement already satisfied: requests in /Users/liamsmith/opt/anaconda3/lib/python3.9/site-packages (from pygris==0.1.5->-r ../environment/requirements.txt (line 7)) (2.28.1)\n",
      "Requirement already satisfied: jinja2 in /Users/liamsmith/opt/anaconda3/lib/python3.9/site-packages (from libpysal==4.7.0->-r ../environment/requirements.txt (line 8)) (2.11.3)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/liamsmith/opt/anaconda3/lib/python3.9/site-packages (from libpysal==4.7.0->-r ../environment/requirements.txt (line 8)) (4.11.1)\n",
      "Requirement already satisfied: platformdirs in /Users/liamsmith/opt/anaconda3/lib/python3.9/site-packages (from libpysal==4.7.0->-r ../environment/requirements.txt (line 8)) (3.8.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/liamsmith/opt/anaconda3/lib/python3.9/site-packages (from matplotlib==3.7.1->-r ../environment/requirements.txt (line 11)) (1.1.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/liamsmith/opt/anaconda3/lib/python3.9/site-packages (from matplotlib==3.7.1->-r ../environment/requirements.txt (line 11)) (4.25.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/liamsmith/opt/anaconda3/lib/python3.9/site-packages (from matplotlib==3.7.1->-r ../environment/requirements.txt (line 11)) (0.11.0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/liamsmith/opt/anaconda3/lib/python3.9/site-packages (from matplotlib==3.7.1->-r ../environment/requirements.txt (line 11)) (6.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/liamsmith/opt/anaconda3/lib/python3.9/site-packages (from matplotlib==3.7.1->-r ../environment/requirements.txt (line 11)) (3.0.9)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/liamsmith/opt/anaconda3/lib/python3.9/site-packages (from matplotlib==3.7.1->-r ../environment/requirements.txt (line 11)) (9.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/liamsmith/opt/anaconda3/lib/python3.9/site-packages (from matplotlib==3.7.1->-r ../environment/requirements.txt (line 11)) (1.4.2)\n",
      "Requirement already satisfied: networkx in /Users/liamsmith/opt/anaconda3/lib/python3.9/site-packages (from mapclassify==2.5.0->-r ../environment/requirements.txt (line 12)) (2.8.4)\n",
      "Requirement already satisfied: scikit-learn in /Users/liamsmith/opt/anaconda3/lib/python3.9/site-packages (from mapclassify==2.5.0->-r ../environment/requirements.txt (line 12)) (1.2.2)\n",
      "Requirement already satisfied: cligj>=0.5 in /Users/liamsmith/opt/anaconda3/lib/python3.9/site-packages (from fiona>=1.8.19->geopandas==0.13.2->-r ../environment/requirements.txt (line 2)) (0.7.2)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /Users/liamsmith/opt/anaconda3/lib/python3.9/site-packages (from fiona>=1.8.19->geopandas==0.13.2->-r ../environment/requirements.txt (line 2)) (21.4.0)\n",
      "Requirement already satisfied: importlib-metadata in /Users/liamsmith/opt/anaconda3/lib/python3.9/site-packages (from fiona>=1.8.19->geopandas==0.13.2->-r ../environment/requirements.txt (line 2)) (4.11.3)\n",
      "Requirement already satisfied: click-plugins>=1.0 in /Users/liamsmith/opt/anaconda3/lib/python3.9/site-packages (from fiona>=1.8.19->geopandas==0.13.2->-r ../environment/requirements.txt (line 2)) (1.1.1)\n",
      "Requirement already satisfied: certifi in /Users/liamsmith/opt/anaconda3/lib/python3.9/site-packages (from fiona>=1.8.19->geopandas==0.13.2->-r ../environment/requirements.txt (line 2)) (2023.5.7)\n",
      "Requirement already satisfied: click~=8.0 in /Users/liamsmith/opt/anaconda3/lib/python3.9/site-packages (from fiona>=1.8.19->geopandas==0.13.2->-r ../environment/requirements.txt (line 2)) (8.0.4)\n",
      "Requirement already satisfied: six in /Users/liamsmith/opt/anaconda3/lib/python3.9/site-packages (from fiona>=1.8.19->geopandas==0.13.2->-r ../environment/requirements.txt (line 2)) (1.16.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/liamsmith/opt/anaconda3/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib==3.7.1->-r ../environment/requirements.txt (line 11)) (3.8.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/liamsmith/opt/anaconda3/lib/python3.9/site-packages (from beautifulsoup4->libpysal==4.7.0->-r ../environment/requirements.txt (line 8)) (2.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/liamsmith/opt/anaconda3/lib/python3.9/site-packages (from jinja2->libpysal==4.7.0->-r ../environment/requirements.txt (line 8)) (2.0.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/liamsmith/opt/anaconda3/lib/python3.9/site-packages (from requests->pygris==0.1.5->-r ../environment/requirements.txt (line 7)) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/liamsmith/opt/anaconda3/lib/python3.9/site-packages (from requests->pygris==0.1.5->-r ../environment/requirements.txt (line 7)) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/liamsmith/opt/anaconda3/lib/python3.9/site-packages (from requests->pygris==0.1.5->-r ../environment/requirements.txt (line 7)) (1.26.11)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/liamsmith/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn->mapclassify==2.5.0->-r ../environment/requirements.txt (line 12)) (2.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/liamsmith/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn->mapclassify==2.5.0->-r ../environment/requirements.txt (line 12)) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# report python version and install required packages\n",
    "# switch if statement from True to False once packages have been installed\n",
    "if True:\n",
    "    !python -V\n",
    "    !pip install -r ../environment/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "113c5897-4a59-45ed-9dfb-df517762143a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import modules, define directories\n",
    "import pygris\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from pygris.data import get_census\n",
    "from pygris import counties\n",
    "from pyhere import here\n",
    "import numpy as np\n",
    "import libpysal as lps\n",
    "import lxml\n",
    "import tabulate\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats.mstats import zscore as ZSCORE\n",
    "from scipy.stats import rankdata\n",
    "import mdp as MDP\n",
    "from operator import itemgetter\n",
    "import copy\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib import patheffects as pe\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from IPython.display import Markdown, Latex\n",
    "\n",
    "pd.set_option(\"chained_assignment\", None)\n",
    "\n",
    "path = {\n",
    "    \"dscr\": here(\"data\", \"scratch\"),\n",
    "    \"drpub\": here(\"data\", \"raw\", \"public\", \"spielman\", \"input\"),\n",
    "    \"drpub2\": here(\"data\", \"raw\", \"public\"),\n",
    "    \"drpriv\": here(\"data\", \"raw\", \"private\"),\n",
    "    \"ddpub\": here(\"data\", \"derived\", \"public\", \"version1\"),\n",
    "    \"ddpriv\": here(\"data\", \"derived\", \"private\"),\n",
    "    \"rfig\": here(\"results\", \"figures\"),\n",
    "    \"roth\": here(\"results\", \"other\"),\n",
    "    \"rtab\": here(\"results\", \"tables\"),\n",
    "    \"og_out\": here(\"data\", \"raw\", \"public\", \"spielman\", \"output\"),\n",
    "    \"dmet\": here(\"data\", \"metadata\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84769898-6733-4f30-a7e6-9ac384edd57f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Switch from False to True to regenerate documentation of computational environment\n",
    "# Note that this approach is not perfect -- it may miss some packages\n",
    "# This code may work better from the command prompt\n",
    "if False:\n",
    "    !pip install pigar\n",
    "    !pigar generate -f ../environment/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0da483-07cc-494a-aa07-5896779b0a37",
   "metadata": {},
   "source": [
    "### Data and variables\n",
    "\n",
    "For Spielman et al.'s original study, the data sources were the 2008-2012 5-year American Community Survey and the 2010 decennial census.\n",
    "Spielman et al. downloaded their data from Social Explorer; in our reproduction, we pull our data directly from the census into Python via a census API package known as pygris.\n",
    "These variables are based on the original work by Cutter et al. to create SoVI, and cover a wide range of social and demographic information, the particulars of which are described below.\n",
    "\n",
    "In order to confirm that our data and Spielman et al.'s data perfectly match each other, we import the names of relevant variables from both datasets here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6ddd1b7-75ff-4dd5-8f76-4b5d51dcf6d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import data dictionary\n",
    "acs_vars = pd.read_csv( here(\"data\", \"metadata\", \"ACS_2012_data_dictionary.csv\") )\n",
    "acs_vars.drop(columns=acs_vars.columns[0], axis=1, inplace=True)\n",
    "\n",
    "acs_variables = list(acs_vars['Reproduction Label'][1:])\n",
    "spielman_acs_variables = list(acs_vars['Spielman Label'][1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edb97cb-7411-4ea7-8cc2-17e2008580f3",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "#### (1) 2008-2012 American Community Survey (5-year)\n",
    "Used in both original study and reproduction.\n",
    "\n",
    "**Planned deviation:** to enhance reproducibility, we draw the data directly from the census into python using the pygris package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7283623f-7412-4c0c-a981-93884617aa73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Switch from False to True to download fresh data from the Census\n",
    "if False:\n",
    "    # Acquire attribute data for reproduction \n",
    "    counties_detailed = get_census(dataset = \"acs/acs5\", # dataset name on the Census API you are connecting to; find datasets at https://api.census.gov/data.html\n",
    "                            variables = acs_variables, # string (or list of strings) of desired vars. For the 2021 5-year ACS Data Profile, those variable IDs are found at https://api.census.gov/data/2021/acs/acs5/profile/variables.html\n",
    "                            year = 2012, # year of your data (or end-year for a 5-year ACS sample)\n",
    "                            params = { # dict of query parameters to send to the API.\n",
    "                              \"for\": \"county:*\"},\n",
    "                            guess_dtypes = True,\n",
    "                            return_geoid = True)\n",
    "\n",
    "    # Drop Puerto Rico\n",
    "    counties_detailed = counties_detailed.loc[~counties_detailed['GEOID'].str.startswith('72')]\n",
    "\n",
    "    # Download and save raw data\n",
    "    counties_detailed.to_csv( here(path[\"drpub2\"], \"counties_attributes_raw.csv\"))\n",
    "else:\n",
    "    counties_detailed = pd.read_csv( here(path[\"drpub2\"], \"counties_attributes_raw.csv\"), dtype = {'GEOID': object} )\n",
    "    counties_detailed = counties_detailed.drop(counties_detailed.columns[0],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0877e17-47dd-4c31-99c2-e22990e9d34a",
   "metadata": {},
   "source": [
    "Load data from Spielman et al.'s research repository for validation of the reproduction study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c152b5a9-cacb-447f-80f6-597398c0d655",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import original data from Spielman et al.\n",
    "\n",
    "# Import base ACS data\n",
    "make_strings = {'Geo_FIPS': object, 'Geo_STATE': object, 'Geo_COUNTY': object,\n",
    "                'Geo_TRACT': object, 'Geo_CBSA': object, 'Geo_CSA': object}\n",
    "\n",
    "acs = pd.read_csv(here(path[\"drpub\"], 'sovi_acs.csv'),\n",
    "                  dtype=make_strings, skiprows=1,encoding='latin-1')\n",
    "\n",
    "# Import, join an ACS supplemental\n",
    "acs_sup2 = pd.read_csv(here(path[\"drpub\"], 'sovi_acs_kids.csv'),\n",
    "                           dtype=make_strings, skiprows=1,encoding='latin-1')\n",
    "\n",
    "acs = acs.merge(acs_sup2, how = \"inner\", on='Geo_FIPS')\n",
    "\n",
    "# Drop Puerto Rico\n",
    "acs = acs[acs.Geo_STATE_x != '72'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78c3c905-1571-448a-92e1-e609cebc745b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "- `Title`: American Community Survey 2012 5-year Estimate Demographic Variables\n",
       "- `Abstract`: The 5-year ACS provides estimates surrounding demographic information in the USA. These estimates are more reliable than 1-year and 3-year estimates but less reliable than decennial census data. On the other hand, 5-year estimates are less current than 1-year and 3-year estimates because they represent measurements taken over 60 months. See the [census website](https://www.census.gov/programs-surveys/acs/guidance/estimates.html) for more details.\n",
       "- `Spatial Coverage`: United States, excluding Puerto Rico\n",
       "- `Spatial Resolution`: County and county-equivalents\n",
       "- `Spatial Reference System`: None, just attribute data\n",
       "- `Temporal Coverage`: 2008-2012\n",
       "- `Temporal Resolution`: Data averaged over five years\n",
       "- `Lineage`: Original data downloaded from Social Explorer and then placed in the original study's GitHub repository: https://github.com/geoss/sovi-validity. Reproduction data obtained directly from the census via API.\n",
       "- `Distribution`: The reproduction data is distributed via a census API. See the detailed tables on the [census website](https://www.census.gov/data/developers/data-sets/acs-5year/2012.html) and instructions for drawing census data directly into python on the [pygris website](https://walker-data.com/pygris/). Spielman et al. originally accessed the ACS data with Social Explorer from the following two tables.\n",
       "  - http://www.socialexplorer.com/pub/reportdata/HtmlResults.aspx?reportid=R10728365\n",
       "  - http://www.socialexplorer.com/pub/reportdata/HtmlResults.aspx?reportid=R10775556\n",
       "- `Constraints`: Census data is available in the public domain\n",
       "- `Data Quality`: Margin of error provided by the Census Bureau for relevant variables\n",
       "- `Variables`:  See ACS_2012_data_dictionary.csv\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown( here(path[\"dmet\"], \"ACS_2012_geographic_metadata.md\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8db055c9-0130-4508-8139-55fda362cb80",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reproduction Label</th>\n",
       "      <th>Spielman Label</th>\n",
       "      <th>Alias</th>\n",
       "      <th>Definition</th>\n",
       "      <th>Type</th>\n",
       "      <th>Domain</th>\n",
       "      <th>Missing Data Value(s)</th>\n",
       "      <th>Missing Data Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GEOID</td>\n",
       "      <td>Geo_FIPS</td>\n",
       "      <td>FIPS code unique identifier</td>\n",
       "      <td>Unique code for every county and county-equiva...</td>\n",
       "      <td>string</td>\n",
       "      <td>01001 - 56045</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B01002_001E</td>\n",
       "      <td>ACS12_5yr_B01002001</td>\n",
       "      <td>median age</td>\n",
       "      <td>MEDIAN AGE BY SEX: Estimate!!Median age!!Total</td>\n",
       "      <td>float64</td>\n",
       "      <td>21.7 - 63</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B03002_001E</td>\n",
       "      <td>ACS12_5yr_B03002001</td>\n",
       "      <td>total population of respondents to race/ethnicity</td>\n",
       "      <td>HISPANIC OR LATINO ORIGIN BY RACE: Estimate!!T...</td>\n",
       "      <td>int64</td>\n",
       "      <td>66 - 9840024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B03002_004E</td>\n",
       "      <td>ACS12_5yr_B03002004</td>\n",
       "      <td>total Black population</td>\n",
       "      <td>HISPANIC OR LATINO ORIGIN BY RACE: Estimate!!T...</td>\n",
       "      <td>int64</td>\n",
       "      <td>0 - 1267825</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B03002_005E</td>\n",
       "      <td>ACS12_5yr_B03002005</td>\n",
       "      <td>total Native American population</td>\n",
       "      <td>HISPANIC OR LATINO ORIGIN BY RACE: Estimate!!T...</td>\n",
       "      <td>int64</td>\n",
       "      <td>0 - 59060</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>B03002_006E</td>\n",
       "      <td>ACS12_5yr_B03002006</td>\n",
       "      <td>total Asian population</td>\n",
       "      <td>HISPANIC OR LATINO ORIGIN BY RACE: Estimate!!T...</td>\n",
       "      <td>int64</td>\n",
       "      <td>0 - 1343920</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>B03002_012E</td>\n",
       "      <td>ACS12_5yr_B03002012</td>\n",
       "      <td>total Latinx population</td>\n",
       "      <td>HISPANIC OR LATINO ORIGIN BY RACE: Estimate!!T...</td>\n",
       "      <td>int64</td>\n",
       "      <td>0 - 4694846</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>B06001_002E</td>\n",
       "      <td>ACS12_5yr_B06001002</td>\n",
       "      <td>total population under 5 years of age</td>\n",
       "      <td>PLACE OF BIRTH BY AGE IN THE UNITED STATES: Es...</td>\n",
       "      <td>float64</td>\n",
       "      <td>0 - 651662</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>B09020_001E</td>\n",
       "      <td>ACS12_5yr_B09020001</td>\n",
       "      <td>total population over 65 years of age</td>\n",
       "      <td>RELATIONSHIP BY HOUSEHOLD TYPE (INCLUDING LIVI...</td>\n",
       "      <td>int64</td>\n",
       "      <td>5 - 1078555</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>B01003_001E</td>\n",
       "      <td>ACS12_5yr_B01003001</td>\n",
       "      <td>total population</td>\n",
       "      <td>TOTAL POPULATION: Estimate!!Total</td>\n",
       "      <td>int64</td>\n",
       "      <td>66 - 9840024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>B25008_001E</td>\n",
       "      <td>ACS12_5yr_B25008001</td>\n",
       "      <td>total population in occupied housing units</td>\n",
       "      <td>TOTAL POPULATION IN OCCUPIED HOUSING UNITS BY ...</td>\n",
       "      <td>int64</td>\n",
       "      <td>62 - 9664175</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>B25002_002E</td>\n",
       "      <td>ACS12_5yr_B25002002</td>\n",
       "      <td>total occupied housing units</td>\n",
       "      <td>OCCUPANCY STATUS: Estimate!!Total!!Occupied</td>\n",
       "      <td>int64</td>\n",
       "      <td>35 - 3218511</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>B25003_003E</td>\n",
       "      <td>ACS12_5yr_B25003003</td>\n",
       "      <td>total renter occupied housing units</td>\n",
       "      <td>TENURE: Estimate!!Total!!Renter occupied</td>\n",
       "      <td>int64</td>\n",
       "      <td>14 - 1695180</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>B25002_001E</td>\n",
       "      <td>ACS12_5yr_B25002001</td>\n",
       "      <td>total housing units for which occupancy status...</td>\n",
       "      <td>OCCUPANCY STATUS: Estimate!!Total</td>\n",
       "      <td>int64</td>\n",
       "      <td>70 - 3441416</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>B09020_021E</td>\n",
       "      <td>ACS12_5yr_B09020021</td>\n",
       "      <td>total 65+ living in group quarters</td>\n",
       "      <td>RELATIONSHIP BY HOUSEHOLD TYPE (INCLUDING LIVI...</td>\n",
       "      <td>int64</td>\n",
       "      <td>0 - 37611</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>B01001_026E</td>\n",
       "      <td>ACS12_5yr_B01001026</td>\n",
       "      <td>total female population</td>\n",
       "      <td>SEX BY AGE: Estimate!!Total!!Female</td>\n",
       "      <td>int64</td>\n",
       "      <td>20 - 4987765</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>B11001_006E</td>\n",
       "      <td>ACS12_5yr_B11001006</td>\n",
       "      <td>total female-headed family households</td>\n",
       "      <td>HOUSEHOLD TYPE (INCLUDING LIVING ALONE): Estim...</td>\n",
       "      <td>int64</td>\n",
       "      <td>0 - 498851</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>B11001_001E</td>\n",
       "      <td>ACS12_5yr_B11001001</td>\n",
       "      <td>total households for which household type is k...</td>\n",
       "      <td>HOUSEHOLD TYPE (INCLUDING LIVING ALONE): Estim...</td>\n",
       "      <td>int64</td>\n",
       "      <td>35 - 3218511</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>B25002_003E</td>\n",
       "      <td>ACS12_5yr_B25002003</td>\n",
       "      <td>total vacant housing units</td>\n",
       "      <td>OCCUPANCY STATUS: Estimate!!Total!!Vacant</td>\n",
       "      <td>int64</td>\n",
       "      <td>35 - 245069</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>B19025_001E</td>\n",
       "      <td>ACS12_5yr_B19025001</td>\n",
       "      <td>aggregate household income</td>\n",
       "      <td>AGGREGATE HOUSEHOLD INCOME IN THE PAST 12 MONT...</td>\n",
       "      <td>int64</td>\n",
       "      <td>1785600 - 263044380000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>B23022_025E</td>\n",
       "      <td>ACS12_5yr_B23022025</td>\n",
       "      <td>total males unemployed for last 12 months</td>\n",
       "      <td>SEX BY WORK STATUS IN THE PAST 12 MONTHS BY US...</td>\n",
       "      <td>int64</td>\n",
       "      <td>1 - 726803</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>B23022_049E</td>\n",
       "      <td>ACS12_5yr_B23022049</td>\n",
       "      <td>total females unemployed for last 12 months</td>\n",
       "      <td>SEX BY WORK STATUS IN THE PAST 12 MONTHS BY US...</td>\n",
       "      <td>int64</td>\n",
       "      <td>0 - 1131737</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>B23022_001E</td>\n",
       "      <td>ACS12_5yr_B23022001</td>\n",
       "      <td>total population for which unemployment and se...</td>\n",
       "      <td>SEX BY WORK STATUS IN THE PAST 12 MONTHS BY US...</td>\n",
       "      <td>int64</td>\n",
       "      <td>45 - 6658456</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>B17021_002E</td>\n",
       "      <td>ACS12_5yr_B17021002</td>\n",
       "      <td>total population below poverty level</td>\n",
       "      <td>POVERTY STATUS OF INDIVIDUALS IN THE PAST 12 M...</td>\n",
       "      <td>int64</td>\n",
       "      <td>0 - 1658231</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>B17021_001E</td>\n",
       "      <td>ACS12_5yr_B17021001</td>\n",
       "      <td>total population for which poverty information...</td>\n",
       "      <td>POVERTY STATUS OF INDIVIDUALS IN THE PAST 12 M...</td>\n",
       "      <td>int64</td>\n",
       "      <td>64 - 9684503</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>B25024_010E</td>\n",
       "      <td>ACS12_5yr_B25024010</td>\n",
       "      <td>number of mobile home housing units in structure</td>\n",
       "      <td>UNITS IN STRUCTURE: Estimate!!Total!!Mobile home</td>\n",
       "      <td>int64</td>\n",
       "      <td>0 - 85377</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>B25024_001E</td>\n",
       "      <td>ACS12_5yr_B25024001</td>\n",
       "      <td>total housing units in structure</td>\n",
       "      <td>UNITS IN STRUCTURE: Estimate!!Total</td>\n",
       "      <td>int64</td>\n",
       "      <td>70 - 3441416</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>C24010_038E</td>\n",
       "      <td>ACS12_5yr_C24010038</td>\n",
       "      <td>total female employed</td>\n",
       "      <td>SEX BY OCCUPATION FOR THE CIVILIAN EMPLOYED PO...</td>\n",
       "      <td>int64</td>\n",
       "      <td>12 - 2056023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>C24010_001E</td>\n",
       "      <td>ACS12_5yr_C24010001</td>\n",
       "      <td>total population for which sex and occupation ...</td>\n",
       "      <td>SEX BY OCCUPATION FOR THE CIVILIAN EMPLOYED PO...</td>\n",
       "      <td>int64</td>\n",
       "      <td>54 - 4495118</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>B19055_002E</td>\n",
       "      <td>ACS12_5yr_B19055002</td>\n",
       "      <td>total households with social security income</td>\n",
       "      <td>SOCIAL SECURITY INCOME IN THE PAST 12 MONTHS F...</td>\n",
       "      <td>int64</td>\n",
       "      <td>9 - 726298</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>B19055_001E</td>\n",
       "      <td>ACS12_5yr_B19055001</td>\n",
       "      <td>total households for which social security inc...</td>\n",
       "      <td>SOCIAL SECURITY INCOME IN THE PAST 12 MONTHS F...</td>\n",
       "      <td>int64</td>\n",
       "      <td>35 - 3218511</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>B09002_002E</td>\n",
       "      <td>ACS12_5yr_B09002002</td>\n",
       "      <td>total children in married couple families</td>\n",
       "      <td>OWN CHILDREN UNDER 18 YEARS BY FAMILY TYPE AND...</td>\n",
       "      <td>int64</td>\n",
       "      <td>0 - 1380977</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>B09002_001E</td>\n",
       "      <td>ACS12_5yr_B09002001</td>\n",
       "      <td>total children for which family type and age a...</td>\n",
       "      <td>OWN CHILDREN UNDER 18 YEARS BY FAMILY TYPE AND...</td>\n",
       "      <td>int64</td>\n",
       "      <td>0 - 2032147</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>B19001_017E</td>\n",
       "      <td>ACS12_5yr_B19001017</td>\n",
       "      <td>total households with over 200k income</td>\n",
       "      <td>HOUSEHOLD INCOME IN THE PAST 12 MONTHS (IN 201...</td>\n",
       "      <td>int64</td>\n",
       "      <td>0 - 208954</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>B06007_005E</td>\n",
       "      <td>ACS12_5yr_B06007005</td>\n",
       "      <td>total Spanish-speakers who speak english less ...</td>\n",
       "      <td>PLACE OF BIRTH BY LANGUAGE SPOKEN AT HOME AND ...</td>\n",
       "      <td>float64</td>\n",
       "      <td>0 - 1695391</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>B06007_008E</td>\n",
       "      <td>ACS12_5yr_B06007008</td>\n",
       "      <td>total people who speak another language and sp...</td>\n",
       "      <td>PLACE OF BIRTH BY LANGUAGE SPOKEN AT HOME AND ...</td>\n",
       "      <td>float64</td>\n",
       "      <td>0 - 743418</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>B06007_001E</td>\n",
       "      <td>ACS12_5yr_B06007001</td>\n",
       "      <td>total population with known language spoken at...</td>\n",
       "      <td>PLACE OF BIRTH BY LANGUAGE SPOKEN AT HOME AND ...</td>\n",
       "      <td>float64</td>\n",
       "      <td>66 - 9188362</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>B16010_002E</td>\n",
       "      <td>ACS12_5yr_B16010002</td>\n",
       "      <td>total population with less than a high school ...</td>\n",
       "      <td>EDUCATIONAL ATTAINMENT AND EMPLOYMENT STATUS B...</td>\n",
       "      <td>int64</td>\n",
       "      <td>5 - 1508273</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>B16010_001E</td>\n",
       "      <td>ACS12_5yr_B16010001</td>\n",
       "      <td>total for which education, employment, languag...</td>\n",
       "      <td>EDUCATIONAL ATTAINMENT AND EMPLOYMENT STATUS B...</td>\n",
       "      <td>int64</td>\n",
       "      <td>65 - 6380366</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>C24050_002E</td>\n",
       "      <td>ACS12_5yr_C24050002</td>\n",
       "      <td>total population in extractive industries</td>\n",
       "      <td>INDUSTRY BY OCCUPATION FOR THE CIVILIAN EMPLOY...</td>\n",
       "      <td>int64</td>\n",
       "      <td>0 - 54942</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>C24050_001E</td>\n",
       "      <td>ACS12_5yr_C24050001</td>\n",
       "      <td>total population for which industry known</td>\n",
       "      <td>INDUSTRY BY OCCUPATION FOR THE CIVILIAN EMPLOY...</td>\n",
       "      <td>int64</td>\n",
       "      <td>54 - 4495118</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>C24050_029E</td>\n",
       "      <td>ACS12_5yr_C24050029</td>\n",
       "      <td>total people in service occupations</td>\n",
       "      <td>INDUSTRY BY OCCUPATION FOR THE CIVILIAN EMPLOY...</td>\n",
       "      <td>int64</td>\n",
       "      <td>4 - 837607</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>B08201_002E</td>\n",
       "      <td>ACS12_5yr_B08201002</td>\n",
       "      <td>total households with no available vehicle</td>\n",
       "      <td>HOUSEHOLD SIZE BY VEHICLES AVAILABLE: Estimate...</td>\n",
       "      <td>int64</td>\n",
       "      <td>0 - 577967</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>B08201_001E</td>\n",
       "      <td>ACS12_5yr_B08201001</td>\n",
       "      <td>total households for which vehicle status and ...</td>\n",
       "      <td>HOUSEHOLD SIZE BY VEHICLES AVAILABLE: Estimate...</td>\n",
       "      <td>int64</td>\n",
       "      <td>35 - 3218511</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>B25064_001E</td>\n",
       "      <td>ACS12_5yr_B25064001</td>\n",
       "      <td>median gross rent</td>\n",
       "      <td>MEDIAN GROSS RENT (DOLLARS): Estimate!!Median ...</td>\n",
       "      <td>int64</td>\n",
       "      <td>275 - 1678</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>B25077_001E</td>\n",
       "      <td>ACS12_5yr_B25077001</td>\n",
       "      <td>median home value</td>\n",
       "      <td>MEDIAN VALUE (DOLLARS): Estimate!!Median value...</td>\n",
       "      <td>float64</td>\n",
       "      <td>19400 - 944100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Reproduction Label       Spielman Label  \\\n",
       "0               GEOID             Geo_FIPS   \n",
       "1         B01002_001E  ACS12_5yr_B01002001   \n",
       "2         B03002_001E  ACS12_5yr_B03002001   \n",
       "3         B03002_004E  ACS12_5yr_B03002004   \n",
       "4         B03002_005E  ACS12_5yr_B03002005   \n",
       "5         B03002_006E  ACS12_5yr_B03002006   \n",
       "6         B03002_012E  ACS12_5yr_B03002012   \n",
       "7         B06001_002E  ACS12_5yr_B06001002   \n",
       "8         B09020_001E  ACS12_5yr_B09020001   \n",
       "9         B01003_001E  ACS12_5yr_B01003001   \n",
       "10        B25008_001E  ACS12_5yr_B25008001   \n",
       "11        B25002_002E  ACS12_5yr_B25002002   \n",
       "12        B25003_003E  ACS12_5yr_B25003003   \n",
       "13        B25002_001E  ACS12_5yr_B25002001   \n",
       "14        B09020_021E  ACS12_5yr_B09020021   \n",
       "15        B01001_026E  ACS12_5yr_B01001026   \n",
       "16        B11001_006E  ACS12_5yr_B11001006   \n",
       "17        B11001_001E  ACS12_5yr_B11001001   \n",
       "18        B25002_003E  ACS12_5yr_B25002003   \n",
       "19        B19025_001E  ACS12_5yr_B19025001   \n",
       "20        B23022_025E  ACS12_5yr_B23022025   \n",
       "21        B23022_049E  ACS12_5yr_B23022049   \n",
       "22        B23022_001E  ACS12_5yr_B23022001   \n",
       "23        B17021_002E  ACS12_5yr_B17021002   \n",
       "24        B17021_001E  ACS12_5yr_B17021001   \n",
       "25        B25024_010E  ACS12_5yr_B25024010   \n",
       "26        B25024_001E  ACS12_5yr_B25024001   \n",
       "27        C24010_038E  ACS12_5yr_C24010038   \n",
       "28        C24010_001E  ACS12_5yr_C24010001   \n",
       "29        B19055_002E  ACS12_5yr_B19055002   \n",
       "30        B19055_001E  ACS12_5yr_B19055001   \n",
       "31        B09002_002E  ACS12_5yr_B09002002   \n",
       "32        B09002_001E  ACS12_5yr_B09002001   \n",
       "33        B19001_017E  ACS12_5yr_B19001017   \n",
       "34        B06007_005E  ACS12_5yr_B06007005   \n",
       "35        B06007_008E  ACS12_5yr_B06007008   \n",
       "36        B06007_001E  ACS12_5yr_B06007001   \n",
       "37        B16010_002E  ACS12_5yr_B16010002   \n",
       "38        B16010_001E  ACS12_5yr_B16010001   \n",
       "39        C24050_002E  ACS12_5yr_C24050002   \n",
       "40        C24050_001E  ACS12_5yr_C24050001   \n",
       "41        C24050_029E  ACS12_5yr_C24050029   \n",
       "42        B08201_002E  ACS12_5yr_B08201002   \n",
       "43        B08201_001E  ACS12_5yr_B08201001   \n",
       "44        B25064_001E  ACS12_5yr_B25064001   \n",
       "45        B25077_001E  ACS12_5yr_B25077001   \n",
       "\n",
       "                                                Alias  \\\n",
       "0                         FIPS code unique identifier   \n",
       "1                                          median age   \n",
       "2   total population of respondents to race/ethnicity   \n",
       "3                              total Black population   \n",
       "4                    total Native American population   \n",
       "5                              total Asian population   \n",
       "6                             total Latinx population   \n",
       "7               total population under 5 years of age   \n",
       "8               total population over 65 years of age   \n",
       "9                                    total population   \n",
       "10         total population in occupied housing units   \n",
       "11                       total occupied housing units   \n",
       "12                total renter occupied housing units   \n",
       "13  total housing units for which occupancy status...   \n",
       "14                 total 65+ living in group quarters   \n",
       "15                            total female population   \n",
       "16              total female-headed family households   \n",
       "17  total households for which household type is k...   \n",
       "18                         total vacant housing units   \n",
       "19                         aggregate household income   \n",
       "20          total males unemployed for last 12 months   \n",
       "21        total females unemployed for last 12 months   \n",
       "22  total population for which unemployment and se...   \n",
       "23               total population below poverty level   \n",
       "24  total population for which poverty information...   \n",
       "25   number of mobile home housing units in structure   \n",
       "26                   total housing units in structure   \n",
       "27                              total female employed   \n",
       "28  total population for which sex and occupation ...   \n",
       "29       total households with social security income   \n",
       "30  total households for which social security inc...   \n",
       "31          total children in married couple families   \n",
       "32  total children for which family type and age a...   \n",
       "33             total households with over 200k income   \n",
       "34  total Spanish-speakers who speak english less ...   \n",
       "35  total people who speak another language and sp...   \n",
       "36  total population with known language spoken at...   \n",
       "37  total population with less than a high school ...   \n",
       "38  total for which education, employment, languag...   \n",
       "39          total population in extractive industries   \n",
       "40          total population for which industry known   \n",
       "41                total people in service occupations   \n",
       "42         total households with no available vehicle   \n",
       "43  total households for which vehicle status and ...   \n",
       "44                                  median gross rent   \n",
       "45                                  median home value   \n",
       "\n",
       "                                           Definition     Type  \\\n",
       "0   Unique code for every county and county-equiva...   string   \n",
       "1      MEDIAN AGE BY SEX: Estimate!!Median age!!Total  float64   \n",
       "2   HISPANIC OR LATINO ORIGIN BY RACE: Estimate!!T...    int64   \n",
       "3   HISPANIC OR LATINO ORIGIN BY RACE: Estimate!!T...    int64   \n",
       "4   HISPANIC OR LATINO ORIGIN BY RACE: Estimate!!T...    int64   \n",
       "5   HISPANIC OR LATINO ORIGIN BY RACE: Estimate!!T...    int64   \n",
       "6   HISPANIC OR LATINO ORIGIN BY RACE: Estimate!!T...    int64   \n",
       "7   PLACE OF BIRTH BY AGE IN THE UNITED STATES: Es...  float64   \n",
       "8   RELATIONSHIP BY HOUSEHOLD TYPE (INCLUDING LIVI...    int64   \n",
       "9                   TOTAL POPULATION: Estimate!!Total    int64   \n",
       "10  TOTAL POPULATION IN OCCUPIED HOUSING UNITS BY ...    int64   \n",
       "11        OCCUPANCY STATUS: Estimate!!Total!!Occupied    int64   \n",
       "12           TENURE: Estimate!!Total!!Renter occupied    int64   \n",
       "13                  OCCUPANCY STATUS: Estimate!!Total    int64   \n",
       "14  RELATIONSHIP BY HOUSEHOLD TYPE (INCLUDING LIVI...    int64   \n",
       "15                SEX BY AGE: Estimate!!Total!!Female    int64   \n",
       "16  HOUSEHOLD TYPE (INCLUDING LIVING ALONE): Estim...    int64   \n",
       "17  HOUSEHOLD TYPE (INCLUDING LIVING ALONE): Estim...    int64   \n",
       "18          OCCUPANCY STATUS: Estimate!!Total!!Vacant    int64   \n",
       "19  AGGREGATE HOUSEHOLD INCOME IN THE PAST 12 MONT...    int64   \n",
       "20  SEX BY WORK STATUS IN THE PAST 12 MONTHS BY US...    int64   \n",
       "21  SEX BY WORK STATUS IN THE PAST 12 MONTHS BY US...    int64   \n",
       "22  SEX BY WORK STATUS IN THE PAST 12 MONTHS BY US...    int64   \n",
       "23  POVERTY STATUS OF INDIVIDUALS IN THE PAST 12 M...    int64   \n",
       "24  POVERTY STATUS OF INDIVIDUALS IN THE PAST 12 M...    int64   \n",
       "25   UNITS IN STRUCTURE: Estimate!!Total!!Mobile home    int64   \n",
       "26                UNITS IN STRUCTURE: Estimate!!Total    int64   \n",
       "27  SEX BY OCCUPATION FOR THE CIVILIAN EMPLOYED PO...    int64   \n",
       "28  SEX BY OCCUPATION FOR THE CIVILIAN EMPLOYED PO...    int64   \n",
       "29  SOCIAL SECURITY INCOME IN THE PAST 12 MONTHS F...    int64   \n",
       "30  SOCIAL SECURITY INCOME IN THE PAST 12 MONTHS F...    int64   \n",
       "31  OWN CHILDREN UNDER 18 YEARS BY FAMILY TYPE AND...    int64   \n",
       "32  OWN CHILDREN UNDER 18 YEARS BY FAMILY TYPE AND...    int64   \n",
       "33  HOUSEHOLD INCOME IN THE PAST 12 MONTHS (IN 201...    int64   \n",
       "34  PLACE OF BIRTH BY LANGUAGE SPOKEN AT HOME AND ...  float64   \n",
       "35  PLACE OF BIRTH BY LANGUAGE SPOKEN AT HOME AND ...  float64   \n",
       "36  PLACE OF BIRTH BY LANGUAGE SPOKEN AT HOME AND ...  float64   \n",
       "37  EDUCATIONAL ATTAINMENT AND EMPLOYMENT STATUS B...    int64   \n",
       "38  EDUCATIONAL ATTAINMENT AND EMPLOYMENT STATUS B...    int64   \n",
       "39  INDUSTRY BY OCCUPATION FOR THE CIVILIAN EMPLOY...    int64   \n",
       "40  INDUSTRY BY OCCUPATION FOR THE CIVILIAN EMPLOY...    int64   \n",
       "41  INDUSTRY BY OCCUPATION FOR THE CIVILIAN EMPLOY...    int64   \n",
       "42  HOUSEHOLD SIZE BY VEHICLES AVAILABLE: Estimate...    int64   \n",
       "43  HOUSEHOLD SIZE BY VEHICLES AVAILABLE: Estimate...    int64   \n",
       "44  MEDIAN GROSS RENT (DOLLARS): Estimate!!Median ...    int64   \n",
       "45  MEDIAN VALUE (DOLLARS): Estimate!!Median value...  float64   \n",
       "\n",
       "                    Domain Missing Data Value(s)  Missing Data Frequency  \n",
       "0            01001 - 56045                  None                     0.0  \n",
       "1                21.7 - 63                   NaN                     0.0  \n",
       "2             66 - 9840024                   NaN                     0.0  \n",
       "3              0 - 1267825                   NaN                     0.0  \n",
       "4                0 - 59060                   NaN                     0.0  \n",
       "5              0 - 1343920                   NaN                     0.0  \n",
       "6              0 - 4694846                   NaN                     0.0  \n",
       "7               0 - 651662                   NaN                    78.0  \n",
       "8              5 - 1078555                   NaN                     0.0  \n",
       "9             66 - 9840024                   NaN                     0.0  \n",
       "10            62 - 9664175                   NaN                     0.0  \n",
       "11            35 - 3218511                   NaN                     0.0  \n",
       "12            14 - 1695180                   NaN                     0.0  \n",
       "13            70 - 3441416                   NaN                     0.0  \n",
       "14               0 - 37611                   NaN                     0.0  \n",
       "15            20 - 4987765                   NaN                     0.0  \n",
       "16              0 - 498851                   NaN                     0.0  \n",
       "17            35 - 3218511                   NaN                     0.0  \n",
       "18             35 - 245069                   NaN                     0.0  \n",
       "19  1785600 - 263044380000                   NaN                     0.0  \n",
       "20              1 - 726803                   NaN                     0.0  \n",
       "21             0 - 1131737                   NaN                     0.0  \n",
       "22            45 - 6658456                   NaN                     0.0  \n",
       "23             0 - 1658231                   NaN                     0.0  \n",
       "24            64 - 9684503                   NaN                     0.0  \n",
       "25               0 - 85377                   NaN                     0.0  \n",
       "26            70 - 3441416                   NaN                     0.0  \n",
       "27            12 - 2056023                   NaN                     0.0  \n",
       "28            54 - 4495118                   NaN                     0.0  \n",
       "29              9 - 726298                   NaN                     0.0  \n",
       "30            35 - 3218511                   NaN                     0.0  \n",
       "31             0 - 1380977                   NaN                     0.0  \n",
       "32             0 - 2032147                   NaN                     0.0  \n",
       "33              0 - 208954                   NaN                     0.0  \n",
       "34             0 - 1695391                   NaN                    78.0  \n",
       "35              0 - 743418                   NaN                    78.0  \n",
       "36            66 - 9188362                   NaN                    78.0  \n",
       "37             5 - 1508273                   NaN                     0.0  \n",
       "38            65 - 6380366                   NaN                     0.0  \n",
       "39               0 - 54942                   NaN                     0.0  \n",
       "40            54 - 4495118                   NaN                     0.0  \n",
       "41              4 - 837607                   NaN                     0.0  \n",
       "42              0 - 577967                   NaN                     0.0  \n",
       "43            35 - 3218511                   NaN                     0.0  \n",
       "44              275 - 1678                   NaN                     0.0  \n",
       "45          19400 - 944100                   NaN                     1.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acs_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba60fbe4-94ad-46f8-9401-7182992d669a",
   "metadata": {},
   "source": [
    "The above are the metadata files that we wrote for our pygris-acquired version of this data, stored as ACS_2012_data_dictionary.csv and ACS_2012_geographic_metadata.md. \n",
    "The metadata files provided by Spielman et al. are also in our repository, named sovi_acs.txt and sovi_acs_kids.txt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb64c5f-753d-4400-aa27-84020ab5ab64",
   "metadata": {},
   "source": [
    "#### (2) 2010 Decennial Census\n",
    "Used in Spielman et al.'s original study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b3a3b73-07db-4c92-9eff-59e7a4168701",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import decennial supplemental\n",
    "dec_sup1 = pd.read_csv(here(path[\"drpub\"],'sovi_decennial_sup1.csv'),\n",
    "        dtype=make_strings,skiprows=1,encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88eece61-5e76-402a-9ccf-006cea0101df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "- `Title`: 2010 Decennial Census\n",
       "- `Abstract`: Collected once every ten years, the decennial census documents demographic and population data in the United States.\n",
       "- `Spatial Coverage`: United States, excluding Puerto Rico\n",
       "- `Spatial Resolution`: County and county-equivalents\n",
       "- `Spatial Reference System`: None, just attribute data\n",
       "- `Temporal Coverage`: 2010\n",
       "- `Temporal Resolution`: One-time observations\n",
       "- `Lineage`: Original data downloaded from Social Explorer and then placed in the original study's GitHub repository: https://github.com/geoss/sovi-validity.\n",
       "- `Distribution`: Visit [this URL](http://www.socialexplorer.com/pub/reportdata/HtmlResults.aspx?reportid=R10728369) for access\n",
       "- `Constraints`: Census data is available in the public domain\n",
       "- `Data Quality`: Margin of error provided by the Census Bureau for relevant variables\n",
       "- `Variables`:\n",
       "\n",
       "| Label | Alias | Definition | Type | Accuracy | Domain | Missing Data Value(s) | Missing Data Frequency |\n",
       "| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n",
       "| SE_T02A_002 | Land area | Area (Land) in square miles | float64 |  ... | 1.998779 - 145504.8 | nan | 0 |\n",
       "| Geo_FIPS | FIPS code unique identifier | Unique code for every county and county-equivalent in USA | string | ... | g01001 - g56045 | None | 0 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown( here(path[\"dmet\"], \"dec_2010_metadata.md\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4848b82-b8f4-4b4f-b33d-5027e3cdb96c",
   "metadata": {
    "tags": []
   },
   "source": [
    "Original metadata file provided by Spielman et al. as sovi_decennial_sup1.txt.\n",
    "\n",
    "#### (3) USA Counties Shapefile\n",
    "Used in Spielman et al.'s original study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d98aa9a5-28f0-45b1-a0fe-ca0eb9fa721f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spielman_geom = gpd.read_file( here(path[\"drpub\"], \"USA_Counties_500k.shp\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a59c760a-1c31-47c2-acaa-b8689392bd24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "- `Title`: USA Counties Geographic Shapefile\n",
       "- `Abstract`: No metadata provided, so it is unclear exactly where Spielman et al. acquired this file but they likely downloaded it directly or indirectly from the census. The shapefile provides the geometries of counties and county-equivalents in the United States, with limited attribute information including county name and a unique identifier.\n",
       "- `Spatial Coverage`: United States, excluding Puerto Rico\n",
       "- `Spatial Resolution`: County and county-equivalents\n",
       "- `Spatial Reference System`: EPSG:4269\n",
       "- `Temporal Coverage`: Unknown\n",
       "- `Temporal Resolution`: One-time observations\n",
       "- `Lineage`: Unknown\n",
       "- `Distribution`: Unknown. Presumably downloaded from census.\n",
       "- `Constraints`: Census data is available in the public domain\n",
       "- `Data Quality`: 1:500,000 scale\n",
       "- `Variables`: For each variable, enter the following information. If you have two or more variables per data source, you may want to present this information in table form (shown below)\n",
       "\n",
       "| Label | Alias | Definition | Type | Accuracy | Domain | Missing Data Value(s) | Missing Data Frequency |\n",
       "| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n",
       "| geoFIPS | FIPS code unique identifer | Unique code for every county and county-equivalent in USA | string | ... | g01001 - g56045 | None | 0 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown( here(path[\"dmet\"], 'USA_counties_metadata.md') )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e39487-bdcd-4bc1-8f10-1b02cf2af4c1",
   "metadata": {},
   "source": [
    "No original metadata file provided.\n",
    "\n",
    "#### (4) USA Counties Cartographic Boundaries\n",
    "\n",
    "Used in reproduction study.\n",
    "\n",
    "**Planned deviation:** to enhance reproducibility, we draw the data directly from the census into python using the pygris package. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03dfa8d7-a08e-4411-8c37-855d0b18d170",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Switch False to True if you wish to acquire data directly from census\n",
    "if False:\n",
    "    # Acquire geographical data for reproduction\n",
    "    counties_shp = counties(cb = True, year = 2010, cache = True) # year 2012 (and 2011) cartographic boundaries not available\n",
    "\n",
    "    # Save raw data\n",
    "    counties_shp.to_file( here(path[\"drpub2\"], \"counties_geometries_raw.gpkg\") )\n",
    "else:\n",
    "    counties_shp = gpd.read_file( here(path[\"drpub2\"], \"counties_geometries_raw.gpkg\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c28364df-eabd-46eb-aa2f-b9b95dfad9f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "- `Title`: USA Counties Cartographic Boundaries\n",
       "- `Abstract`: The cartographic boundary files provided by the US census are simplified representations of the MAF/TIGER files. We use the 2010 boundary file because the census has not made such a file available for 2012 or 2011 and the original paper also used land area from 2010. This shapefile provides the geometries of counties and county-equivalents in the United States, with limited attribute information including land area.\n",
       "- `Spatial Coverage`: United States, excluding Puerto Rico\n",
       "- `Spatial Resolution`: County and county-equivalents\n",
       "- `Spatial Reference System`: EPSG:4269\n",
       "- `Temporal Coverage`: 2010\n",
       "- `Temporal Resolution`: One-time observations\n",
       "- `Lineage`: We use [pygris](https://walker-data.com/pygris/) to pull the data directly from the census into python.\n",
       "- `Distribution`: This file is distributed via a census API. See more information on the [census website](https://www.census.gov/geographies/mapping-files/time-series/geo/carto-boundary-file.2010.html#list-tab-1556094155) and instructions for drawing census data directly into python on the [pygris website](https://walker-data.com/pygris/).\n",
       "- `Constraints`: Census data is available in the public domain\n",
       "- `Data Quality`: 1:500,000 scale\n",
       "- `Variables`:\n",
       "\n",
       "| Label | Alias | Definition | Type | Domain | Missing Data Value(s) | Missing Data Frequency |\n",
       "| :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n",
       "| STATE | State-level FIPS code | State-level FIPS code | string | 01 - 56 | None | 0 |\n",
       "| COUNTY | County-level FIPS code | County-level FIPS code | string | 001 - 840 | None | 0 |\n",
       "| CENSUSAREA | land area | land area in square miles | float64 | 1.999 - 145504.789 | NaN | 0 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown( here(path[\"dmet\"], \"county_geom_2010_metadata.md\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0340f287-0381-499a-be9c-67ccd590edfd",
   "metadata": {},
   "source": [
    "The metadata file for this data is stored as county_geom_2010_metadata.md."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d02df98-648b-4350-9d6d-9e67fb62906e",
   "metadata": {},
   "source": [
    "### Data transformations\n",
    "A workflow diagram for this section is displayed below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13988a56-258a-4bfa-96c5-529f0eea4c07",
   "metadata": {},
   "source": [
    "![Data Preparation Workflow](../../results/figures/workflow_data_transformations.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab958819-38ce-43f3-ac73-f6bc3662ad24",
   "metadata": {},
   "source": [
    "We begin with step P1: joining the geometry and attribute data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd52589e-1758-43f7-b5df-ac7544dbc567",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step P1\n",
    "# Join geometry and attribute data for reproduction \n",
    "counties_shp['GEOID'] = counties_shp.STATE + counties_shp.COUNTY\n",
    "counties = counties_shp.merge(counties_detailed, how = \"inner\", on = \"GEOID\")\n",
    "\n",
    "# Also join Spielman's land area information to the rest of Spielman's data\n",
    "# (to check that all data is accurate, not for purposes of analysis)\n",
    "acs = acs.merge(dec_sup1, how = \"inner\", on='Geo_FIPS')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32deeaf1-fb2a-40a8-8f32-d8165a668f9c",
   "metadata": {},
   "source": [
    "**Planned deviation:** Because we decided to acquire our data independently from Spielman et al., we need to check that our data is indeed the same as theirs.\n",
    "\n",
    "To begin, we define a function that can check that the entries of two pandas DataFrames are equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16709b92-5656-4319-b3dc-6e4add5de861",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a function that can determine whether every entry in specified columns of two tables match  \n",
    "def equiv(table1, sort1, column1, table2, sort2, column2):\n",
    "    ''' \n",
    "    Tests two tables to see whether corresponding columns have equivalent entries.\n",
    "    \n",
    "    Parameters:\n",
    "    table1 - the first table\n",
    "    sort1 - the column in the first table to join by (str)\n",
    "    column1 - the column(s) in the first table to test the values of (list of str) (should list analogous columns for columns2) \n",
    "    table2 - the second table\n",
    "    sort2 - the column in the second table to join by (str)\n",
    "    column2 - the column(s) in the second table to test the values of (list of str)\n",
    "    '''\n",
    "    # Sort tables\n",
    "    table1 = table1.sort_values(by = sort1).reset_index()\n",
    "    table2 = table2.sort_values(by = sort2).reset_index()\n",
    "    \n",
    "    # Rename column name in table2 to match that in table1\n",
    "    for i in range(len(column1)):\n",
    "        table2 = table2.rename(columns={column2[i]: column1[i]})\n",
    "\n",
    "    # Select the columns to test equivalency of\n",
    "    table1 = table1[column1]\n",
    "    table2 = table2[column1]\n",
    "    \n",
    "    # Perform equivalency test\n",
    "    test = table1.eq(table2)\n",
    "    return test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e627ae1-bf8d-4d3c-867a-2fb55c39cec2",
   "metadata": {},
   "source": [
    "Next, we round our area columns to the nearest integer, just for the purposes of comparing the two columns. These columns came from different sources and we know that they do not match up exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b63cc6f-6e08-4cd6-9a65-7478b24bd763",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Round area column\n",
    "# acs['SE_T02A_002_check'] = acs.SE_T02A_002.round(0)\n",
    "# counties['CENSUSAREA_check'] = counties.CENSUSAREA.round(0)\n",
    "\n",
    "# Add the area variables to the lists of variables\n",
    "acs_variables.append('CENSUSAREA')\n",
    "spielman_acs_variables.append('SE_T02A_002')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7bbb216f-fa73-4cc8-9508-b49ebae49e40",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>B25077_001E</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CENSUSAREA</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              test\n",
       "B25077_001E  False\n",
       "CENSUSAREA   False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform equivalency test\n",
    "test = equiv(counties, \"GEOID\", acs_variables, acs, \"Geo_FIPS\", spielman_acs_variables)\n",
    "matching_cols = pd.DataFrame({\"test\": test.sum().eq(3143)}) # 3143 matches the number of rows \n",
    "matching_cols.loc[~matching_cols.test] # Identify the columns that have some data discrepencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43d1418-7187-4f97-82dd-180fa118d185",
   "metadata": {},
   "source": [
    "The following variables have some discrepancy between the original and reproduction data:\n",
    "- B25077_001E\n",
    "- CENSUSAREA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0490293-126a-4052-b089-518129bd03ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GEOID</th>\n",
       "      <th>B25077_001E</th>\n",
       "      <th>Geo_FIPS</th>\n",
       "      <th>ACS12_5yr_B25077001</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15005</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   GEOID  B25077_001E Geo_FIPS  ACS12_5yr_B25077001\n",
       "0  15005          NaN    15005                  NaN"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the data values for B25077_001E at the indices with data discrepancies\n",
    "\n",
    "# Find the rows for which there are data discrepancies\n",
    "messed_up_indices = test[[\"B25077_001E\"]].loc[~test.B25077_001E]\n",
    "\n",
    "# Select the data of interest from tygris\n",
    "tygris_data = counties.sort_values(by = \"GEOID\")\\\n",
    "    .reset_index().loc[messed_up_indices.index]\\\n",
    "    [[\"GEOID\", \"B25077_001E\"]]\n",
    "\n",
    "# Select the data of interest from Spielman et al.\n",
    "spielman_data = acs.sort_values(by = \"Geo_FIPS\")\\\n",
    "    .reset_index().loc[messed_up_indices.index]\\\n",
    "    [[\"Geo_FIPS\", \"ACS12_5yr_B25077001\"]]\n",
    "\n",
    "# Join and inspect\n",
    "merged = tygris_data.merge(spielman_data, how = \"inner\", left_on = \"GEOID\", right_on = \"Geo_FIPS\")\n",
    "merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0ddbbd-93c9-4221-b84f-9bc795db1ee6",
   "metadata": {},
   "source": [
    "By inspection, we see that the one disagreement between B25077_001E and ACS12_5yr_B25077001 occurs because of a NaN value in an analogous location in each of the two datasets. \n",
    "Rather than revealing an issue in matching our data, this shows us that we will need to impute a missing value for one NaN in B25077_001E -- median home value (see P3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91406dcb-fba5-4176-b16c-ff0eb8a643e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GEOID</th>\n",
       "      <th>CENSUSAREA</th>\n",
       "      <th>Geo_FIPS</th>\n",
       "      <th>SE_T02A_002</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01001</td>\n",
       "      <td>594.436</td>\n",
       "      <td>01001</td>\n",
       "      <td>594.4361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01005</td>\n",
       "      <td>884.876</td>\n",
       "      <td>01005</td>\n",
       "      <td>884.8763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01007</td>\n",
       "      <td>622.582</td>\n",
       "      <td>01007</td>\n",
       "      <td>622.5823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01009</td>\n",
       "      <td>644.776</td>\n",
       "      <td>01009</td>\n",
       "      <td>644.7759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01011</td>\n",
       "      <td>622.805</td>\n",
       "      <td>01011</td>\n",
       "      <td>622.8047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2285</th>\n",
       "      <td>55137</td>\n",
       "      <td>626.153</td>\n",
       "      <td>55137</td>\n",
       "      <td>626.1533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2286</th>\n",
       "      <td>55141</td>\n",
       "      <td>793.116</td>\n",
       "      <td>55141</td>\n",
       "      <td>793.1165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2287</th>\n",
       "      <td>56013</td>\n",
       "      <td>9183.814</td>\n",
       "      <td>56013</td>\n",
       "      <td>9183.8130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2288</th>\n",
       "      <td>56023</td>\n",
       "      <td>4076.129</td>\n",
       "      <td>56023</td>\n",
       "      <td>4076.1300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2289</th>\n",
       "      <td>56037</td>\n",
       "      <td>10426.649</td>\n",
       "      <td>56037</td>\n",
       "      <td>10426.6500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2290 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      GEOID  CENSUSAREA Geo_FIPS  SE_T02A_002\n",
       "0     01001     594.436    01001     594.4361\n",
       "1     01005     884.876    01005     884.8763\n",
       "2     01007     622.582    01007     622.5823\n",
       "3     01009     644.776    01009     644.7759\n",
       "4     01011     622.805    01011     622.8047\n",
       "...     ...         ...      ...          ...\n",
       "2285  55137     626.153    55137     626.1533\n",
       "2286  55141     793.116    55141     793.1165\n",
       "2287  56013    9183.814    56013    9183.8130\n",
       "2288  56023    4076.129    56023    4076.1300\n",
       "2289  56037   10426.649    56037   10426.6500\n",
       "\n",
       "[2290 rows x 4 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the data values for CENSUSAREA at the indices with data discrepancies\n",
    "\n",
    "# Find the rows for which there are data discrepancies\n",
    "messed_up_indices = test[[\"CENSUSAREA\"]].loc[~test.CENSUSAREA]\n",
    "\n",
    "# Select the data of interest from tygris\n",
    "tygris_data = counties.sort_values(by = \"GEOID\")\\\n",
    "    .reset_index().loc[messed_up_indices.index]\\\n",
    "    [[\"GEOID\", \"CENSUSAREA\"]]\n",
    "\n",
    "# Select the data of interest from Spielman et al.\n",
    "spielman_data = acs.sort_values(by = \"Geo_FIPS\")\\\n",
    "    .reset_index().loc[messed_up_indices.index]\\\n",
    "    [[\"Geo_FIPS\", \"SE_T02A_002\"]]\n",
    "\n",
    "# Join and inspect\n",
    "merged = tygris_data.merge(spielman_data, how = \"inner\", left_on = \"GEOID\", right_on = \"Geo_FIPS\")\n",
    "merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4dc08b-e781-4253-b655-0464a84c818b",
   "metadata": {},
   "source": [
    "There are many disagreements between CENSUSAREA and SE_T02A_002, but they appear to be relatively minor differences. Let us evaluate how large those differences are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1fb330bb-7720-4ef2-b1d6-cc4b08c73a91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum difference: 0.010999999998603016 \n",
      "Average difference: 0.00031027379912322995\n"
     ]
    }
   ],
   "source": [
    "merged[\"Difference\"] = abs(merged[\"CENSUSAREA\"] - merged[\"SE_T02A_002\"])\n",
    "print(\"Maximum difference:\", merged['Difference'].max(), \"\\nAverage difference:\", merged['Difference'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e5af6b81-5a9a-44a7-906e-803deb860450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GEO_ID</th>\n",
       "      <th>STATE</th>\n",
       "      <th>COUNTY</th>\n",
       "      <th>NAME</th>\n",
       "      <th>LSAD</th>\n",
       "      <th>CENSUSAREA</th>\n",
       "      <th>geometry</th>\n",
       "      <th>GEOID</th>\n",
       "      <th>B01002_001E</th>\n",
       "      <th>B03002_001E</th>\n",
       "      <th>...</th>\n",
       "      <th>B06007_001E</th>\n",
       "      <th>B16010_002E</th>\n",
       "      <th>B16010_001E</th>\n",
       "      <th>C24050_002E</th>\n",
       "      <th>C24050_001E</th>\n",
       "      <th>C24050_029E</th>\n",
       "      <th>B08201_002E</th>\n",
       "      <th>B08201_001E</th>\n",
       "      <th>B25064_001E</th>\n",
       "      <th>B25077_001E</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0500000US01029</td>\n",
       "      <td>01</td>\n",
       "      <td>029</td>\n",
       "      <td>Cleburne</td>\n",
       "      <td>County</td>\n",
       "      <td>560.100</td>\n",
       "      <td>POLYGON ((-85.38872 33.91304, -85.38088 33.873...</td>\n",
       "      <td>01029</td>\n",
       "      <td>41.2</td>\n",
       "      <td>14905</td>\n",
       "      <td>...</td>\n",
       "      <td>13958.0</td>\n",
       "      <td>2622</td>\n",
       "      <td>10173</td>\n",
       "      <td>285</td>\n",
       "      <td>5868</td>\n",
       "      <td>931</td>\n",
       "      <td>352</td>\n",
       "      <td>5518</td>\n",
       "      <td>566</td>\n",
       "      <td>91300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0500000US01031</td>\n",
       "      <td>01</td>\n",
       "      <td>031</td>\n",
       "      <td>Coffee</td>\n",
       "      <td>County</td>\n",
       "      <td>678.972</td>\n",
       "      <td>POLYGON ((-86.03044 31.61894, -86.00408 31.619...</td>\n",
       "      <td>01031</td>\n",
       "      <td>37.8</td>\n",
       "      <td>49966</td>\n",
       "      <td>...</td>\n",
       "      <td>46580.0</td>\n",
       "      <td>6092</td>\n",
       "      <td>33409</td>\n",
       "      <td>635</td>\n",
       "      <td>19332</td>\n",
       "      <td>3015</td>\n",
       "      <td>1270</td>\n",
       "      <td>19002</td>\n",
       "      <td>638</td>\n",
       "      <td>126400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0500000US01037</td>\n",
       "      <td>01</td>\n",
       "      <td>037</td>\n",
       "      <td>Coosa</td>\n",
       "      <td>County</td>\n",
       "      <td>650.926</td>\n",
       "      <td>POLYGON ((-86.00928 33.10164, -86.00917 33.090...</td>\n",
       "      <td>01037</td>\n",
       "      <td>44.4</td>\n",
       "      <td>11305</td>\n",
       "      <td>...</td>\n",
       "      <td>10762.0</td>\n",
       "      <td>1974</td>\n",
       "      <td>8153</td>\n",
       "      <td>129</td>\n",
       "      <td>4354</td>\n",
       "      <td>722</td>\n",
       "      <td>213</td>\n",
       "      <td>4573</td>\n",
       "      <td>573</td>\n",
       "      <td>74500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0500000US01039</td>\n",
       "      <td>01</td>\n",
       "      <td>039</td>\n",
       "      <td>Covington</td>\n",
       "      <td>County</td>\n",
       "      <td>1030.456</td>\n",
       "      <td>POLYGON ((-86.34851 30.99434, -86.35023 30.994...</td>\n",
       "      <td>01039</td>\n",
       "      <td>42.5</td>\n",
       "      <td>37803</td>\n",
       "      <td>...</td>\n",
       "      <td>35540.0</td>\n",
       "      <td>5936</td>\n",
       "      <td>26210</td>\n",
       "      <td>973</td>\n",
       "      <td>15087</td>\n",
       "      <td>2341</td>\n",
       "      <td>986</td>\n",
       "      <td>14899</td>\n",
       "      <td>560</td>\n",
       "      <td>86700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0500000US01041</td>\n",
       "      <td>01</td>\n",
       "      <td>041</td>\n",
       "      <td>Crenshaw</td>\n",
       "      <td>County</td>\n",
       "      <td>608.840</td>\n",
       "      <td>POLYGON ((-86.14699 31.68045, -86.14711 31.663...</td>\n",
       "      <td>01041</td>\n",
       "      <td>40.3</td>\n",
       "      <td>13971</td>\n",
       "      <td>...</td>\n",
       "      <td>13069.0</td>\n",
       "      <td>2245</td>\n",
       "      <td>9477</td>\n",
       "      <td>487</td>\n",
       "      <td>5899</td>\n",
       "      <td>827</td>\n",
       "      <td>344</td>\n",
       "      <td>5648</td>\n",
       "      <td>488</td>\n",
       "      <td>70000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3138</th>\n",
       "      <td>0500000US56027</td>\n",
       "      <td>56</td>\n",
       "      <td>027</td>\n",
       "      <td>Niobrara</td>\n",
       "      <td>County</td>\n",
       "      <td>2626.037</td>\n",
       "      <td>POLYGON ((-104.05298 42.85955, -104.05286 42.7...</td>\n",
       "      <td>56027</td>\n",
       "      <td>44.5</td>\n",
       "      <td>2478</td>\n",
       "      <td>...</td>\n",
       "      <td>2386.0</td>\n",
       "      <td>177</td>\n",
       "      <td>1787</td>\n",
       "      <td>292</td>\n",
       "      <td>1144</td>\n",
       "      <td>204</td>\n",
       "      <td>60</td>\n",
       "      <td>1013</td>\n",
       "      <td>557</td>\n",
       "      <td>122900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3139</th>\n",
       "      <td>0500000US56031</td>\n",
       "      <td>56</td>\n",
       "      <td>031</td>\n",
       "      <td>Platte</td>\n",
       "      <td>County</td>\n",
       "      <td>2084.208</td>\n",
       "      <td>POLYGON ((-104.77417 42.60996, -104.76422 42.6...</td>\n",
       "      <td>56031</td>\n",
       "      <td>47.8</td>\n",
       "      <td>8677</td>\n",
       "      <td>...</td>\n",
       "      <td>8197.0</td>\n",
       "      <td>634</td>\n",
       "      <td>6349</td>\n",
       "      <td>659</td>\n",
       "      <td>4205</td>\n",
       "      <td>892</td>\n",
       "      <td>146</td>\n",
       "      <td>3662</td>\n",
       "      <td>519</td>\n",
       "      <td>133300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3140</th>\n",
       "      <td>0500000US56037</td>\n",
       "      <td>56</td>\n",
       "      <td>037</td>\n",
       "      <td>Sweetwater</td>\n",
       "      <td>County</td>\n",
       "      <td>10426.649</td>\n",
       "      <td>POLYGON ((-109.05008 41.00066, -109.17368 41.0...</td>\n",
       "      <td>56037</td>\n",
       "      <td>32.9</td>\n",
       "      <td>43890</td>\n",
       "      <td>...</td>\n",
       "      <td>40222.0</td>\n",
       "      <td>2680</td>\n",
       "      <td>27584</td>\n",
       "      <td>4212</td>\n",
       "      <td>22670</td>\n",
       "      <td>3428</td>\n",
       "      <td>533</td>\n",
       "      <td>16535</td>\n",
       "      <td>881</td>\n",
       "      <td>174600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3141</th>\n",
       "      <td>0500000US56043</td>\n",
       "      <td>56</td>\n",
       "      <td>043</td>\n",
       "      <td>Washakie</td>\n",
       "      <td>County</td>\n",
       "      <td>2238.549</td>\n",
       "      <td>POLYGON ((-107.12892 43.99455, -107.12797 43.9...</td>\n",
       "      <td>56043</td>\n",
       "      <td>41.6</td>\n",
       "      <td>8425</td>\n",
       "      <td>...</td>\n",
       "      <td>7879.0</td>\n",
       "      <td>520</td>\n",
       "      <td>5647</td>\n",
       "      <td>512</td>\n",
       "      <td>4029</td>\n",
       "      <td>674</td>\n",
       "      <td>158</td>\n",
       "      <td>3439</td>\n",
       "      <td>511</td>\n",
       "      <td>157700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3142</th>\n",
       "      <td>0500000US56045</td>\n",
       "      <td>56</td>\n",
       "      <td>045</td>\n",
       "      <td>Weston</td>\n",
       "      <td>County</td>\n",
       "      <td>2398.089</td>\n",
       "      <td>POLYGON ((-104.05518 43.76163, -104.05514 43.7...</td>\n",
       "      <td>56045</td>\n",
       "      <td>41.7</td>\n",
       "      <td>7152</td>\n",
       "      <td>...</td>\n",
       "      <td>6703.0</td>\n",
       "      <td>438</td>\n",
       "      <td>4962</td>\n",
       "      <td>901</td>\n",
       "      <td>3418</td>\n",
       "      <td>537</td>\n",
       "      <td>106</td>\n",
       "      <td>2958</td>\n",
       "      <td>640</td>\n",
       "      <td>131200.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3143 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              GEO_ID STATE COUNTY        NAME    LSAD  CENSUSAREA  \\\n",
       "0     0500000US01029    01    029    Cleburne  County     560.100   \n",
       "1     0500000US01031    01    031      Coffee  County     678.972   \n",
       "2     0500000US01037    01    037       Coosa  County     650.926   \n",
       "3     0500000US01039    01    039   Covington  County    1030.456   \n",
       "4     0500000US01041    01    041    Crenshaw  County     608.840   \n",
       "...              ...   ...    ...         ...     ...         ...   \n",
       "3138  0500000US56027    56    027    Niobrara  County    2626.037   \n",
       "3139  0500000US56031    56    031      Platte  County    2084.208   \n",
       "3140  0500000US56037    56    037  Sweetwater  County   10426.649   \n",
       "3141  0500000US56043    56    043    Washakie  County    2238.549   \n",
       "3142  0500000US56045    56    045      Weston  County    2398.089   \n",
       "\n",
       "                                               geometry  GEOID  B01002_001E  \\\n",
       "0     POLYGON ((-85.38872 33.91304, -85.38088 33.873...  01029         41.2   \n",
       "1     POLYGON ((-86.03044 31.61894, -86.00408 31.619...  01031         37.8   \n",
       "2     POLYGON ((-86.00928 33.10164, -86.00917 33.090...  01037         44.4   \n",
       "3     POLYGON ((-86.34851 30.99434, -86.35023 30.994...  01039         42.5   \n",
       "4     POLYGON ((-86.14699 31.68045, -86.14711 31.663...  01041         40.3   \n",
       "...                                                 ...    ...          ...   \n",
       "3138  POLYGON ((-104.05298 42.85955, -104.05286 42.7...  56027         44.5   \n",
       "3139  POLYGON ((-104.77417 42.60996, -104.76422 42.6...  56031         47.8   \n",
       "3140  POLYGON ((-109.05008 41.00066, -109.17368 41.0...  56037         32.9   \n",
       "3141  POLYGON ((-107.12892 43.99455, -107.12797 43.9...  56043         41.6   \n",
       "3142  POLYGON ((-104.05518 43.76163, -104.05514 43.7...  56045         41.7   \n",
       "\n",
       "      B03002_001E  ...  B06007_001E  B16010_002E  B16010_001E  C24050_002E  \\\n",
       "0           14905  ...      13958.0         2622        10173          285   \n",
       "1           49966  ...      46580.0         6092        33409          635   \n",
       "2           11305  ...      10762.0         1974         8153          129   \n",
       "3           37803  ...      35540.0         5936        26210          973   \n",
       "4           13971  ...      13069.0         2245         9477          487   \n",
       "...           ...  ...          ...          ...          ...          ...   \n",
       "3138         2478  ...       2386.0          177         1787          292   \n",
       "3139         8677  ...       8197.0          634         6349          659   \n",
       "3140        43890  ...      40222.0         2680        27584         4212   \n",
       "3141         8425  ...       7879.0          520         5647          512   \n",
       "3142         7152  ...       6703.0          438         4962          901   \n",
       "\n",
       "      C24050_001E  C24050_029E  B08201_002E  B08201_001E  B25064_001E  \\\n",
       "0            5868          931          352         5518          566   \n",
       "1           19332         3015         1270        19002          638   \n",
       "2            4354          722          213         4573          573   \n",
       "3           15087         2341          986        14899          560   \n",
       "4            5899          827          344         5648          488   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "3138         1144          204           60         1013          557   \n",
       "3139         4205          892          146         3662          519   \n",
       "3140        22670         3428          533        16535          881   \n",
       "3141         4029          674          158         3439          511   \n",
       "3142         3418          537          106         2958          640   \n",
       "\n",
       "      B25077_001E  \n",
       "0         91300.0  \n",
       "1        126400.0  \n",
       "2         74500.0  \n",
       "3         86700.0  \n",
       "4         70000.0  \n",
       "...           ...  \n",
       "3138     122900.0  \n",
       "3139     133300.0  \n",
       "3140     174600.0  \n",
       "3141     157700.0  \n",
       "3142     131200.0  \n",
       "\n",
       "[3143 rows x 53 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3485a9e1-cb07-49ce-842b-ad47424d1ab2",
   "metadata": {},
   "source": [
    "The largest discrepency between the two different sources of land area data is just over 0.01 square miles and the average difference (amongst those with a difference) is tiny. With such a minor difference between our data and theirs, for our purposes we will consider our data roughly the same as Spielman et al.'s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e53402be-c51e-422b-917b-a49af9d9c568",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step P2\n",
    "# Calculating the variables used in SoVI\n",
    "counties['MEDAGE_ACS'] = counties.B01002_001E\n",
    "counties['BLACK_ACS'] = counties.B03002_004E / (counties.B03002_001E)\n",
    "counties['QNATAM_ACS'] = counties.B03002_005E / (counties.B03002_001E)\n",
    "counties['QASIAN_ACS'] = counties.B03002_006E / (counties.B03002_001E)\n",
    "counties['QHISP_ACS'] = counties.B03002_012E / (counties.B03002_001E)\n",
    "counties['QAGEDEP_ACS'] = (counties.B06001_002E + counties.B09020_001E) / (counties.B01003_001E)\n",
    "counties['QPUNIT_ACS'] = counties.B25008_001E / (counties.B25002_002E)\n",
    "counties['PRENTER_ACS'] = counties.B25003_003E / (counties.B25002_001E)\n",
    "counties['QNRRES_ACS'] = counties.B09020_021E / (counties.B01003_001E)\n",
    "counties['QFEMALE_ACS'] = counties.B01001_026E / (counties.B01003_001E)\n",
    "counties['QFHH_ACS'] = counties.B11001_006E / (counties.B11001_001E)\n",
    "counties['QUNOCCHU_ACS'] = counties.B25002_003E / (counties.B25002_001E)\n",
    "counties['QCVLUN'] = (counties.B23022_025E + counties.B23022_049E) / \\\n",
    "                counties.B23022_001E\n",
    "counties['QPOVTY'] = (counties.B17021_002E) / counties.B17021_001E\n",
    "counties['QMOHO'] = (counties.B25024_010E) / counties.B25024_001E\n",
    "counties['QFEMLBR'] = (counties.C24010_038E) / counties.C24010_001E\n",
    "counties['QSSBEN'] = (counties.B19055_002E) / counties.B19055_001E\n",
    "counties['QFAM'] = (counties.B09002_002E) / counties.B09002_001E\n",
    "counties['QRICH200K'] = (counties.B19001_017E) / counties.B11001_001E\n",
    "counties['PERCAP_ALT'] = counties.B19025_001E / (counties.B25008_001E)\n",
    "counties['QESL_ALT'] = (counties.B06007_005E + counties.B06007_008E) / \\\n",
    "                  counties.B06007_001E\n",
    "counties['QED12LES_ALT'] = (counties.B16010_002E) / counties.B16010_001E \n",
    "counties['QEXTRCT_ALT'] = (counties.C24050_002E) / counties.C24050_001E \n",
    "counties['QSERV_ALT'] = (counties.C24050_029E) / counties.C24050_001E \n",
    "counties['QNOAUTO_ALT'] = (counties.B08201_002E) / counties.B08201_001E \n",
    "counties['MDGRENT_ALT'] = counties.B25064_001E \n",
    "counties['MHSEVAL_ALT'] = counties.B25077_001E \n",
    "counties['POPDENS'] = counties.B01003_001E / (counties.CENSUSAREA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab95227-ba96-4298-bb8a-2e60bfb07d09",
   "metadata": {},
   "source": [
    "As noted before, B25077_001E is missing a data value.\n",
    "We now perform one final check to see if we need to impute anything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "088f0061-ed35-43dc-9617-7a18dc7136c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSAD contains 2 missing value(s).\n",
      "B25077_001E contains 1 missing value(s).\n",
      "QFAM contains 2 missing value(s).\n",
      "MHSEVAL_ALT contains 1 missing value(s).\n"
     ]
    }
   ],
   "source": [
    "# Check for missing data\n",
    "for i in counties.columns:\n",
    "    x = counties[i].isnull().sum()\n",
    "    if x > 0:\n",
    "        print(i, \"contains\", x, \"missing value(s).\")\n",
    "        \n",
    "# Check for infinities\n",
    "counties_num = counties.select_dtypes(include=['int64','float64'])\n",
    "for i in counties_num.columns:\n",
    "    xmin = counties_num[i].min()\n",
    "    xmax = counties_num[i].max()\n",
    "    if xmin == -np.inf:\n",
    "        print(i, \"contains a negative infinity\")\n",
    "    elif xmax == np.inf:\n",
    "        print(i, \"contains a positive infinity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57592b5-e7f0-4e87-8dd5-2ac0de94665d",
   "metadata": {},
   "source": [
    "There are four variables with missing data. LSAD is not used in our analysis, so we may ignore this. B25077_001E and MHSEVAL_ALT are literally identical, so we will ignore B25077_001E and simply impute for MHSEVAL_ALT's one missing value. We also need to impute for QFAM's 2 missing values. We use the same imputation decisions that Spielman et al. employ in their analysis.\n",
    "\n",
    "**Unplanned deviation:** When imputing for MHSEVAL_ALT's missing data, we removed a fair amount of extraneous code that was filling in missing spatial lag data with original data for the county. This was unnecessary because we only needed to impute data for one county and that county had spatial lag data. Also note that Spielman et al.'s method for imputing data for MHSEVAL_ALT is a deviation from Cutter's original methodology, in which she imputed a 0 for any missing value. While this is a deviation from the original SoVI methodology, 0 is an unrealistic median home value, so Spielman et al.'s method for imputation seems like a reasonable improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f54e762-4899-4330-99a5-eeffb246755c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step P3\n",
    "# Replace missing QFAM data with 0\n",
    "counties.QFAM = counties.QFAM.replace([np.inf, -np.inf, np.nan], 0)\n",
    "\n",
    "# Replace missing MHSEVAL_ALT data with its spatial lag\n",
    "\n",
    "# Calculate spatial weights matrix\n",
    "w = lps.weights.Queen.from_dataframe(counties) \n",
    "w.transform = 'R'\n",
    "# Calculate spatial lag\n",
    "counties['MHSEVAL_ALT_LAG'] = lps.weights.lag_spatial(w, counties.MHSEVAL_ALT)\n",
    "# Impute for the missing value\n",
    "counties.MHSEVAL_ALT[np.isnan(counties['MHSEVAL_ALT'])] = counties[[\"MHSEVAL_ALT_LAG\"]][pd.isna(counties['MHSEVAL_ALT'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7b868c-9a1c-465e-be0b-1275d292ada9",
   "metadata": {},
   "source": [
    "The missing data procedure attempts to fill some counties' missing data with the average values of the surrounding counties.\n",
    "This procedure produces a warning about 10 disconnected components and 7 islands in the weights matrix.\n",
    "The cause of this error is areas of the United States that are not contiguous with one another.\n",
    "Fortunately, these areas are not missing any data, and therefore this error does not affect our procedure to fill gaps in missing data.\n",
    "\n",
    "Before adjusting directionality, let us check that all of our derived variables match all of Spielman et al.'s derived variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be79dfe4-ed76-4113-858f-feb85d819d4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import Spielman et al.'s derived variables\n",
    "US_All = pd.read_csv(here(\"data\", \"raw\", \"public\", \"spielman\", \"output\", \"sovi_inputs.csv\"))\n",
    "counties.to_csv(here(path[\"ddpub\"],'counties.csv'))  \n",
    "counties.to_file(here(path[\"ddpub\"],'counties.gpkg'))  \n",
    "counties = pd.read_csv(here(path[\"ddpub\"], \"counties.csv\"), dtype = {'GEOID': object})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617f616b-a51f-4a00-8d50-ba74dffb0873",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select only the relevant columns\n",
    "\n",
    "# Attribute name and expected influence on vulnerability\n",
    "input_names = [['MEDAGE_ACS', 'pos', 'person', 'Median Age'],\n",
    "               ['BLACK_ACS', 'pos', 'person', 'Pop African-American (%)'],\n",
    "               ['QNATAM_ACS', 'pos', 'person', 'Pop Native American (%)'],\n",
    "               ['QASIAN_ACS', 'pos', 'person', 'Pop Asian (%)'],\n",
    "               ['QHISP_ACS', 'pos', 'person', 'Pop Hispanic (%)'],\n",
    "               ['QAGEDEP_ACS', 'pos', 'person', 'Age Dependency (%)'],\n",
    "               ['QPUNIT_ACS', 'pos', 'person', 'Persons Per Housing Unit'],\n",
    "               ['PRENTER_ACS', 'pos', 'hu', 'Rental Housing (%)'],\n",
    "               ['QNRRES_ACS', 'pos', 'person', 'Nursing Home Residents (%)'],\n",
    "               ['QFEMALE_ACS', 'pos', 'person', 'Pop Female (%)'],\n",
    "               ['QFHH_ACS', 'pos', 'hu', 'Female-Headed Households (%)'],\n",
    "               ['QUNOCCHU_ACS', 'pos', 'hu', 'Vacant Housing (%)'],\n",
    "               ['PERCAP_ALT', 'neg', 'person', 'Per-Capita Income'],\n",
    "               ['QESL_ALT', 'pos', 'person', 'English as Second Language (%)'],\n",
    "               ['QCVLUN', 'pos', 'person', 'Unemployment (%)'],\n",
    "               ['QPOVTY', 'pos', 'person', 'Poverty (%)'],\n",
    "               ['QMOHO', 'pos', 'hu', 'Mobile Homes (%)'],\n",
    "               ['QED12LES_ALT', 'pos', 'person',\n",
    "                   'Adults Completed <Grade 12 (%)'],\n",
    "               ['QFEMLBR', 'pos', 'person', 'Female Employment (%)'],\n",
    "               ['QEXTRCT_ALT', 'pos', 'person',\n",
    "                   'Extractive Sector Employment (%)'],\n",
    "               ['QSERV_ALT', 'pos', 'person', 'Service Sector Employment (%)'],\n",
    "               ['QSSBEN', 'pos', 'hu', 'Social Security Income (%)'],\n",
    "               ['QNOAUTO_ALT', 'pos', 'hu', 'No Automobile (%)'],\n",
    "               ['QFAM', 'neg', 'person', 'Children in Married Families (%)'],\n",
    "               ['QRICH200K', 'neg', 'hu', 'Annual Income >$200K (%)'],\n",
    "               ['MDGRENT_ALT', 'neg', 'hu', 'Median Rent'],\n",
    "               ['MHSEVAL_ALT', 'neg', 'hu', 'Median Home Value'],\n",
    "               ['POPDENS', 'pos', 'person', 'Population Density']]\n",
    "\n",
    "# Get attribute names\n",
    "attr_names1 = [j[0] for j in input_names] + ['GEOID']\n",
    "attr_names2 = [j[0] for j in input_names] + ['Geo_FIPS']\n",
    "\n",
    "# Select only the columns needed to compute SoVI\n",
    "counties = counties[attr_names1]\n",
    "US_All = US_All[attr_names2]\n",
    "\n",
    "counties[\"GEOID\"] = \"g\" + counties[\"GEOID\"]\n",
    "counties['stateID'] = counties.GEOID.str.slice(0, 3, 1)\n",
    "attr_names1.remove('GEOID')\n",
    "counties = counties.set_index(counties[\"GEOID\"]).sort_index()\n",
    "\n",
    "US_All['stateID'] = US_All.Geo_FIPS.str.slice(0, 3, 1)\n",
    "attr_names2.remove('Geo_FIPS')\n",
    "US_All = US_All.set_index(US_All[\"Geo_FIPS\"]).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9050a601-a856-4663-94ef-2c0bafe0d0e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "counties.eq(US_All).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa439b5-7e60-48a6-9a3c-a90f400fb6ce",
   "metadata": {},
   "source": [
    "Therre are 3143 observations in the dataset, so it appears that all variables match up perfectly except POPDENS. POPDENS is the one variable that was derived from the land area, so this was to be expected. Let us confirm that the differences between the two datasets are minor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a511f6-0635-4e8f-a354-f67fb7e788b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "diff = (abs(counties[[\"POPDENS\"]] - US_All[[\"POPDENS\"]]))\n",
    "print(\"Maximum difference:\", diff.max()[0], \"\\nAverage difference:\", diff.mean()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a06b0d-9c1e-442a-85a4-cabe5df733cf",
   "metadata": {},
   "source": [
    "With a maximum difference less than 1 and an average difference less than 0.01, our data is sufficiently close to Spielman et al.'s for our purposes.\n",
    "\n",
    "Now we proceed to step P4, switching the directionality of variables as needed in order to ensure that higher values of a variable are associated with higher levels of vulnerability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e94f49-7ca9-47eb-b3c1-03bbb50198f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step P4\n",
    "# Flip signs as needed to ensure that each variable contributes as expected to the final Sovi\n",
    "for name, sign, sample, hrname in input_names:\n",
    "    if sign == 'neg':\n",
    "        counties[name] = -counties[name].values\n",
    "        print(\"Inverting variable:\", name)\n",
    "    elif sign == 'pos':\n",
    "        pass\n",
    "    else:\n",
    "        print(\"problem in flipping signs\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a2aa19-baa0-4f90-9ed3-26375019d6c9",
   "metadata": {},
   "source": [
    "A final step of data transformation will be performed at the beginning of the SoVI model analysis.\n",
    "Each demographic variable will be normalized by calculating its z-score.\n",
    "\n",
    "### Analysis\n",
    "#### Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4844792-5872-4240-af81-14f250b4ba45",
   "metadata": {},
   "source": [
    "Spielman et al. constructed a class to conduct SPSS-style PCA with varimax rotation in python and validated their procedure against Cutter et al.'s SPSS workflow used to calculate SoVI.\n",
    "Below I include a workflow diagram that shows, without too much detail, the main operations and important outputs of their SPSS_PCA class. \n",
    "After that, I have included their relevant code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e542db85-0392-44c4-894f-421948441ad7",
   "metadata": {},
   "source": [
    "![PCA Workflow](../../results/figures/workflow_PCA.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1182438-bf82-4ea1-abcd-a485ac631888",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SPSS_PCA:\n",
    "\t'''\n",
    "\tA class that integrates most (all?) of the assumptions SPSS imbeds in their\n",
    "    implimnetation of principal components analysis (PCA), which can be found in\n",
    "    thier GUI under Analyze > Dimension Reduction > Factor. This class is not\n",
    "\tintended to be a full blown recreation of the SPSS Factor Analysis GUI, but\n",
    "\tit does replicate (possibly) the most common use cases. Note that this class\n",
    "\twill not produce exactly the same results as SPSS, probably due to differences\n",
    "\tin how eigenvectors/eigenvalues and/or singular values are computed. However,\n",
    "\tthis class does seem to get all the signs to match, which is not really necessary\n",
    "\tbut kinda nice. Most of the approach came from the official SPSS documentation.\n",
    "\n",
    "\tReferences\n",
    "\t----------\n",
    "\tftp://public.dhe.ibm.com/software/analytics/spss/documentation/statistics/20.0/en/client/Manuals/IBM_SPSS_Statistics_Algorithms.pdf\n",
    "\thttp://spssx-discussion.1045642.n5.nabble.com/Interpretation-of-PCA-td1074350.html\n",
    "\thttp://mdp-toolkit.sourceforge.net/api/mdp.nodes.WhiteningNode-class.html\n",
    "\thttps://github.com/mdp-toolkit/mdp-toolkit/blob/master/mdp/nodes/pca_nodes.py\n",
    "\n",
    "\tParameters\n",
    "\t----------\n",
    "\tinputs:  numpy array\n",
    "\t\t\t n x k numpy array; n observations and k variables on each observation\n",
    "\treduce:  boolean (default=False)\n",
    "\t\t\t If True, then use eigenvalues to determine which factors to keep; all\n",
    "\t\t\t results will be based on just these factors. If False use all factors.\n",
    "\tmin_eig: float (default=1.0)\n",
    "\t\t\t If reduce=True, then keep all factors with an eigenvalue greater than\n",
    "\t\t\t min_eig. SPSS default is 1.0. If reduce=False, then min_eig is ignored.\n",
    "\tvarimax: boolean (default=False)\n",
    "\t\t\t If True, then apply a varimax rotation to the results. If False, then\n",
    "\t\t\t return the unrotated results only.\n",
    "\n",
    "\tAttributes\n",
    "\t----------\n",
    "\tz_inputs:\tnumpy array\n",
    "\t\t\t\tz-scores of the input array.\n",
    "\tcomp_mat:\tnumpy array\n",
    "\t\t\t\tComponent matrix (a.k.a, \"loadings\").\n",
    "\tscores:\t\tnumpy array\n",
    "\t\t\t\tNew uncorrelated vectors associated with each observation.\n",
    "\teigenvals_all:\tnumpy array\n",
    "\t\t\t\tEigenvalues associated with each factor.\n",
    "\teigenvals:\tnumpy array\n",
    "\t\t\t\tSubset of eigenvalues_all reflecting only those that meet the\n",
    "\t\t\t\tcriterion defined by parameters reduce and min_eig.\n",
    "\tweights:    numpy array\n",
    "\t\t\t\tValues applied to the input data (after z-scores) to get the PCA\n",
    "\t\t\t\tscores. \"Component score coefficient matrix\" in SPSS or\n",
    "\t\t\t\t\"projection matrix\" in the MDP library.\n",
    "\tcomms: \t\tnumpy array\n",
    "\t\t\t\tCommunalities\n",
    "\tsum_sq_load: numpy array\n",
    "\t\t\t\t Sum of squared loadings.\n",
    "\tcomp_mat_rot: numpy array or None\n",
    "\t\t\t\t  Component matrix after rotation. Ordered from highest to lowest\n",
    "\t\t\t\t  variance explained based on sum_sq_load_rot. None if varimax=False.\n",
    "\tscores_rot:\tnumpy array or None\n",
    "\t\t\t\tUncorrelated vectors associated with each observation, after\n",
    "\t\t\t\trotation. None if varimax=False.\n",
    "\tweights_rot: numpy array or None\n",
    "\t\t\t\tRotated values applied to the input data (after z-scores) to get\n",
    "\t\t\t\tthe PCA\tscores. None if varimax=False.\n",
    "\tsum_sq_load_rot: numpy array or None\n",
    "\t\t\t\t Sum of squared loadings for rotated results. None if\n",
    "\t\t\t\t varimax=False.\n",
    "\n",
    "\t'''\n",
    "\n",
    "\tdef __init__(self, inputs, reduce=False, min_eig=1.0, varimax=False):\n",
    "        # Step S1\n",
    "\t\tz_inputs = ZSCORE(inputs)  # seems necessary for SPSS \"correlation matrix\" setting (their default)\n",
    "        \n",
    "        # The rest is step S2\n",
    "\t\t# run base SPSS-style PCA to get all eigenvalues\n",
    "\t\tpca_node = MDP.nodes.WhiteningNode()  # settings for the PCA\n",
    "\t\tscores = pca_node.execute(z_inputs)  # base run PCA\n",
    "\t\teigenvalues_all = pca_node.d   # rename PCA results\n",
    "\n",
    "\t\t# run SPSS-style PCA based on user settings\n",
    "\t\tpca_node = MDP.nodes.WhiteningNode(reduce=reduce, var_abs=min_eig)  # settings for the PCA\n",
    "\t\tscores = pca_node.execute(z_inputs)  # run PCA  (these have mean=0, std_dev=1)\n",
    "\t\tweights = pca_node.v  # rename PCA results (these might be a transformation of the eigenvectors)\n",
    "\t\teigenvalues = pca_node.d   # rename PCA results\n",
    "\t\tcomponent_matrix = weights * eigenvalues  # compute the loadings\n",
    "\t\tcomponent_matrix = self._reflect(component_matrix)   # get signs to match SPSS\n",
    "\t\tcommunalities = (component_matrix**2).sum(1)   # compute the communalities\n",
    "\t\tsum_sq_loadings = (component_matrix**2).sum(0) # note that this is the same as eigenvalues\n",
    "\t\tweights_reflected = component_matrix/eigenvalues  # get signs to match SPSS\n",
    "\t\tscores_reflected = np.dot(z_inputs, weights_reflected)  # note that abs(scores)=abs(scores_reflected)\n",
    "\n",
    "\t\tif varimax:\n",
    "\t\t\t# SPSS-style varimax rotation prep\n",
    "\t\t\tc_normalizer = 1. / MDP.numx.sqrt(communalities)  # used to normalize inputs to varimax\n",
    "\t\t\tc_normalizer.shape = (component_matrix.shape[0],1)  # reshape to vectorize normalization\n",
    "\t\t\tcm_normalized = c_normalizer * component_matrix  # normalize component matrix for varimax\n",
    "\n",
    "\t\t\t# varimax rotation\n",
    "\t\t\tcm_normalized_varimax = self._varimax(cm_normalized)  # run varimax\n",
    "\t\t\tc_normalizer2 = MDP.numx.sqrt(communalities)  # used to denormalize varimax output\n",
    "\t\t\tc_normalizer2.shape = (component_matrix.shape[0],1)  # reshape to vectorize denormalization\n",
    "\t\t\tcm_varimax = c_normalizer2 * cm_normalized_varimax  # denormalize varimax output\n",
    "\n",
    "\t\t\t# reorder varimax component matrix\n",
    "\t\t\tsorter = (cm_varimax**2).sum(0)  # base the ordering on sum of squared loadings\n",
    "\t\t\tsorter = zip(sorter.tolist(), range(sorter.shape[0]))  # add index to denote current order\n",
    "\t\t\tsorter = sorted(sorter, key=itemgetter(0), reverse=True)  # sort from largest to smallest\n",
    "\t\t\tsum_sq_loadings_varimax, reorderer = zip(*sorter)  # unzip the sorted list\n",
    "\t\t\tsum_sq_loadings_varimax = np.array(sum_sq_loadings_varimax)  # convert to array\n",
    "\t\t\tcm_varimax = cm_varimax[:,reorderer]  # reorder component matrix\n",
    "\n",
    "\t\t\t# varimax scores\n",
    "\t\t\tcm_varimax_reflected = self._reflect(cm_varimax)  # get signs to match SPSS\n",
    "\t\t\tvarimax_weights = np.dot(cm_varimax_reflected,\n",
    "\t\t\t\t\t\t\t  np.linalg.inv(np.dot(cm_varimax_reflected.T,\n",
    "\t\t\t\t\t\t\t  cm_varimax_reflected))) # CM(CM'CM)^-1\n",
    "\t\t\tscores_varimax = np.dot(z_inputs, varimax_weights)\n",
    "\t\telse:\n",
    "\t\t\tcomp_mat_rot = None\n",
    "\t\t\tscores_rot = None\n",
    "\t\t\tweights_rot = None\n",
    "\n",
    "\t\t# assign output variables\n",
    "\t\tself.z_inputs = z_inputs\n",
    "\t\tself.scores = scores_reflected\n",
    "\t\tself.comp_mat = component_matrix\n",
    "\t\tself.eigenvals_all = eigenvalues_all\n",
    "\t\tself.eigenvals = eigenvalues\n",
    "\t\tself.weights = weights_reflected\n",
    "\t\tself.comms = communalities\n",
    "\t\tself.sum_sq_load = sum_sq_loadings\n",
    "\t\tself.comp_mat_rot = cm_varimax_reflected\n",
    "\t\tself.scores_rot = scores_varimax # PCA scores output\n",
    "\t\tself.weights_rot = varimax_weights # PCA weights output\n",
    "\t\tself.sum_sq_load_rot = sum_sq_loadings_varimax\n",
    "\n",
    "\tdef _reflect(self, cm):\n",
    "\t\t# reflect factors with negative sums; SPSS default\n",
    "\t\tcm = copy.deepcopy(cm)\n",
    "\t\treflector = cm.sum(0)\n",
    "\t\tfor column, measure in enumerate(reflector):\n",
    "\t\t\tif measure < 0:\n",
    "\t\t\t\tcm[:,column] = -cm[:,column]\n",
    "\t\treturn cm\n",
    "\n",
    "\tdef _varimax(self, Phi, gamma = 1.0, q = 100, tol = 1e-6):\n",
    "\t\t# downloaded from http://en.wikipedia.org/wiki/Talk%3aVarimax_rotation\n",
    "\t\t# also here http://stackoverflow.com/questions/17628589/perform-varimax-rotation-in-python-using-numpy\n",
    "\t\tp,k = Phi.shape\n",
    "\t\tR = np.eye(k)\n",
    "\t\td=0\n",
    "\t\tfor i in range(q):\n",
    "\t\t\td_old = d\n",
    "\t\t\tLambda = np.dot(Phi, R)\n",
    "\t\t\tu,s,vh = np.linalg.svd(np.dot(Phi.T,np.asarray(Lambda)**3 - (gamma/p) *\n",
    "\t\t\t\t\t\t\tnp.dot(Lambda, np.diag(np.diag(np.dot(Lambda.T,Lambda))))))\n",
    "\t\t\tR = np.dot(u,vh)\n",
    "\t\t\td = np.sum(s)\n",
    "\t\t\tif d_old!=0 and d/d_old < 1 + tol:\n",
    "\t\t\t\tbreak\n",
    "\t\treturn np.dot(Phi, R)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a10f795-7c15-4ac7-abd5-207b72968706",
   "metadata": {},
   "source": [
    "#### Basic set-up for storing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb063798-04fc-43b4-9b97-7b1d14384d94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build FEMA subRegions Dict values= state ID's\n",
    "FEMA_subs = dict()\n",
    "FEMA_subs['FEMA_1'] = ['g23g33g25', 'g50', 'g09', 'g44']\n",
    "FEMA_subs['FEMA_2'] = ['g36', 'g34']\n",
    "FEMA_subs['FEMA_3'] = ['g42', 'g10', 'g11', 'g24', 'g51', 'g54']\n",
    "FEMA_subs['FEMA_4'] = ['g21', 'g47', 'g37', 'g28', 'g01', 'g13', 'g45', 'g12']\n",
    "FEMA_subs['FEMA_5'] = ['g27', 'g55', 'g26', 'g17', 'g18', 'g39']\n",
    "FEMA_subs['FEMA_6'] = ['g35', 'g48', 'g40', 'g05', 'g22']\n",
    "FEMA_subs['FEMA_7'] = ['g31', 'g19', 'g20', 'g29']\n",
    "FEMA_subs['FEMA_8'] = ['g30', 'g38', 'g56', 'g46', 'g49', 'g08']\n",
    "FEMA_subs['FEMA_9'] = ['g06', 'g32', 'g04']\n",
    "FEMA_subs['FEMA_10'] = ['g53', 'g41', 'g16']\n",
    "\n",
    "####################################\n",
    "# DataFrames to hold US, FEMA region, and state level results\n",
    "####################################\n",
    "\n",
    "# Dict to hold variable loadings\n",
    "# key will be [USA, Fema_region, stateid] depending on level of analysis\n",
    "varContrib = {}\n",
    "\n",
    "# National Score\n",
    "US_Sovi_Score = pd.DataFrame(index=counties.GEOID,\n",
    "                             columns=['sovi', 'rank'])\n",
    "\n",
    "# In the FEMA_Region_Sovi_Score data frame ranks are BY FEMA REGION.\n",
    "# The data frame holds both the SOVI score and the county rank\n",
    "# This means that there should be 10 counties with rank 1 (one for each\n",
    "# FEMA Region)\n",
    "FEMA_Region_Sovi_Score = pd.DataFrame(index=counties.GEOID,\n",
    "                                      columns=['sovi', 'rank', 'fema_region'])\n",
    "\n",
    "# Create New England conglomerate of states\n",
    "# These are the FIPS codes for the states with the letter \"g\" appended\n",
    "counties.loc[counties.stateID.isin(['g23', 'g33', 'g25']), 'stateID'] = 'g23g33g25'\n",
    "\n",
    "# These are the states in the state level analysis\n",
    "stateList = ['g23g33g25', 'g36', 'g51', 'g13',\n",
    "             'g17', 'g48', 'g29', 'g46', 'g06', 'g16']\n",
    "\n",
    "# In the State_Sovi_Score data frame ranks are BY STATE.\n",
    "# The data frame holds both the SOVI score and the county rank\n",
    "# This means that there should be 10 counties with rank 1 (one for each\n",
    "# state in stateList)\n",
    "State_Sovi_Score = pd.DataFrame(\n",
    "    index=counties.index[counties.stateID.isin(stateList)],\n",
    "    columns=['sovi', 'rank', 'state_id'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b133f6-632a-4091-bc21-d0c3df5ff21e",
   "metadata": {},
   "source": [
    "#### Calculating SoVI\n",
    "At this stage, we seek to calculate the SoVI ranks and variable weightings on the national, FEMA region, and state-level spatial extents. Below is a workflow for calculating SoVI, followed by the code for each spatial extent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfffdd8-2d0d-4929-9aa7-fbc3737380c1",
   "metadata": {},
   "source": [
    "![National SoVI Workflow](../../results/figures/workflow_SoVI.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d6a547-b9c0-4b15-ba0e-e621db2c1243",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#######################\n",
    "# Compute National SoVI\n",
    "#######################\n",
    "# compute SoVI\n",
    "# Step M2\n",
    "inputData = counties.drop(['GEOID', 'stateID'], axis=1, inplace=False)\n",
    "\n",
    "# Step M3\n",
    "inputData_array = inputData.values  # Convert DataFrame to NumPy array\n",
    "\n",
    "# Step M4\n",
    "pca = SPSS_PCA(inputData_array, reduce=True, varimax=True)\n",
    "\n",
    "# Step M5\n",
    "sovi_actual_us = pca.scores_rot.sum(1)\n",
    "\n",
    "# Step M6\n",
    "sovi_actual_us = pd.DataFrame(\n",
    "    sovi_actual_us, index=counties.GEOID, columns=['sovi'])\n",
    "\n",
    "# Step M8\n",
    "sovi_actual_us['rank'] = sovi_actual_us.rank(\n",
    "    method='average', ascending=False)\n",
    "US_Sovi_Score.update(sovi_actual_us)\n",
    "\n",
    "# Step M9\n",
    "attrib_contribution_us = pca.weights_rot.sum(1)\n",
    "varContrib['USA'] = zip(attr_names1, attrib_contribution_us.tolist()) # Generate dictionary for all net loadings by variable for US\n",
    "\n",
    "# Quick check of ranks: max should equal number of counties in US\n",
    "try:\n",
    "    US_Sovi_Score['rank'].max() == len(counties)\n",
    "except:\n",
    "    print(\"error in ranking check\")\n",
    "    raise\n",
    "\n",
    "# cleanup\n",
    "del inputData\n",
    "# del inputData_norm\n",
    "del sovi_actual_us\n",
    "del attrib_contribution_us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4988115b-20fb-4f75-ba31-1b114329dc20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "# Compute FEMA Region SoVI\n",
    "###########################\n",
    "for i in FEMA_subs:\n",
    "\n",
    "    # Step M1: Subset FEMA subregion\n",
    "    FEMARegionData = counties[counties['stateID'].isin(FEMA_subs[i])]\n",
    "\n",
    "    # Step M2\n",
    "    inputData = FEMARegionData.drop(\n",
    "        ['GEOID', 'stateID'], axis=1, inplace=False)\n",
    "    \n",
    "    # Step M3\n",
    "    inputData_array = inputData.values  # Convert DataFrame to NumPy array\n",
    "    \n",
    "    # Step M4\n",
    "    pca = SPSS_PCA(inputData_array, reduce=True, varimax=True)\n",
    "    \n",
    "    # Step M5\n",
    "    sovi_actual_fema = pca.scores_rot.sum(1)\n",
    "\n",
    "    # Step M6\n",
    "    sovi_actual_fema = pd.DataFrame(\n",
    "        sovi_actual_fema, index=FEMARegionData.index, columns=['sovi'])\n",
    "    \n",
    "    # Step M7\n",
    "    sovi_actual_fema['fema_region'] = i\n",
    "    \n",
    "    # Step M8\n",
    "    sovi_actual_fema['rank'] = sovi_actual_fema['sovi'].rank(\n",
    "        method='average', ascending=False)\n",
    "    FEMA_Region_Sovi_Score.update(sovi_actual_fema)\n",
    "    \n",
    "    # Step M9\n",
    "    attrib_contribution_fema = pca.weights_rot.sum(1)\n",
    "    varContrib[i] = zip(attr_names1, attrib_contribution_fema.tolist())\n",
    "\n",
    "# cleanup\n",
    "del FEMARegionData\n",
    "del inputData\n",
    "del sovi_actual_fema\n",
    "del attrib_contribution_fema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a464eb-91cc-4956-9751-97e862b368dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "############################\n",
    "# Compute State Level SoVI\n",
    "############################\n",
    "for st in stateList:\n",
    "    \n",
    "    # Step M1: Subset FEMA subregion\n",
    "    stateData = counties[counties.stateID == st]\n",
    "\n",
    "    # Step M2\n",
    "    inputData = stateData.drop(['GEOID', 'stateID'], axis=1, inplace=False)\n",
    "    \n",
    "    # Step M3\n",
    "    inputData_array = inputData.values  # Convert DataFrame to NumPy array\n",
    "    \n",
    "    # Step M4\n",
    "    pca = SPSS_PCA(inputData_array, reduce=True, varimax=True)\n",
    "    \n",
    "    # Step M5\n",
    "    sovi_actual = pca.scores_rot.sum(1)\n",
    "    \n",
    "    # Step M6\n",
    "    sovi_actual = pd.DataFrame(\n",
    "        sovi_actual, index=stateData.index, columns=['sovi'])\n",
    "    \n",
    "    # Step M7\n",
    "    sovi_actual['state_id'] = st\n",
    "    \n",
    "    # Step M8\n",
    "    sovi_actual['rank'] = sovi_actual['sovi'].rank(\n",
    "        method='average', ascending=False)\n",
    "    State_Sovi_Score.update(sovi_actual)\n",
    "    \n",
    "    # Step M9\n",
    "    attrib_contribution = pca.weights_rot.sum(1)\n",
    "    varContrib[st] = zip(attr_names1, attrib_contribution.tolist())\n",
    "\n",
    "# cleanup\n",
    "del stateData\n",
    "del inputData\n",
    "del sovi_actual\n",
    "del attrib_contribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb65294-7d26-4764-aa0b-1a1782cafca8",
   "metadata": {},
   "source": [
    "#### Internal consistency analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289d245e-b2af-4e37-8540-613ed8a65c0e",
   "metadata": {},
   "source": [
    "Now that we have generated the SoVI scores for the 21 different models, we turn to our analysis of internal consistency.\n",
    "\n",
    "This analysis checks for consistent SoVI rankings of counties in a region of interest (a state or group of small states) through three versions of a SoVI model, each using a different geographic extent for input data. \n",
    "Those extents are: 1) all counties in the country, 2) all the counties in a FEMA region, and 3) all counties in a single state or group of small states.\n",
    "The SoVI scores for the counties in the region of interest are selected and ranked.\n",
    "The agreement between the three sets of rankings is calculated using the Spearman's Rho rank correlation coefficient.\n",
    "If the model is internally consistent, one could expect a nearly perfect positive rank correlation close to 1, implying that counties have similar levels of social vulnerability *vis a vis* one another in the region of interest, regardless of how much extraneous information from other counties in the FEMA region or from the whole United States has been included in the SoVI model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9866486e-ecb3-4762-91df-bf0517565ceb",
   "metadata": {},
   "source": [
    "![Internal Consistency Workflow](../../results/figures/workflow_internal_consistency.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1b4cbc-e946-4842-a678-805d969f30d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##########################################################################\n",
    "# Ranks w/ Geographic Extent\n",
    "# For each county rank within state for US, state, and fema_region sovis\n",
    "##########################################################################\n",
    "\n",
    "# Step IC1: Create an empty DataFrame with a column for each SoVI spatial extent and an index of each county FIPS code in the selected state(s) of the 10 FEMA regions\n",
    "county_in_state_rank = pd.DataFrame(index=State_Sovi_Score.index,\n",
    "                                    columns=['state_sovi_rank', 'fema_region_sovi_rank', 'us_sovi_rank'])\n",
    "\n",
    "for st in stateList:\n",
    "    if st == 'g23g33g25':\n",
    "       # Step IC2: Select the index and SoVI scores from national model for Maine, New Hampshire, and Massachusetts\n",
    "        st_cty_scores1 = US_Sovi_Score.loc[['g23' in s for s in US_Sovi_Score.index], 'sovi']\n",
    "        st_cty_scores2 = US_Sovi_Score.loc[['g33' in s for s in US_Sovi_Score.index], 'sovi']\n",
    "        st_cty_scores3 = US_Sovi_Score.loc[['g25' in s for s in US_Sovi_Score.index], 'sovi']\n",
    "        st_cty_scores = pd.concat([st_cty_scores1, st_cty_scores2, st_cty_scores3])\n",
    "\n",
    "        # Step IC3: Re-rank the national SoVI scores but just for the counties in the relevant states\n",
    "        county_in_state_rank.loc[st_cty_scores.index, 'us_sovi_rank'] = st_cty_scores.rank(method='average', ascending=False)\n",
    "\n",
    "        # Step IC4: Select the index and SoVI scores from FEMA model for Maine, New Hampshire, and Massachusetts\n",
    "        st_cty_scores1 = FEMA_Region_Sovi_Score.loc[['g23' in s for s in FEMA_Region_Sovi_Score.index], 'sovi']\n",
    "        st_cty_scores2 = FEMA_Region_Sovi_Score.loc[['g33' in s for s in FEMA_Region_Sovi_Score.index], 'sovi']\n",
    "        st_cty_scores3 = FEMA_Region_Sovi_Score.loc[['g25' in s for s in FEMA_Region_Sovi_Score.index], 'sovi']\n",
    "        st_cty_scores = pd.concat([st_cty_scores1, st_cty_scores2, st_cty_scores3])\n",
    "\n",
    "        # Step IC5: Re-rank the FEMA SoVI scores but just for the counties in the relevant states\n",
    "        county_in_state_rank.loc[st_cty_scores.index, 'fema_region_sovi_rank'] = st_cty_scores.rank(method='average', ascending=False)\n",
    "\n",
    "        # Step IC6: Pull the state-only SoVI ranks into the same dataframe as the other data\n",
    "        county_in_state_rank.loc[st_cty_scores.index, 'state_sovi_rank'] = State_Sovi_Score.loc[State_Sovi_Score['state_id'] == 'g23g33g25', 'rank']\n",
    "\n",
    "    else:\n",
    "        # Step IC2: select the index and SoVI scores from national model for the relevant state\n",
    "        st_cty_scores = US_Sovi_Score.loc[[st in s for s in US_Sovi_Score.index], 'sovi']\n",
    "\n",
    "        # Step IC3: Re-rank the national SoVI scores but just for the counties in the relevant state\n",
    "        county_in_state_rank.loc[st_cty_scores.index, 'us_sovi_rank'] = st_cty_scores.rank(method='average', ascending=False)\n",
    "\n",
    "        # Step IC4: Select the index and SoVI scores from FEMA model for the relevant state\n",
    "        st_cty_scores = FEMA_Region_Sovi_Score.loc[[st in s for s in FEMA_Region_Sovi_Score.index], 'sovi']\n",
    "\n",
    "        # Step IC5: Re-rank the FEMA SoVI scores but just for the counties in the relevant state\n",
    "        county_in_state_rank.loc[st_cty_scores.index, 'fema_region_sovi_rank'] = st_cty_scores.rank(method='average', ascending=False)\n",
    "\n",
    "        # Step IC6: Pull the state-only SoVI ranks into the same dataframe as the other data\n",
    "        county_in_state_rank.loc[st_cty_scores.index, 'state_sovi_rank'] = State_Sovi_Score.loc[State_Sovi_Score['state_id'] == st, 'rank']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70b6e1c-4a53-436c-9433-e3034a7b4ed1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "######################\n",
    "# CORRELATIONS\n",
    "######################\n",
    "\n",
    "# Step IC 7: Create an empty DataFrame to hold Spearman test results\n",
    "state_corrs = pd.DataFrame(index = stateList, columns = ['spearman_r_st_fema', 'pvalue_st_fema', 'spearman_r_st_us', 'pvalue_st_us'])\n",
    "\n",
    "for st in stateList:\n",
    "  if st == 'g23g33g25':\n",
    "    # Step IC8: Calculate spearman correlation between state and FEMA, state and national\n",
    "    multi_state_data_tmp1 = county_in_state_rank.loc[['g23' in s for s in county_in_state_rank.index], ]\n",
    "    multi_state_data_tmp2 = county_in_state_rank.loc[['g25' in s for s in county_in_state_rank.index], ]\n",
    "    multi_state_data_tmp3 = county_in_state_rank.loc[['g33' in s for s in county_in_state_rank.index], ]\n",
    "    multi_state_data_tmp = pd.concat([multi_state_data_tmp1, multi_state_data_tmp2, multi_state_data_tmp3])\n",
    "    st_fema_spearman = spearmanr(multi_state_data_tmp[['state_sovi_rank', 'fema_region_sovi_rank']])\n",
    "    st_us_spearman = spearmanr(multi_state_data_tmp[['state_sovi_rank', 'us_sovi_rank']])\n",
    "    state_corrs.loc['g23g33g25', ] = [st_fema_spearman[0], st_fema_spearman[1], st_us_spearman[0], st_us_spearman[1]]\n",
    "  else:\n",
    "    # Step IC8: Calculate spearman correlation between state and FEMA, state and national\n",
    "    st_fema_spearman = spearmanr(county_in_state_rank.loc[[st in s for s in county_in_state_rank.index], ['state_sovi_rank', 'fema_region_sovi_rank']])\n",
    "    st_us_spearman = spearmanr(county_in_state_rank.loc[[st in s for s in county_in_state_rank.index], ['state_sovi_rank', 'us_sovi_rank']])\n",
    "    state_corrs.loc[st, ] = [st_fema_spearman[0], st_fema_spearman[1], st_us_spearman[0], st_us_spearman[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8a7d5d-0eb1-4dbc-86a2-fe7550de80b3",
   "metadata": {},
   "source": [
    "#### Theoretical consistency analysis\n",
    "\n",
    "Finally, we investigate the questions surrounding theoretical consistency.\n",
    "\n",
    "This analysis checks for consistent signs and ranks of variables across the same 21 models that were used in the internal consistency analysis.\n",
    "To evaluate the signs and ranks of variables, we sum all components together, producing one vector for each model containing the net effect of each variable on the SoVI score.\n",
    "Theoretical consistency is indicated by little variation amongst all models in the signs and magnitudes of variable weights.\n",
    "Theoretical inconsistency is indicated by substantial variation in the signs and weights of variables and by disagreement between a variable's theoretical influence and modeled influence on vulnerability.\n",
    "\n",
    "**Unplanned deviation:** we were unable to find all of the code for this part the analysis, so we wrote much of this code ourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb20f01a-b94c-48b1-8f7a-cd468479057d",
   "metadata": {},
   "source": [
    "![Theoretical Consistency Workflow](../../results/figures/workflow_theoretical_consistency.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141d276c-faef-4824-af07-b4bbebf50b93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step TC1: Create a DataFrame to hold variable contributions values\n",
    "variable_contributions = pd.DataFrame(index=attr_names1)\n",
    "\n",
    "# Step TC2: Add variable contributions values to DataFrame\n",
    "for area in varContrib.keys():\n",
    "    variable_contributions[area] = [x for i, x in varContrib[area]]\n",
    "    \n",
    "# Step TC3: For all SoVI models, rank variables from the greatest to the least magnitudes\n",
    "rankContrib = abs(variable_contributions).apply(rankdata, axis=0, method='average')\n",
    "rankContrib = (28-rankContrib) + 1\n",
    "\n",
    "# Step TC4: Sort variable rankings according to national model's most to least important\n",
    "rankContrib = rankContrib.sort_values(\"USA\", ascending = True).reset_index()\n",
    "rankContrib.index = rankContrib[\"index\"]\n",
    "rankContrib = rankContrib.drop(columns = [\"index\"])\n",
    "\n",
    "# Step TC5: Calculate summary statistics for each variable\n",
    "summary_stats = pd.DataFrame( {\"Min\": rankContrib.min(axis = 1).round(),\n",
    "                               \"Max\": rankContrib.max(axis = 1).round(),\n",
    "                               \"Range\": rankContrib.max(axis = 1) - rankContrib.min(axis = 1).round(),\n",
    "                               \"Average\": rankContrib.mean(axis = 1).round(2)\n",
    "                              } )\n",
    "\n",
    "# Step TC6: determine signs of USA model\n",
    "def pos_neg(x):\n",
    "    if x > 0:\n",
    "        return \"+\"\n",
    "    else:\n",
    "        return \"-\"\n",
    "usa = variable_contributions[\"USA\"].apply(pos_neg)\n",
    "\n",
    "# Step TC7: Determine all positive/negatives\n",
    "reversals_adj = variable_contributions < 0\n",
    "\n",
    "# Step TC8: Separate data \n",
    "reference = reversals_adj[[\"USA\"]]\n",
    "other_vars = reversals_adj.drop(columns = [\"USA\"])\n",
    "\n",
    "# Step TC9: Determine all reversals from expected sign\n",
    "for i in range(len(other_vars.columns)):\n",
    "    other_vars.iloc[:, i] = reversals_adj[\"USA\"].eq(other_vars.iloc[:, i]).eq(False)\n",
    "    \n",
    "# Step TC10: calculate reversals\n",
    "reversal_sum = pd.DataFrame( {\"Reversals\": other_vars.sum(axis = 1)} )\n",
    "\n",
    "# Step TC11: Join data\n",
    "summary_stats = summary_stats.merge(reversal_sum, left_index = True, right_index = True)\n",
    "\n",
    "# Step TC12: Join data\n",
    "summary_stats = summary_stats.merge(usa, left_index = True, right_index = True)\n",
    "\n",
    "# Step TC13: Change index labels to reflect any changes prior to SoVI calculation\n",
    "for name, sign, sample, hrname in input_names:\n",
    "    if sign == 'neg':\n",
    "        summary_stats = summary_stats.rename(index={name: '- '+ name})\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "# Table aesthetics edits\n",
    "summary_stats = summary_stats.rename(columns={\"USA\": \"National Model\"})\n",
    "summary_stats = summary_stats.loc[:,['National Model', 'Reversals', 'Min', 'Average', 'Max', 'Range']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f19ca9-595d-45a9-ac98-fe8eee1481f1",
   "metadata": {},
   "source": [
    "We made a different choice than Spielman et al. in this table. Since we adjusted all variables such that larger values are theoretically associated with a higher degree of vulnerability before calculating SoVI, we would expect all outputs to be positive. In Spielman et al.'s \"expected contribution\" column, some signs were negative -- they recorded the directionality we would expect of the variables if they had made no adjustments before calculating SoVI. This leads to a misleading table because their \"original contribution\" column displays the signs of the model output including prior adjustments to directionaity, but the \"expected contribution\" column displays the signs if the model had not adjusted directionality. To make their \"expected contribution\" column consistent with their \"original contribution\" column, they would need all of the signs to be positive in the \"expected contribution\" column. We choose to simply not include an \"expected contribution\" column since there would be no variation within it anyway. Additionally, we add a negative sign in front of any variables that we changed the directionality of for the sake of clarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6728e8cc-e194-4098-82dd-c04ceeaa7a39",
   "metadata": {},
   "source": [
    "#### Save analysis results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d1cff8-9673-4a50-9258-d8106d68dab7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#####################################################\n",
    "# OUTPUT TABLES\n",
    "#####################################################\n",
    "US_Sovi_Score.to_csv( here(path[\"ddpub\"], 'US_Sovi_Score.csv') )\n",
    "\n",
    "# In the FEMA_Region_Sovi_Score data frame ranks are BY FEMA REGION.\n",
    "# The data frame holds both the SOVI score and the county rank\n",
    "# This means that there should be 10 counties with rank 1 (one for each\n",
    "# FEMA Region)\n",
    "FEMA_Region_Sovi_Score.to_csv( here(path[\"ddpub\"], 'FEMA_Region_Sovi_Score.csv') )\n",
    "\n",
    "# In the State_Sovi_Score data frame ranks are BY STATE.\n",
    "# The data frame holds both the SOVI score and the county rank\n",
    "# This means that there should be 10 counties with rank 1 (one for each\n",
    "# state in stateList)\n",
    "State_Sovi_Score.to_csv( here(path[\"ddpub\"], 'State_Sovi_Score.csv') )\n",
    "\n",
    "# County rank within state for US, state, and fema_region sovis\n",
    "county_in_state_rank.to_csv( here(path[\"ddpub\"], 'County_in_State_Rank.csv') )\n",
    "\n",
    "# Variable contributions for sovis at all geographic extents\n",
    "variable_contributions.to_csv( here(path[\"ddpub\"], 'variable_contributions.csv') )\n",
    "\n",
    "# Correlation of ranks\n",
    "state_corrs.to_csv( here(path[\"ddpub\"], 'state_fema_us_rank_correlations.csv') )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ad915f-d24f-4ee7-ba5f-03b5fc33f346",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf6bfd7-cfe7-4649-9e28-f9ce459204c9",
   "metadata": {},
   "source": [
    "### Rpr-H1\n",
    "\n",
    "First, we tested RPr-H1, that reproduced SoVI model scores for each county are not identical to the original study SoVI model scores for each county for each of the 21 SoVI models.\n",
    "\n",
    "We define a function, check_it to check equivalency of the original output files to our reproduced output files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec318f0-eaa0-4bf4-868c-7e81a2e25419",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_it(file, rounder = False):\n",
    "    '''\n",
    "    Given a file name, this function finds the corresponding file provided by Spielman et al. and the file produced\n",
    "    by our code and returns the number of matches for each column.    \n",
    "    '''\n",
    "    global rpl\n",
    "    global og\n",
    "    global test\n",
    "    \n",
    "    rpl = pd.read_csv( here(path[\"ddpub\"], file) )\n",
    "    og = pd.read_csv( here(path[\"og_out\"], file) )\n",
    "    og = og.rename(columns = {\"Geo_FIPS\": \"GEOID\"})\n",
    "    \n",
    "    if \"sovi\" in rpl.columns:\n",
    "        rpl[\"sovi\"] = rpl[\"sovi\"].round(2)\n",
    "        og[\"sovi\"] = og[\"sovi\"].round(2)\n",
    "    \n",
    "    if \"Unnamed: 0\" in rpl.columns:\n",
    "        rpl.index = rpl[\"Unnamed: 0\"]\n",
    "        rpl = rpl.drop(columns = [\"Unnamed: 0\"])\n",
    "        \n",
    "    if \"Unnamed: 0\" in og.columns:\n",
    "        og.index = og[\"Unnamed: 0\"]\n",
    "        og = og.drop(columns = [\"Unnamed: 0\"])\n",
    "        \n",
    "    if rounder != False:\n",
    "        og = og.round(rounder)\n",
    "        rpl = rpl.round(rounder)\n",
    "        \n",
    "    test = rpl.eq(og)\n",
    "    \n",
    "    if test.sum().eq(len(rpl)).sum() == len(test.sum()):\n",
    "        return print(\"All values match!\")\n",
    "    else:\n",
    "        return test.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c9cdf8-2b59-45f1-a2ad-f20337bea7ed",
   "metadata": {},
   "source": [
    "#### US SoVI Scores & Rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec12fee5-66c5-4433-9ac8-f9cb5c2499b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "check_it('US_Sovi_Score.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e4583a-3a1f-4e73-85e3-e0a4f2438cae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged = og.merge(rpl, how = \"inner\", on = \"GEOID\")\n",
    "merged.loc[~test[\"rank\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238e57cc-d0f5-4e9d-9303-0d3c9efe6a47",
   "metadata": {},
   "source": [
    "We have identically reproduced national SoVI scores (rounded to 2 decimal points) for all 3143 counties compared to the original study, but two county ranks are different, probably due to the small differences between our area column and theirs. \n",
    "\n",
    "#### FEMA Region SoVI Scores & Rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9e4e1d-d7d8-492c-b869-5bd0bb228b79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "check_it('FEMA_Region_Sovi_Score.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f44e71-b9dc-4638-8b14-97312ffd02de",
   "metadata": {},
   "source": [
    "Our check it function found potential differences in 34 counties.\n",
    "The following table shows the SoVI scores and ranks for those counties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7668a46e-fb2d-4a6d-9778-a00f63994be0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged = og.merge(rpl, how = \"inner\", on = \"GEOID\")\n",
    "merged.loc[~test[\"rank\"] | ~test[\"sovi\"] | ~test[\"fema_region\"]]#.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a2e651-0c5a-44f9-a79e-cedd2ff6a118",
   "metadata": {},
   "source": [
    "These 34 counties are missing data in both the original study and our reproduction study.\n",
    "The counties and county equivalents are all located in Hawaii (FIPS code 15) and Alaska (FIPS code 02).\n",
    "In Spielman et al.'s code, when they define the states in FEMA region IX, they do not include HI, and when they define the states in FEMA region X, they do not include AK. All differences here arise from missing data in analogous places in both my output and theirs. This result was successfully reproduced.\n",
    "\n",
    "#### State SoVI Scores & Rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fa2fea-026b-4621-a5a2-cc06fc2389a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "check_it('State_Sovi_Score.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98abad9-8a7a-4e8f-a146-68f704a1f6ed",
   "metadata": {},
   "source": [
    "We have identically reproduced SoVI scores for all state models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406d6d80-bb5f-4e55-8807-c4115ed3bb4e",
   "metadata": {},
   "source": [
    "#### County in State Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d004d9c3-e0e4-4569-8f33-13c870b86cd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "check_it(\"County_in_State_Rank.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625ea236-14ca-4242-b70d-c3421d3affb9",
   "metadata": {},
   "source": [
    "We have identically reproduced the SoVI rankings in the state(s) of interest for all 21 models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10082428-a3ac-4dae-8802-95025ab2d229",
   "metadata": {},
   "source": [
    "#### Variable Contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8301666b-a1c8-412a-854c-10eeabbfaa97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "check_it(\"variable_contributions.csv\", rounder = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6c3041-58b0-49d8-834d-250fcc128cf0",
   "metadata": {},
   "source": [
    "When rounded to 3 decimal places, we have successfully reproduced all variable contributions for all models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4ae4a3-a948-4f14-8412-621d64875931",
   "metadata": {},
   "source": [
    "#### State FEMA US Rank Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c29b90-1536-4379-88fb-33cda0d6c4c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "check_it(\"state_fema_us_rank_correlations.csv\", rounder = 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7902571-4692-49b1-8a36-996193ef1365",
   "metadata": {},
   "source": [
    "When rounded to 14 decimal places, we have succesfully reproduced all Spearman's rank correlations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378180f4-22f0-4309-aa76-dea07c1ce4ae",
   "metadata": {},
   "source": [
    "### RPr-H2\n",
    "\n",
    "Next, we tested RPr-H2, that reproduced figures and tables for the internal consistency analysis are not identical to the figures and tables of the original study.\n",
    "\n",
    "#### Figure 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe37233-19df-4145-be18-1adf1919034a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read files\n",
    "counties = gpd.read_file( here(path[\"ddpub\"], \"counties.gpkg\") )\n",
    "USA = pd.read_csv( here(path[\"ddpub\"], \"US_Sovi_Score.csv\") ).rename( columns={\"sovi\": \"sovi_USA\"} )\n",
    "FEMA = pd.read_csv( here(path[\"ddpub\"], \"FEMA_Region_Sovi_Score.csv\") ).rename( columns={\"sovi\": \"sovi_FEMA\"} )\n",
    "CA = pd.read_csv( here(path[\"ddpub\"], \"State_Sovi_Score.csv\") ).rename( columns={\"sovi\": \"sovi_CA\"} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d8189f-6653-498d-930c-240ec79dfaf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Edit counties GEOID to match other datasets\n",
    "counties[\"GEOID\"] = \"g\" + counties[\"GEOID\"]\n",
    "\n",
    "# Select just the rows and columns needed\n",
    "counties_CA = counties.loc[counties[\"STATE\"] == \"06\"]\n",
    "counties_CA = counties_CA[[\"GEOID\", \"geometry\"]]\n",
    "\n",
    "# Join all datasets\n",
    "counties_CA = counties_CA.merge(USA, on = \"GEOID\")\n",
    "counties_CA = counties_CA.merge(FEMA, on = \"GEOID\")\n",
    "counties_CA = counties_CA.merge(CA, on = \"GEOID\")\n",
    "\n",
    "counties_CA['rank_USA'] = counties_CA['sovi_USA'].rank(method='average', ascending=False)\n",
    "counties_CA['rank_FEMA'] = counties_CA['sovi_FEMA'].rank(method='average', ascending=False)\n",
    "counties_CA['rank_CA'] = counties_CA['sovi_CA'].rank(method='average', ascending=False)\n",
    "\n",
    "mycolor = ListedColormap('#DBDBDB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5eb32c1-5885-4d10-8939-3d6c804b45e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create overarching plot\n",
    "fig, ax = plt.subplots(1, 4, figsize=(20, 8))\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "ax[0].axis('off')\n",
    "ax[1].axis('off')\n",
    "ax[2].axis('off')\n",
    "ax[3].axis('off');\n",
    "\n",
    "# Create CA rank map\n",
    "top5_CA = counties_CA.loc[counties_CA[\"rank_CA\"] < 6]\n",
    "bottom5_CA = counties_CA.loc[counties_CA[\"rank_CA\"] > 53]\n",
    "ax[0].set_title(\"(a) California Analysis\")\n",
    "counties_CA.plot(ax = ax[0], cmap = mycolor, edgecolor = 'black', linewidth = .1)\n",
    "top5_CA.plot(ax = ax[0], column = \"rank_CA\", cmap = \"Reds_r\")\n",
    "top5_CA.apply(lambda x: ax[0].text(s=round(x['rank_CA']), color = 'black', x=x.geometry.centroid.coords[0][0], y=x.geometry.centroid.coords[0][1], ha='center', path_effects=[pe.withStroke(linewidth=1.5, foreground=\"white\")]), axis=1, );\n",
    "bottom5_CA.plot(ax = ax[0], column = \"rank_CA\", cmap = \"Blues\");\n",
    "bottom5_CA.apply(lambda x: ax[0].text(s=round(x['rank_CA']), color = 'black', x=x.geometry.centroid.coords[0][0], y=x.geometry.centroid.coords[0][1], ha='center',  path_effects=[pe.withStroke(linewidth=1.5, foreground=\"white\")]), axis=1, );\n",
    "# Create FEMA rank map\n",
    "top5_CA = counties_CA.loc[counties_CA[\"rank_FEMA\"] < 6]\n",
    "bottom5_CA = counties_CA.loc[counties_CA[\"rank_FEMA\"] > 53]\n",
    "ax[1].set_title(\"(b) FEMA Region IX Analysis\")\n",
    "counties_CA.plot(ax = ax[1], cmap = mycolor, edgecolor = 'black', linewidth = .1)\n",
    "top5_CA.plot(ax = ax[1], column = \"rank_FEMA\", cmap = \"Reds_r\")\n",
    "top5_CA.apply(lambda x: ax[1].text(s=round(x['rank_FEMA']), color = 'black', x=x.geometry.centroid.coords[0][0], y=x.geometry.centroid.coords[0][1], ha='center', path_effects=[pe.withStroke(linewidth=1.5, foreground=\"white\")]), axis=1, );\n",
    "bottom5_CA.plot(ax = ax[1], column = \"rank_FEMA\", cmap = \"Blues\");\n",
    "bottom5_CA.apply(lambda x: ax[1].text(s=round(x['rank_FEMA']), color = 'black', x=x.geometry.centroid.coords[0][0], y=x.geometry.centroid.coords[0][1], ha='center',  path_effects=[pe.withStroke(linewidth=1.5, foreground=\"white\")]), axis=1, );\n",
    "# Create USA rank map\n",
    "top5_CA = counties_CA.loc[counties_CA[\"rank_USA\"] < 6]\n",
    "bottom5_CA = counties_CA.loc[counties_CA[\"rank_USA\"] > 53]\n",
    "ax[2].set_title(\"(c) United States Analysis\")\n",
    "counties_CA.plot(ax = ax[2], cmap = mycolor, edgecolor = 'black', linewidth = .1)\n",
    "top5_CA.plot(ax = ax[2], column = \"rank_USA\", cmap = \"Reds_r\")\n",
    "top5_CA.apply(lambda x: ax[2].text(s=round(x['rank_USA']), color = 'black', x=x.geometry.centroid.coords[0][0], y=x.geometry.centroid.coords[0][1], ha='center', path_effects=[pe.withStroke(linewidth=1.5, foreground=\"white\")]), axis=1, );\n",
    "bottom5_CA.plot(ax = ax[2], column = \"rank_USA\", cmap = \"Blues\");\n",
    "bottom5_CA.apply(lambda x: ax[2].text(s=round(x['rank_USA']), color = 'black', x=x.geometry.centroid.coords[0][0], y=x.geometry.centroid.coords[0][1], ha='center',  path_effects=[pe.withStroke(linewidth=1.5, foreground=\"white\")]), axis=1, );\n",
    "# Create range rank map\n",
    "ax[3].set_title(\"(d) Range of SoVI Rankings\")\n",
    "counties_CA[\"min_rank\"] = counties_CA[[\"rank_USA\", \"rank_FEMA\", \"rank_CA\"]].min(axis = 1)\n",
    "counties_CA[\"max_rank\"] = counties_CA[[\"rank_USA\", \"rank_FEMA\", \"rank_CA\"]].max(axis = 1)\n",
    "counties_CA[\"range_rank\"] = counties_CA[\"max_rank\"] - counties_CA[\"min_rank\"]\n",
    "counties_CA.plot(ax = ax[3], column = \"range_rank\", cmap = \"Reds\", edgecolor = 'black', linewidth = .1, scheme=\"User_Defined\", \n",
    "         legend=True, classification_kwds=dict(bins=[5,15,25,35,45]));\n",
    "plt.savefig( here(path[\"rfig\"], 'fig1.png') ) # Save image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f3a5e8-e9a5-4533-9d20-1f36171fbf2c",
   "metadata": {},
   "source": [
    "This figure looks almost the same as Spielman et al.'s. In (a), rank 3 is in a different place; in (b), rank 4 and rank 5 have switched places, but otherwise everything looks good. Our FEMA and state SoVI score data perfectly matched Spielman et al.'s output, so we are not sure what caused these differences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34300212-4ee3-47ca-8d8b-06493be2326f",
   "metadata": {},
   "source": [
    "#### Table 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1610e072-d67b-4932-9055-b9af205ad9f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read in data\n",
    "table2 = pd.read_csv( here(path[\"ddpub\"], \"state_fema_us_rank_correlations.csv\") )\n",
    "\n",
    "# Formatting table\n",
    "table2.index = table2[\"Unnamed: 0\"]\n",
    "table2 = table2.drop(columns = [\"Unnamed: 0\"])\n",
    "table2[\"FEMA Region\"] = [\"I\", \"II\", \"III\", \"IV\", \"V\", \"VI\", \"VII\", \"VIII\", \"IX\", \"X\"]\n",
    "table2[\"All US counties input file versus all counties in a state input file\"] = table2[\"spearman_r_st_us\"].round(2)\n",
    "table2[\"All counties in a FEMA region versus counties in a state within the FEMA region input file\"] = table2[\"spearman_r_st_fema\"].round(2)\n",
    "table2[\"State used for comparison\"] = [\"Composite of ME, NH, MA\", \"NY\", \"VA\", \"GA\", \"IL\", \"TX\", \"MO\", \"SD\", \"CA\", \"ID\"]\n",
    "table2 = table2.transpose().rename_axis('FEMA Region', axis='columns')\n",
    "table2 = table2.rename(columns=table2.iloc[4])\n",
    "table2 = table2.drop(labels = [\"FEMA Region\", \"spearman_r_st_fema\", \"spearman_r_st_us\"], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e8ae66-541e-4534-9800-acc878e750dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if (table2.loc[\"pvalue_st_fema\"] < 0.01).sum() == 10 and (table2.loc[\"pvalue_st_us\"] < 0.01).sum() == 10:\n",
    "    table2 = table2.drop([\"pvalue_st_fema\", \"pvalue_st_us\"])\n",
    "    print(\"p < 0.01 for all values\")\n",
    "else:\n",
    "    print(\"Different result than in paper\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9bc7a1-1030-47b3-b81f-40fea4a0a457",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "table2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cac828-9be4-4dbd-9338-c9a1cee82805",
   "metadata": {},
   "source": [
    "All of these numbers match up with those in Spielman et al.'s paper except for one, which is reported as 0.65 in their paper rather as opposed to a 0.68 in our work. Since we checked that our data matches their provided output data, they likely made a simple typo when typing up their work for publication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739f8b93-2ae6-4a4f-8f60-7d36b282ffd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save file\n",
    "table2.to_csv( here(path[\"rtab\"],\"table2.csv\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4b4520-bd4e-43ad-a6c5-8742416237f6",
   "metadata": {},
   "source": [
    "### RPr-H3\n",
    "\n",
    "Finally, we tested RPr-H3, that reproduced direction reversals and min, average, and max SoVI rank value of 28 demographic variables are not identical to the direction reversals and min, average, and max SoVI rank values shown in figure 2 of the original study.\n",
    "\n",
    "#### Figure 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7efb4a2-1bbb-49c8-8513-3ebe6e0f61cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d8c504-0feb-4c08-acec-150d866865c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save file\n",
    "summary_stats.to_csv( here(path[\"rtab\"],\"fig2.csv\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3339e51a-da78-4e19-801b-45bf5fe8635c",
   "metadata": {},
   "source": [
    "As mentioned in the analysis, we eliminated the \"expected\" column and added minus signs to the variable labels where needed to make the figure more interpretable. \n",
    "The \"original\" column matches Spielman et al.'s exactly, but the \"reversals\" column has 2 minor differences: specifically, in Spielman et al.'s paper, QNOAUTO_ALT has 1 reversal instead of 0 and QEXTRCT_ALT has 7 reversals instead of 6. \n",
    "These are pretty minor differences. Because our variable_contributions dataset matches Spielman et al.'s data perfectly, our best guess is that the differences are due to transciption errors when they constructed their figure.\n",
    "\n",
    "Spielman et al. do not provide exact values for our last 4 columns, but everything looks accurate when we compare our numbers to their figure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ff6716-c3ce-46c6-9e9e-0e904fc3a20f",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "The goal of this study was to computationally reproduce Spielman et al.'s \"Evaluating social vulnerability indicators: criteria and their application to the Social Vulnerability Index\" (Spielman et al., 2020). \n",
    "To their credit, Spielman et al. provided their code, data, and metadata in their [sovi-validity GitHub repository](https://github.com/geoss/sovi-validity), making their analysis transparent, accessible, and reproducible in a manner that is rare in the field of geography.\n",
    "\n",
    "We have rejected RPr-H1, finding that our reproductions of each of 21 SoVI models were identical to the original results, with the possible exception of a few minor changes in county rank caused by very slightly different calculations of land area and population density.\n",
    "The implication of this finding is that the codified procedures used in this reproduction study can reliably reproduce and replicate the SoVI model.\n",
    "Given our rejection of RPr-H1, we were surprised to have difficulty exactly reproducing RPr-H2 and RPr-H3.\n",
    "Although our results were very similar to figure 1 and figure 2, we did find a few discrepancies in each figure which we can only assume are related to the data visualization process in the original study, which was not automated in code.\n",
    "\n",
    "\n",
    "In addition to checking the original study results, a major aim of this reproduction study was to improve its computational reproducibility.\n",
    "With all the necessary data and code in one GitHub repository, we assumed that the computational reproduction would be trivial. \n",
    "However, even with all of the resources they provided, we still spent a month of full-time work on this project before successfully reproducing their results. \n",
    "Our experiences working with this data motivated us to publish this report, so that we can share the obstacles that made the reproduction process so time-intensive and point out methods that researchers can employ to enhance the reproducibility of their work.\n",
    "\n",
    "The main obstacles that stood in the way of reproducing Spielman et al.'s results were:\n",
    "1. Outdated packages\n",
    "2. Extraneous data and code\n",
    "3. A confusing file system\n",
    "4. Incomplete code\n",
    "5. Our own edits\n",
    "\n",
    "#### Outdated packages\n",
    "\n",
    "The first obstacle, and one that we anticipated because much of Spielman et al.'s code was written 6 or 7 years ago, was the presence of outdated packages. \n",
    "When working with code developed on outdated packages, one has the option of adjusting their package versions or adapting your code to run on current package versions. \n",
    "We opted for the latter because we found no documentation regarding the package versions used by Spielman et al., and we hope to use our code in the future for a replication study, preferably operating on an up-to-date software environment.\n",
    "\n",
    "One notable package issue occurred because Spielman et al. wrote their code before the [refactoring of PySAL](https://github.com/pysal/pysal/wiki/PEP-13:-Refactor-PySAL-Using-Submodules) into several submodules that occured with the release of [PySAL 2.0.0](https://pypi.org/project/pysal/2.0.0/) in 2019.\n",
    "In Spielman et al.'s work, functions for calculating contiguity-based spatial weights were included in the PySAL package, but after the refactoring, this feature is included in the libpysal package.\n",
    "Without prior familiarity with the intricacies of PySAL's updates over the years, it took us some time to locate equivalent functions in the updated package.\n",
    "Changes to PySAL were perhaps the most time-consuming package update issue, but there were other smaller issues along the way, such as [Pandas's deprecation of the `.ix` indexer](https://pandas.pydata.org/pandas-docs/version/0.20/whatsnew.html) in favor of `.iloc` and `.loc`.\n",
    "Most edits due to package updates were small, but they all took some time to figure out, time that adds up.\n",
    "\n",
    "Researchers can eliminate this obstacle to reproducibility by containerizing their work.\n",
    "For this study in particular, we found that providing a list of required packages and their versions in a text file was sufficient to reconstruct the environment on another machine.\n",
    "\n",
    "#### Extraneous data and code\n",
    "\n",
    "Another issue that required a substantial amount of effort to overcome was the presence of extraneous data and code.\n",
    "For example, although Spielman et al.'s paper only mentions 5-year ACS data from 2012, in their `data_prep.py` file, they also import and manipulate decennial census variables.\n",
    "Unfortunately, they do not comment their code well-enough for other researchers to understand why they do this without combing through every line of code.\n",
    "After some close inspection, we found that all of the decennial variables except for land area are not used to generate their results, allowing us to discard unused data and a substantial amount of code.\n",
    "In their `data_prep.py` code, Spielman et al. also include some analysis of standard errors that we eventually discovered to be unnecessary.\n",
    "\n",
    "Several other files in Spielman et al.'s code folder also include extraneous code. \n",
    "In particular, the entire contents of the `drop1_place.py`, `spearman.py`, and `visualization.py`, as well as portions of `compute_sovis.py` implementing a drop1 analysis turned out to be unnecessary to generate the results they describe in their paper.\n",
    "It seems that Spielman et al. were considering several possible directions of research, and they left their dead ends in their code.\n",
    "We omit all of the unnecessary steps in our report, reducing the computational intensity of the analysis and making our work easier to follow.\n",
    "\n",
    "While the presence of unnecessary code may not have bothered the original authors of the paper, in the absence of comments explaining their purpose, extra code makes it far more difficult for an independent party to understand their work.\n",
    "If one does not quickly realize which parts of the code are actually necessary, they may spend time debugging code just to discard it later on, as we did.\n",
    "Researchers can make it much easier for others to reproduce their work by publishing a clean version of their code with informative comments and no extraneous work.\n",
    "\n",
    "#### Confusing file system\n",
    "\n",
    "From our experience working with Spielman et al.'s repository, we find that an index or some metadata regarding the structure of code and data would be beneficial.\n",
    "\n",
    "The code for Spielman et al.'s analysis was originally divided into 6 different python scripts:\n",
    "- `data_prep.py`\n",
    "- `spss_pca.py`\n",
    "- `drop1_place.py`\n",
    "- `compute_sovis.py`\n",
    "- `spearman.py`\n",
    "- `visualization.py`\n",
    "\n",
    "While one could infer the order of the scripts from the file names, file contents, and whether a file called any other files, that process took a fair amount of work and left room for error.\n",
    "When one script calls another script which calls another script, it can become difficult to locate the source of an error.\n",
    "Had the researchers provided an index explaining the purpose of each script and how they work together, like our `procedure_metadata.csv`, that would have reduced the confusion of working with multiple scripts and prevented us from even attempting to debug unnecessary scripts.\n",
    "Similarly, had the authors provided a quick summary of each of their data files, like our `data_metadata.csv`, then we would have quickly understood the purpose of each data source, instead of guessing at each's purpose based on its name and the code that manipulates it.\n",
    "Generally, the more information a researcher can provide about their data and code upfront, the less time other researchers will need to spend deciphering their files during a reproduction.\n",
    "\n",
    "#### Incomplete code\n",
    "\n",
    "While Spielman et al. provide all of the code required to reproduce their data files, they do not provide any code for reproducing their figures.\n",
    "By providing their code, data, and metadata in a GitHub repository, they are on the leading edge of reproducibility in geography.\n",
    "However, they could further improve reproducibility by including code to generate their figures.\n",
    "As noted in the results section, the output data files produced by our analysis and provided by Spielman et al. were identical, yet our figures exhibited slight differences.\n",
    "Had Spielman et al. provided code to produce their figures, it would be absolutely clear whether the differences between our figures were due to typos or a difference in code; and if the differences were typos, then producing their figures with code working directly from their data may have eliminated that issue altogether.\n",
    "\n",
    "#### Our own edits\n",
    "\n",
    "The other major time sink occurred because of our own edits.\n",
    "Spielman et al. provided data and metadata for reproducing their results; assuming that they acquired their data appropriately, this should be sufficient for a reproduction.\n",
    "However, our end-goal with this project is to produce a replication study that will potentially involve census data from multiple years.\n",
    "To facilitate the acquisition of analogous data in several different time periods, it is helpful to automate the process rather than manually downloading a large number of files.\n",
    "For this reason, we used the python package, [pygris](https://walker-data.com/pygris/), to acquire our data directly from the census via an API.\n",
    "Learning to use pygris and checking that our data sufficiently matches Spielman et al.'s data was a lengthy but worthy process, as it improves reproducibility and will be useful for our future work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d958961-6eec-4ba6-aaa2-c5e536033145",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Conclusions\n",
    "\n",
    "While there are ways that Spielman et al. could make it easier to reproduce their work, we were able to sufficiently reproduce every relevant output dataset.\n",
    "We find that their results to be legitimate, highlighting issues of internal and theoretical consistency with SoVI.\n",
    "\n",
    "Our main takeaway from reproducing Spielman et al.'s work is that merely providing one's code, data, and metadata is insufficient for allowing other researchers to quickly reproduce one's results.\n",
    "In particular, containerizing their software environment, cleaning their code and omitting extraneous information, providing some metadata regarding the structure of their code and data files, and including code for every step of the analysis from data acquisition to figure production would all enhance the reproducibility of their work.\n",
    "Spielman et al. produced a well-designed study in a reproducible repository, but a more carefully designed and fully executable research compendium would reduce the risk of transcription errors and allow researchers to reproduce their results in a more reasonable time frame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba380294-7bfe-4eff-b61c-4cba8caff739",
   "metadata": {},
   "source": [
    "## References\n",
    "- Cutter, S. L., Boruff, B. J., & Shirley, W. L. (2003). Social Vulnerability to Environmental Hazards. Social Science Quarterly, 84(2), 242–261. https://doi.org/10.1111/1540-6237.8402002\n",
    "- Rey, S. J., & Anselin, L. (2007). PySAL: A Python Library of Spatial Analytical Methods. Review of Regional Studies, 37(1). https://doi.org/10.52324/001c.8285\n",
    "- Spielman, S. E., Tuccillo, J., Folch, D. C., Schweikert, A., Davies, R., Wood, N., & Tate, E. (2020). Evaluating Social Vulnerability Indicators: Criteria and their Application to the Social Vulnerability Index. Natural Hazards, 100(1), 417–436. https://doi.org/10.1007/s11069-019-03820-z\n",
    "\n",
    "## Funding\n",
    "- `Funding Name`: NSF Directorate for Social, Behavioral and Economic Sciences\n",
    "- `Funding Title`: Transforming theory-building and STEM education through reproductions and replications in the geographical sciences\n",
    "- `Award info URI`: https://www.nsf.gov/awardsearch/showAward?AWD_ID=2049837\n",
    "- `Award number`: BCS-2049837"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
