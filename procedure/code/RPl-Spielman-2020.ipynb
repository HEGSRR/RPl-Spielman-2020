{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46fdde48",
   "metadata": {},
   "source": [
    "# Replication of Spielman et al.’s 2020 Evaluation of the Social Vulnerability Index: Analysis Plan\n",
    "### Authors\n",
    "\n",
    "- Liam Smith, lwsmith@middlebury.edu, @Liam-W-Smith, Middlebury College\n",
    "- Joseph Holler\\*, josephh@middlebury.edu , @josephholler, [ORCID link](https://orcid.org/0000-0002-2381-2699), Middlebury College\n",
    "\n",
    "\\* Corresponding author"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6442e6c",
   "metadata": {},
   "source": [
    "### Abstract\n",
    "\n",
    "This study is a *replication* of:\n",
    "\n",
    "> Spielman, S. E., Tuccillo, J., Folch, D. C., Schweikert, A., Davies, R., Wood, N., & Tate, E. (2020). Evaluating Social Vulnerability Indicators: Criteria and their Application to the Social Vulnerability Index. Natural Hazards, 100(1), 417–436. https://doi.org/10.1007/s11069-019-03820-z\n",
    "\n",
    "The Spielman et al. (2020) paper is in turn a replication of:\n",
    "\n",
    "> Cutter, S. L., Boruff, B. J., & Shirley, W. L. (2003). Social vulnerability to environmental hazards. Social Science Quarterly, 84(2), 242–261. https://doi.org/10.1111/1540-6237.8402002\n",
    "\n",
    "Spielman and others (2020) evaluate the internal consistency and construct validity of the Cutter, Boruff and Shirley (2003) Social Vulnerability Index (SoVI) through replications with varying geographic extents.\n",
    "First, they reproduce a national SoVI model and validate it against an SPSS procedure provided by the original research group (Hazards Vulnerability Research Institute at University of South Carolina).\n",
    "The original SoVI uses 42 independent z-score normalized variables from the U.S. Census, reduces the data to factors using Principal Components Analysis, selects the first eleven factors, inverts factors with inverse relationships to social vulnerability, and sums the factors together to produce a SoVI score.\n",
    "The reproduced SoVI model was slightly different than the original model due to changes in U.S. Census data, using only 28 variables.\n",
    "\n",
    "Spielman et al. first reproduce SoVI with a nationwide extent.\n",
    "Then they replicate the SoVI by modifying the geographic extent and recalculating SoVI for each of ten Federal Emergency Management Agency (FEMA) regions and for a single state or cluster of states within each of the ten regions, resulting in 21 total indices.\n",
    "Internal consistency is assessed by calculating the Spearman's rank correlation coefficient of the SoVI score for counties in the state model compared to the FEMA region model and national model.\n",
    "Construct validity is assessed by summing the loadings for each input variable across the PCA factors in each model and calculating the variable's sign (positive/negative) and the rank of the variable's total loading compared to the other variables.\n",
    "The variables are summarized across all 21 versions of the SoVI model with regard to the number of times the sign is different from the national model and with regard to the minimum, maximum, and mean of their ranks.\n",
    "\n",
    "**Planned Deviation for Replication:**\n",
    "\n",
    "In this replication study, we extend Spielman et al.'s work by addressing the robustness of SoVI in the temporal dimension. \n",
    "Specifically, we construct a SoVI model using 5-year ACS data published each year between 2012 and 2021, resulting in 10 SoVI models altogether.\n",
    "We assess internal consistency by modelling each county's trend in SoVI scores over time.\n",
    "We construct a linear regression model for every county in our spatial extent, using time as our independent variable and z-score standardized SoVI scores as our dependent variable.\n",
    "Predicting SoVI scores using the year allows us to control for linear changes in social vulnerability over time.\n",
    "We calculate summary statistics and create a map of standard errors in order to interrogate SoVI's robustness over time.\n",
    "\n",
    "We investigate theoretical consistency by summing the loadings for each input variable across the PCA factors in each model and calculating the variables sign (positive/negative) and the rank of the variable's total loading compared to the other variables.\n",
    "Since we vary time rather than space, we use a rank chart to display our results rather than a table of summary statistics like Spielman et al.\n",
    "\n",
    "Parts of the code in this Jupyter notebook report are adapted from Spielman et al.'s GitHub repository.\n",
    "The original study states the intended open source permissions in the acknowledgements: \"To facilitate advances to current practice and to allow replication of our results, all of the code and data used in this analysis is open source and available at (https://github.com/geoss/sovi-validity).\n",
    "Funding was provided by the US National Science Foundation (Award No. 1333271) and the U.S. Geological Survey Land Change Science Program.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a030af0",
   "metadata": {},
   "source": [
    "### Study Metadata\n",
    "\n",
    "- `Key words`: Social vulnerability, social indicators, Principal Component Analysis, reproducibility\n",
    "- `Subject`: Social and Behavioral Sciences: Geography: Human Geography\n",
    "- `Date created`: June 19, 2023\n",
    "- `Date modified`: August 22, 2023\n",
    "- `Spatial Coverage`: United States, excluding Puerto Rico\n",
    "- `Spatial Resolution`: Counties and county equivalents\n",
    "- `Spatial Reference System`: EPSG:4269\n",
    "- `Temporal Coverage`: 2008-2021 (data published in years 2012-2021)\n",
    "- `Temporal Resolution`: 5-year estimates, compiled annually\n",
    "- `Funding Name`: NSF Division of Behavioral and Cognitive Sciences\n",
    "- `Funding Title`: Transforming Theory and STEM Education Through Reproductions and Replications in the Geographical Sciences\n",
    "- `Award info URI`: https://www.nsf.gov/awardsearch/showAward?AWD_ID=2049837\n",
    "- `Award number`: 2049837\n",
    "\n",
    "#### Original study spatio-temporal metadata\n",
    "\n",
    "- `Spatial Coverage`: United States, excluding Puerto Rico\n",
    "- `Spatial Resolution`: Counties and county equivalents\n",
    "- `Spatial Reference System`: EPSG:4269\n",
    "- `Temporal Coverage`: 2008 - 2012 (data is the 2012 5-year ACS)\n",
    "- `Temporal Resolution`: Estimated values are averaged from measurements over five years. The data in this study does not address change over time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce63b830",
   "metadata": {},
   "source": [
    "## Study design\n",
    "\n",
    "In our previous work, we computationally reproduced Spielman et al.'s original paper using the code provided in their Github repository (https://github.com/geoss/sovi-validity).\n",
    "A report on our reproduction is available online (https://osf.io/4s62b), and the code is included in our GitHub repository (https://github.com/HEGSRR/RPl-Spielman-2020).\n",
    "\n",
    "The original paper was a replication study testing the sensitivity of SoVI to changes in geographic extent.\n",
    "The SoVI is a descriptive empirical model based on social vulnerability and place theory.\n",
    "Spielman et al. addressed the following null hypotheses in their work:\n",
    "\n",
    "> OR-H1: SoVI is internally inconsistent.\n",
    "\n",
    "To address this hypothesis, Spielman et al. illustrated that SoVI is not robust to changes in geographic extent by calculating SoVI scores for ten selected states or groups of states on three geographic extents: national, FEMA region, and state(s).\n",
    "The counties within the state(s) of interest were then selected and ranked according to their SoVI score.\n",
    "OR-H1 was tested by calculating Spearman's rank correlation between the state and FEMA region models and between the state and national models.\n",
    "\n",
    "> OR-H2: SoVI is theoretically inconsistent.\n",
    "\n",
    "To address this hypothesis, Spielman et al. used the same SoVI models as described under OR-H1.\n",
    "For each model, they summed all of the PCA factors together to determine the net influence of each variable in each model.\n",
    "Then they recorded the signs of each variable and calculated the number of deviations of the ten state and FEMA region models from the national model.\n",
    "They also ranked the variables by absolute value for each model and calculated summary statistics regarding the distribution of ranks for each variable amongst all models.\n",
    "Spielman et al. did not use a particular statistical method to test OR-H2, but illustrated substantial disagreements between variable rankings and signs amongst the 21 SoVI models.\n",
    "\n",
    "In our replication, we begin with the same null hypotheses as Spielman et al., but we will test those hypotheses by varying the temporal extent rather than spatial extent.\n",
    "\n",
    "> RPl-H1: SoVI is internally inconsistent.\n",
    "\n",
    "To address this hypothesis, we will calculate SoVI scores for the entire nation (excluding Puerto Rico) using 5-year ACS data published each year between 2012 and 2021.\n",
    "In Spielman et al.'s analysis, we expect rankings in a given subregion to be identical regardless of the spatial extent, because the attributes in the area and the factors causing vulnerability remain constant.\n",
    "This is no longer true when we vary time, because we expect some areas to become more or less vulnerable over time.\n",
    "Since we do not expect rankings to remain constant, Spearman's rank correlation coefficient is no longer an appropriate statistical test.\n",
    "Instead, we seek to analyze consistency of SoVI scores while controlling for change over time.\n",
    "To do this, we construct a linear regression model for every county in our spatial extent using time as our independent variable and z-score standardized SoVI scores as our dependent variable.\n",
    "We calculate summary statistics and create a map of standard errors of our regression coefficient in order to interrogate SoVI's robustness over time.\n",
    "\n",
    "> RPl-H2: SoVI is theoretically inconsistent.\n",
    "\n",
    "To address this hypothesis, we will use the same SoVI models as described under RPl-H1.\n",
    "Like Spielman et al., we sum all of the PCA components together to determine the net influence of each variable in each model.\n",
    "We conduct an analogous analysis to Spielman et al.'s, calculating each variable's sign (positive/negative) and rank compared to the other variables.\n",
    "Since we address change over time, we present our results in the form of a rank chart rather than a table of summary statistics.\n",
    "Furthermore, rather than ranking the magnitude of coefficients, we split each model into positive and negative coefficients, and rank them separately.\n",
    "This allows us to present information about sign and magnitude of coefficients in the same graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860184ab",
   "metadata": {},
   "source": [
    "## Materials and procedure\n",
    "\n",
    "### Computational environment\n",
    "\n",
    "From the reproduction study, our environment consisted of Python 3.9.16 and the software packages listed in [requirements.txt](../environment/requirements.txt).\n",
    "We are working in the same software environment for this replication and will add additional software packages as needed for our analysis, including statsmodels for linear regression.\n",
    "\n",
    "Among the most important packages for this analysis are [pygris](https://walker-data.com/pygris/) for pulling census data directly into Python, [pandas](https://pandas.pydata.org/) and [geopandas](https://geopandas.org/en/stable/) for working with data tables and geospatial data, [NumPy](https://numpy.org/) for linear algebra functions, and [statsmodels](https://www.statsmodels.org/stable/index.html) for linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7b690fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules, define directories\n",
    "import pygris\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from pygris.data import get_census\n",
    "from pygris import counties\n",
    "from pyhere import here\n",
    "import numpy as np\n",
    "import libpysal as lps\n",
    "import lxml\n",
    "import tabulate\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats.mstats import zscore as ZSCORE\n",
    "from scipy.stats import rankdata\n",
    "import mdp as MDP\n",
    "from operator import itemgetter\n",
    "import copy\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib import patheffects as pe\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from IPython.display import Markdown, Latex\n",
    "import statsmodels.api as sm\n",
    "\n",
    "pd.set_option(\"chained_assignment\", None)\n",
    "\n",
    "path = {\n",
    "    \"dscr\": here(\"data\", \"scratch\"),\n",
    "    \"drpub\": here(\"data\", \"raw\", \"public\", \"spielman\", \"input\"),\n",
    "    \"drpub2\": here(\"data\", \"raw\", \"public\"),\n",
    "    \"drpriv\": here(\"data\", \"raw\", \"private\"),\n",
    "    \"ddpub\": here(\"data\", \"derived\", \"public\", \"version1\"),\n",
    "    \"ddpriv\": here(\"data\", \"derived\", \"private\"),\n",
    "    \"rfig\": here(\"results\", \"figures\"),\n",
    "    \"roth\": here(\"results\", \"other\"),\n",
    "    \"rtab\": here(\"results\", \"tables\"),\n",
    "    \"og_out\": here(\"data\", \"raw\", \"public\", \"spielman\", \"output\"),\n",
    "    \"dmet\": here(\"data\", \"metadata\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fffb6b",
   "metadata": {},
   "source": [
    "### Data and variables\n",
    "\n",
    "For Spielman et al.'s original study, the data sources were the 2008-2012 5-year American Community Survey and the 2010 decennial census, downloaded from Social Explorer.\n",
    "In our replication, we pull our data directly from the census into Python via a census API package known as pygris.\n",
    "These variables are based on the original work by Cutter et al. to create SoVI, and cover a wide range of social and demographic information, the particulars of which are described in the data dictionary below.\n",
    "\n",
    "Since there are so many variables, we print their label, alias, and description just one time.\n",
    "To view information regarding the data type, domain, missing data values, and missing data frequency for each individual dataset, please see their individual metadata files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6d2536",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "#### (1)-(10) American Community Survey 5-year Estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7911e30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "- `Title`: American Community Survey 5-year Estimate Demographic Variables\n",
       "- `Abstract`: The 5-year ACS provides estimates surrounding demographic information in the USA. These estimates are more reliable than 1-year and 3-year estimates but less reliable than decennial census data. On the other hand, 5-year estimates are less current than 1-year and 3-year estimates because they represent measurements taken over 60 months. See the [census website](https://www.census.gov/programs-surveys/acs/guidance/estimates.html) for more details. We use the 5-year estimates published each year from 2012 to 2021.\n",
       "- `Spatial Coverage`: United States, excluding Puerto Rico\n",
       "- `Spatial Resolution`: County and county-equivalents\n",
       "- `Spatial Reference System`: None, just attribute data\n",
       "- `Temporal Coverage`: 2008-2021\n",
       "- `Temporal Resolution`: 5 year estimates\n",
       "- `Lineage`: Obtained directly from the census via API.\n",
       "- `Distribution`: The replication data is distributed via a census API. See the detailed tables on the [census website](https://www.census.gov/data/developers/data-sets/acs-5year/2012.html) and instructions for drawing census data directly into python on the [pygris website](https://walker-data.com/pygris/).\n",
       "- `Constraints`: Census data is available in the public domain\n",
       "- `Data Quality`: Margin of error provided by the Census Bureau for relevant variables\n",
       "- `Variables`:  See RPl_ACS_2012_data_dictionary.csv, RPl_ACS_2013_data_dictionary.csv, RPl_ACS_2014_data_dictionary.csv, RPl_ACS_2015_data_dictionary.csv, RPl_ACS_2016_data_dictionary.csv, RPl_ACS_2017_data_dictionary.csv, RPl_ACS_2018_data_dictionary.csv, RPl_ACS_2019_data_dictionary.csv, RPl_ACS_2020_data_dictionary.csv, RPl_ACS_2021_data_dictionary.csv.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown( here(path[\"dmet\"], \"RPl_ACS_geographic_metadata.md\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ac6feb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Alias</th>\n",
       "      <th>Definition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GEOID</td>\n",
       "      <td>FIPS code unique identifier</td>\n",
       "      <td>Unique code for every county and county-equiva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B01002_001E</td>\n",
       "      <td>median age</td>\n",
       "      <td>MEDIAN AGE BY SEX: Estimate!!Median age!!Total</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B03002_001E</td>\n",
       "      <td>total population of respondents to race/ethnicity</td>\n",
       "      <td>HISPANIC OR LATINO ORIGIN BY RACE: Estimate!!T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B03002_004E</td>\n",
       "      <td>total Black population</td>\n",
       "      <td>HISPANIC OR LATINO ORIGIN BY RACE: Estimate!!T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B03002_005E</td>\n",
       "      <td>total Native American population</td>\n",
       "      <td>HISPANIC OR LATINO ORIGIN BY RACE: Estimate!!T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>B03002_006E</td>\n",
       "      <td>total Asian population</td>\n",
       "      <td>HISPANIC OR LATINO ORIGIN BY RACE: Estimate!!T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>B03002_012E</td>\n",
       "      <td>total Latinx population</td>\n",
       "      <td>HISPANIC OR LATINO ORIGIN BY RACE: Estimate!!T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>B06001_002E</td>\n",
       "      <td>total population under 5 years of age</td>\n",
       "      <td>PLACE OF BIRTH BY AGE IN THE UNITED STATES: Es...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>B09020_001E</td>\n",
       "      <td>total population over 65 years of age</td>\n",
       "      <td>RELATIONSHIP BY HOUSEHOLD TYPE (INCLUDING LIVI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>B01003_001E</td>\n",
       "      <td>total population</td>\n",
       "      <td>TOTAL POPULATION: Estimate!!Total</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>B25008_001E</td>\n",
       "      <td>total population in occupied housing units</td>\n",
       "      <td>TOTAL POPULATION IN OCCUPIED HOUSING UNITS BY ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>B25002_002E</td>\n",
       "      <td>total occupied housing units</td>\n",
       "      <td>OCCUPANCY STATUS: Estimate!!Total!!Occupied</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>B25003_003E</td>\n",
       "      <td>total renter occupied housing units</td>\n",
       "      <td>TENURE: Estimate!!Total!!Renter occupied</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>B25002_001E</td>\n",
       "      <td>total housing units for which occupancy status...</td>\n",
       "      <td>OCCUPANCY STATUS: Estimate!!Total</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>B09020_021E</td>\n",
       "      <td>total 65+ living in group quarters</td>\n",
       "      <td>RELATIONSHIP BY HOUSEHOLD TYPE (INCLUDING LIVI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>B01001_026E</td>\n",
       "      <td>total female population</td>\n",
       "      <td>SEX BY AGE: Estimate!!Total!!Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>B11001_006E</td>\n",
       "      <td>total female-headed family households</td>\n",
       "      <td>HOUSEHOLD TYPE (INCLUDING LIVING ALONE): Estim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>B11001_001E</td>\n",
       "      <td>total households for which household type is k...</td>\n",
       "      <td>HOUSEHOLD TYPE (INCLUDING LIVING ALONE): Estim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>B25002_003E</td>\n",
       "      <td>total vacant housing units</td>\n",
       "      <td>OCCUPANCY STATUS: Estimate!!Total!!Vacant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>B19025_001E</td>\n",
       "      <td>aggregate household income</td>\n",
       "      <td>AGGREGATE HOUSEHOLD INCOME IN THE PAST 12 MONT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>B23022_025E</td>\n",
       "      <td>total males unemployed for last 12 months</td>\n",
       "      <td>SEX BY WORK STATUS IN THE PAST 12 MONTHS BY US...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>B23022_049E</td>\n",
       "      <td>total females unemployed for last 12 months</td>\n",
       "      <td>SEX BY WORK STATUS IN THE PAST 12 MONTHS BY US...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>B23022_001E</td>\n",
       "      <td>total population for which unemployment and se...</td>\n",
       "      <td>SEX BY WORK STATUS IN THE PAST 12 MONTHS BY US...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>B17021_002E</td>\n",
       "      <td>total population below poverty level</td>\n",
       "      <td>POVERTY STATUS OF INDIVIDUALS IN THE PAST 12 M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>B17021_001E</td>\n",
       "      <td>total population for which poverty information...</td>\n",
       "      <td>POVERTY STATUS OF INDIVIDUALS IN THE PAST 12 M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>B25024_010E</td>\n",
       "      <td>number of mobile home housing units in structure</td>\n",
       "      <td>UNITS IN STRUCTURE: Estimate!!Total!!Mobile home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>B25024_001E</td>\n",
       "      <td>total housing units in structure</td>\n",
       "      <td>UNITS IN STRUCTURE: Estimate!!Total</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>C24010_038E</td>\n",
       "      <td>total female employed</td>\n",
       "      <td>SEX BY OCCUPATION FOR THE CIVILIAN EMPLOYED PO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>C24010_001E</td>\n",
       "      <td>total population for which sex and occupation ...</td>\n",
       "      <td>SEX BY OCCUPATION FOR THE CIVILIAN EMPLOYED PO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>B19055_002E</td>\n",
       "      <td>total households with social security income</td>\n",
       "      <td>SOCIAL SECURITY INCOME IN THE PAST 12 MONTHS F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>B19055_001E</td>\n",
       "      <td>total households for which social security inc...</td>\n",
       "      <td>SOCIAL SECURITY INCOME IN THE PAST 12 MONTHS F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>B09002_002E</td>\n",
       "      <td>total children in married couple families</td>\n",
       "      <td>OWN CHILDREN UNDER 18 YEARS BY FAMILY TYPE AND...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>B09002_001E</td>\n",
       "      <td>total children for which family type and age a...</td>\n",
       "      <td>OWN CHILDREN UNDER 18 YEARS BY FAMILY TYPE AND...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>B19001_017E</td>\n",
       "      <td>total households with over 200k income</td>\n",
       "      <td>HOUSEHOLD INCOME IN THE PAST 12 MONTHS (IN 201...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>B06007_005E</td>\n",
       "      <td>total Spanish-speakers who speak english less ...</td>\n",
       "      <td>PLACE OF BIRTH BY LANGUAGE SPOKEN AT HOME AND ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>B06007_008E</td>\n",
       "      <td>total people who speak another language and sp...</td>\n",
       "      <td>PLACE OF BIRTH BY LANGUAGE SPOKEN AT HOME AND ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>B06007_001E</td>\n",
       "      <td>total population with known language spoken at...</td>\n",
       "      <td>PLACE OF BIRTH BY LANGUAGE SPOKEN AT HOME AND ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>B16010_002E</td>\n",
       "      <td>total population with less than a high school ...</td>\n",
       "      <td>EDUCATIONAL ATTAINMENT AND EMPLOYMENT STATUS B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>B16010_001E</td>\n",
       "      <td>total for which education, employment, languag...</td>\n",
       "      <td>EDUCATIONAL ATTAINMENT AND EMPLOYMENT STATUS B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>C24050_002E</td>\n",
       "      <td>total population in extractive industries</td>\n",
       "      <td>INDUSTRY BY OCCUPATION FOR THE CIVILIAN EMPLOY...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>C24050_001E</td>\n",
       "      <td>total population for which industry known</td>\n",
       "      <td>INDUSTRY BY OCCUPATION FOR THE CIVILIAN EMPLOY...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>C24050_029E</td>\n",
       "      <td>total people in service occupations</td>\n",
       "      <td>INDUSTRY BY OCCUPATION FOR THE CIVILIAN EMPLOY...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>B08201_002E</td>\n",
       "      <td>total households with no available vehicle</td>\n",
       "      <td>HOUSEHOLD SIZE BY VEHICLES AVAILABLE: Estimate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>B08201_001E</td>\n",
       "      <td>total households for which vehicle status and ...</td>\n",
       "      <td>HOUSEHOLD SIZE BY VEHICLES AVAILABLE: Estimate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>B25064_001E</td>\n",
       "      <td>median gross rent</td>\n",
       "      <td>MEDIAN GROSS RENT (DOLLARS): Estimate!!Median ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>B25077_001E</td>\n",
       "      <td>median home value</td>\n",
       "      <td>MEDIAN VALUE (DOLLARS): Estimate!!Median value...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Label                                              Alias  \\\n",
       "0         GEOID                        FIPS code unique identifier   \n",
       "1   B01002_001E                                         median age   \n",
       "2   B03002_001E  total population of respondents to race/ethnicity   \n",
       "3   B03002_004E                             total Black population   \n",
       "4   B03002_005E                   total Native American population   \n",
       "5   B03002_006E                             total Asian population   \n",
       "6   B03002_012E                            total Latinx population   \n",
       "7   B06001_002E              total population under 5 years of age   \n",
       "8   B09020_001E              total population over 65 years of age   \n",
       "9   B01003_001E                                   total population   \n",
       "10  B25008_001E         total population in occupied housing units   \n",
       "11  B25002_002E                       total occupied housing units   \n",
       "12  B25003_003E                total renter occupied housing units   \n",
       "13  B25002_001E  total housing units for which occupancy status...   \n",
       "14  B09020_021E                 total 65+ living in group quarters   \n",
       "15  B01001_026E                            total female population   \n",
       "16  B11001_006E              total female-headed family households   \n",
       "17  B11001_001E  total households for which household type is k...   \n",
       "18  B25002_003E                         total vacant housing units   \n",
       "19  B19025_001E                         aggregate household income   \n",
       "20  B23022_025E          total males unemployed for last 12 months   \n",
       "21  B23022_049E        total females unemployed for last 12 months   \n",
       "22  B23022_001E  total population for which unemployment and se...   \n",
       "23  B17021_002E               total population below poverty level   \n",
       "24  B17021_001E  total population for which poverty information...   \n",
       "25  B25024_010E   number of mobile home housing units in structure   \n",
       "26  B25024_001E                   total housing units in structure   \n",
       "27  C24010_038E                              total female employed   \n",
       "28  C24010_001E  total population for which sex and occupation ...   \n",
       "29  B19055_002E       total households with social security income   \n",
       "30  B19055_001E  total households for which social security inc...   \n",
       "31  B09002_002E          total children in married couple families   \n",
       "32  B09002_001E  total children for which family type and age a...   \n",
       "33  B19001_017E             total households with over 200k income   \n",
       "34  B06007_005E  total Spanish-speakers who speak english less ...   \n",
       "35  B06007_008E  total people who speak another language and sp...   \n",
       "36  B06007_001E  total population with known language spoken at...   \n",
       "37  B16010_002E  total population with less than a high school ...   \n",
       "38  B16010_001E  total for which education, employment, languag...   \n",
       "39  C24050_002E          total population in extractive industries   \n",
       "40  C24050_001E          total population for which industry known   \n",
       "41  C24050_029E                total people in service occupations   \n",
       "42  B08201_002E         total households with no available vehicle   \n",
       "43  B08201_001E  total households for which vehicle status and ...   \n",
       "44  B25064_001E                                  median gross rent   \n",
       "45  B25077_001E                                  median home value   \n",
       "\n",
       "                                           Definition  \n",
       "0   Unique code for every county and county-equiva...  \n",
       "1      MEDIAN AGE BY SEX: Estimate!!Median age!!Total  \n",
       "2   HISPANIC OR LATINO ORIGIN BY RACE: Estimate!!T...  \n",
       "3   HISPANIC OR LATINO ORIGIN BY RACE: Estimate!!T...  \n",
       "4   HISPANIC OR LATINO ORIGIN BY RACE: Estimate!!T...  \n",
       "5   HISPANIC OR LATINO ORIGIN BY RACE: Estimate!!T...  \n",
       "6   HISPANIC OR LATINO ORIGIN BY RACE: Estimate!!T...  \n",
       "7   PLACE OF BIRTH BY AGE IN THE UNITED STATES: Es...  \n",
       "8   RELATIONSHIP BY HOUSEHOLD TYPE (INCLUDING LIVI...  \n",
       "9                   TOTAL POPULATION: Estimate!!Total  \n",
       "10  TOTAL POPULATION IN OCCUPIED HOUSING UNITS BY ...  \n",
       "11        OCCUPANCY STATUS: Estimate!!Total!!Occupied  \n",
       "12           TENURE: Estimate!!Total!!Renter occupied  \n",
       "13                  OCCUPANCY STATUS: Estimate!!Total  \n",
       "14  RELATIONSHIP BY HOUSEHOLD TYPE (INCLUDING LIVI...  \n",
       "15                SEX BY AGE: Estimate!!Total!!Female  \n",
       "16  HOUSEHOLD TYPE (INCLUDING LIVING ALONE): Estim...  \n",
       "17  HOUSEHOLD TYPE (INCLUDING LIVING ALONE): Estim...  \n",
       "18          OCCUPANCY STATUS: Estimate!!Total!!Vacant  \n",
       "19  AGGREGATE HOUSEHOLD INCOME IN THE PAST 12 MONT...  \n",
       "20  SEX BY WORK STATUS IN THE PAST 12 MONTHS BY US...  \n",
       "21  SEX BY WORK STATUS IN THE PAST 12 MONTHS BY US...  \n",
       "22  SEX BY WORK STATUS IN THE PAST 12 MONTHS BY US...  \n",
       "23  POVERTY STATUS OF INDIVIDUALS IN THE PAST 12 M...  \n",
       "24  POVERTY STATUS OF INDIVIDUALS IN THE PAST 12 M...  \n",
       "25   UNITS IN STRUCTURE: Estimate!!Total!!Mobile home  \n",
       "26                UNITS IN STRUCTURE: Estimate!!Total  \n",
       "27  SEX BY OCCUPATION FOR THE CIVILIAN EMPLOYED PO...  \n",
       "28  SEX BY OCCUPATION FOR THE CIVILIAN EMPLOYED PO...  \n",
       "29  SOCIAL SECURITY INCOME IN THE PAST 12 MONTHS F...  \n",
       "30  SOCIAL SECURITY INCOME IN THE PAST 12 MONTHS F...  \n",
       "31  OWN CHILDREN UNDER 18 YEARS BY FAMILY TYPE AND...  \n",
       "32  OWN CHILDREN UNDER 18 YEARS BY FAMILY TYPE AND...  \n",
       "33  HOUSEHOLD INCOME IN THE PAST 12 MONTHS (IN 201...  \n",
       "34  PLACE OF BIRTH BY LANGUAGE SPOKEN AT HOME AND ...  \n",
       "35  PLACE OF BIRTH BY LANGUAGE SPOKEN AT HOME AND ...  \n",
       "36  PLACE OF BIRTH BY LANGUAGE SPOKEN AT HOME AND ...  \n",
       "37  EDUCATIONAL ATTAINMENT AND EMPLOYMENT STATUS B...  \n",
       "38  EDUCATIONAL ATTAINMENT AND EMPLOYMENT STATUS B...  \n",
       "39  INDUSTRY BY OCCUPATION FOR THE CIVILIAN EMPLOY...  \n",
       "40  INDUSTRY BY OCCUPATION FOR THE CIVILIAN EMPLOY...  \n",
       "41  INDUSTRY BY OCCUPATION FOR THE CIVILIAN EMPLOY...  \n",
       "42  HOUSEHOLD SIZE BY VEHICLES AVAILABLE: Estimate...  \n",
       "43  HOUSEHOLD SIZE BY VEHICLES AVAILABLE: Estimate...  \n",
       "44  MEDIAN GROSS RENT (DOLLARS): Estimate!!Median ...  \n",
       "45  MEDIAN VALUE (DOLLARS): Estimate!!Median value...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import data dictionary\n",
    "rpl_vars = pd.read_csv( here(path[\"dmet\"], \"replication_vars.csv\") )\n",
    "rpl_vars.drop(columns=rpl_vars.columns[0], axis=1, inplace=True)\n",
    "\n",
    "acs_variables = list(rpl_vars['Label'][1:])\n",
    "rpl_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fe133a",
   "metadata": {},
   "source": [
    "#### (11) USA Counties Cartographic Boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5394f5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping # Comment this first line out if you wish to acquire data directly from census\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping # Comment this first line out if you wish to acquire data directly from census\n",
    "# Acquire geographical data for reproduction\n",
    "counties_shp = counties(cb = True, year = 2010, cache = True) # year 2012 (and 2011) cartographic boundaries not available\n",
    "\n",
    "# Save raw data\n",
    "counties_shp.to_file( here(path[\"drpub2\"], \"counties_geometries_raw.gpkg\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cac8a37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, load data directly from the repository\n",
    "counties_shp = gpd.read_file( here(path[\"drpub2\"], \"counties_geometries_raw.gpkg\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e670da8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "- `Title`: USA Counties Cartographic Boundaries\n",
       "- `Abstract`: The cartographic boundary files provided by the US census are simplified representations of the MAF/TIGER files. We use the 2010 boundary file because the census has not made such a file available for 2012 or 2011 and the original paper also used land area from 2010. This shapefile provides the geometries of counties and county-equivalents in the United States, with limited attribute information including land area.\n",
       "- `Spatial Coverage`: United States, excluding Puerto Rico\n",
       "- `Spatial Resolution`: County and county-equivalents\n",
       "- `Spatial Reference System`: EPSG:4269\n",
       "- `Temporal Coverage`: 2010\n",
       "- `Temporal Resolution`: One-time observations\n",
       "- `Lineage`: We use [pygris](https://walker-data.com/pygris/) to pull the data directly from the census into python.\n",
       "- `Distribution`: This file is distributed via a census API. See more information on the [census website](https://www.census.gov/geographies/mapping-files/time-series/geo/carto-boundary-file.2010.html#list-tab-1556094155) and instructions for drawing census data directly into python on the [pygris website](https://walker-data.com/pygris/).\n",
       "- `Constraints`: Census data is available in the public domain\n",
       "- `Data Quality`: 1:500,000 scale\n",
       "- `Variables`:\n",
       "\n",
       "| Label | Alias | Definition | Type | Domain | Missing Data Value(s) | Missing Data Frequency |\n",
       "| :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n",
       "| STATE | State-level FIPS code | State-level FIPS code | string | 01 - 56 | None | 0 |\n",
       "| COUNTY | County-level FIPS code | County-level FIPS code | string | 001 - 840 | None | 0 |\n",
       "| CENSUSAREA | land area | land area in square miles | float64 | 1.999 - 145504.789 | NaN | 0 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown( here(path[\"dmet\"], \"county_geom_2010_metadata.md\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd31b433",
   "metadata": {},
   "source": [
    "### Prior observations\n",
    "\n",
    "At the time of this study pre-registration, the authors had examined the Python code and data from the original study and modified the code to reproduce the original results in a current software environment.\n",
    "This study is related to [one prior study](https://osf.io/4s62b) by the authors, a reproduction of Spielman et al.'s [\"Evaluating Social Vulnerability Indicators\"](https://link.springer.com/article/10.1007/s11069-019-03820-z) (Spielman et al., 2020).\n",
    "\n",
    "We have already thoroughly observed datasets (1) and (11), as they were necessary to reproduce the original study.\n",
    "On the other hand, we have yet to thoroughly analyze datasets (2) through (10).\n",
    "We have imported these datasets into Python in order to generate the data on missing data and domains in our metadata files.\n",
    "We have otherwise not directly inspected, transformed, visualized, or analyzed the data. \n",
    "All of the work we have done with the data can be seen in our `generate_RPl_metadata.ipynb` file in the metadata folder. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e045e8",
   "metadata": {},
   "source": [
    "### Bias and threats to validity\n",
    "\n",
    "Possible threats to validity in this study include spatial dependence and nonstationarity, temporal dependence, and scale dependency.\n",
    "\n",
    "Regarding spatial dependence, we may expect nearby places to exhibit similar levels of social vulnerability as well as similar underlying causes of it. \n",
    "Regarding spatial nonstationarity, we may expect the causal relationships underlying social vulnerability to change over space in the model. \n",
    "Although spatially explicit PCA methods have been developed, the methodology used to calculate SoVI uses ordinary PCA, which does not account for spatial relationships.\n",
    "Thus, each county is treated as an independent observation even though there may be spatial clustering of social vulnerability and its underlying factors, and PCA is a global model that does not account for spatial nonstationarity.\n",
    "To this end, Spielman et al. (2020) found substantially different variable weights and rankings of SoVI scores depending on the spatial extent of input data.\n",
    "We do not control for spatial dependence or nonstationarity since our work is also an evaluation of SoVI's consistency based on the original study methodology.\n",
    "\n",
    "We also anticipate temporal correlation in county SoVI scores, because demographic characteristics change over time and may be more similar in two subsequent years than in two years that are a decade apart. \n",
    "We account for temporal dependence by using time as an independent variable in linear regression.\n",
    "However, there is temporal dependence not only in social vulnerability but also in the very data we are using for our analysis.\n",
    "Because we use overlapping 5-year averages, four-fifths of the data used to generate each dataset is also used to generate the subsequent and previous year's datasets.\n",
    "The similarity of input data puts us at risk of finding SoVI to be more consistent than it really is.\n",
    "Unfortunately, increasing the temporal resolution to the one-year ACS estimates is not possible beause of the large amount of missing data and uncertainty in that data product and increasing the temporal extent is not possible because of changes in the census data products.\n",
    "\n",
    "Finally, SoVI is likely scale dependent, as some input demographic variables, like race and ethnicity, show substantially greater variation at the tract level than at the county level.\n",
    "Differing variability depending on scale of analysis may have spillover effects on variance-based models such as PCA.\n",
    "For example, Eric Tate (2012) found that PCA-based social vulnerability models like SoVI exhibit high levels of scale dependency (Tate, 2012).\n",
    "The implication for this study is that we are at risk of finding SoVI to be consistent over time at the county level without detecting inconsistency over time at the tract level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5f75eb",
   "metadata": {},
   "source": [
    "### Data transformations\n",
    "\n",
    "To prepare our raw data for input into the SoVI model, we need to consider only the counties that are unchanged in shape over the study window, impute for missing data, normalize the variables, and adjust their directionality such that all variables are (theoretically) directly related with social vulnerability.\n",
    "A draft workflow diagram for this section is displayed below. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f2eddc",
   "metadata": {},
   "source": [
    "![Data Preparation Workflow](../../results/figures/RPl_Data_Transformations.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527c7f76",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Note on Step P1:**\n",
    "\n",
    "Since we are working with census data from ten different years and we want to track changes in SoVI scores in counties over time, we have to account for changes in county boundaries over this time.\n",
    "Fortunately, the Census publishes information regarding such changes on its website.\n",
    "The changes for the 2010 decade are available [here](https://www.census.gov/programs-surveys/geography/technical-documentation/county-changes.2010.html#list-tab-957819518) and there were no such changes listed in the 2020 decade until 2022.\n",
    "\n",
    "Below is a list of all counties with relevant changes between 2012 and 2021:\n",
    "- Chugach Census Area, Alaska (02063). Created from 02261 in 2019.\n",
    "- Copper River Census Area, Alaska (02066). Created from 02261 in 2019.\n",
    "- Valdez-Cordova Census Area, Alaska (02261). Split into 02063 and 02066 in 2019.\n",
    "- Petersburg Borough, Alaska (02195). Created from 02195 and part of 02105 in 2013.\n",
    "- Hoonah-Angoon Census Area, Alaska (02105). Part given to 02195 in 2013.\n",
    "- Bedford (independent) city, Virginia (51515). Became a town, absorbed by Bedford County (51019).\n",
    "- Kusilvak Census Area, Alaska (02158). Changed name and code from Wade Hampton Census Area and 02270 in 2015.\n",
    "- Oglala Lakota County, South Dakota (46102). Changed name and code from Shannon County and 46113 in 2015.\n",
    "- Prince of Wales-Hyder Census Area, Alaska (02198). Added part of 02195 in 2013.\n",
    "- Bedford County, Virginia (51019). Added 51515 in 2013.\n",
    "\n",
    "In order to generate data comparable across all ten years of interest, we simply remove these counties from each dataset containing them, except for counties 02158 and 46102, which simply changed names and codes.\n",
    "For counties 02158 and 46102, we just adjust their FIPS codes to match.\n",
    "\n",
    "A final step of data transformation will be performed at the beginning of the SoVI model analysis.\n",
    "Each demographic variable will be standardized by calculating its z-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d9ccab1-88c7-415e-9717-36cf73aa5acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables and variable names to import data\n",
    "variables = list(rpl_vars['Label'][1:])\n",
    "aliases = list(rpl_vars['Alias'][1:])\n",
    "\n",
    "# Define list of county GEOIDs to remove from dataset\n",
    "changes_list = [\"02063\",\n",
    "                \"02066\",\n",
    "                \"02261\",\n",
    "                \"02195\",\n",
    "                \"02105\",\n",
    "                \"51515\",\n",
    "                \"51019\",\n",
    "                \"02198\",\n",
    "                \"35039\"] # we also drop Rio Arriba because it's missing a lot of data in 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67777632-7ed4-4c19-aa7b-b94277743673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of counties2012 : 3136\n",
      "Length of counties2013 : 3136\n",
      "Length of counties2014 : 3136\n",
      "Length of counties2015 : 3136\n",
      "Length of counties2016 : 3136\n",
      "Length of counties2017 : 3136\n",
      "Length of counties2018 : 3136\n",
      "Length of counties2019 : 3136\n",
      "Length of counties2020 : 3136\n",
      "Length of counties2021 : 3136\n"
     ]
    }
   ],
   "source": [
    "# Create GEOID columns in geometries data\n",
    "counties_shp['GEOID'] = counties_shp.STATE + counties_shp.COUNTY\n",
    "\n",
    "# Create dictionary to store data\n",
    "data = {}\n",
    "\n",
    "# Create element in dictionary for each dataset\n",
    "for i in range(2012, 2022):    \n",
    "    # Obtain data\n",
    "    counties_detailed = get_census(dataset = \"acs/acs5\", # dataset name on the Census API you are connecting to; find datasets at https://api.census.gov/data.html\n",
    "                                     variables = variables, # string (or list of strings) of desired vars. For the 2021 5-year ACS Data Profile, those variable IDs are found at https://api.census.gov/data/2021/acs/acs5/profile/variables.html\n",
    "                                     year = i, # year of your data (or end-year for a 5-year ACS sample)\n",
    "                                     params = { # dict of query parameters to send to the API.\n",
    "                                         \"for\": \"county:*\"},\n",
    "                                     guess_dtypes = True,\n",
    "                                     return_geoid = True)\n",
    "\n",
    "    # Drop Puerto Rico\n",
    "    counties_detailed = counties_detailed.loc[~counties_detailed['GEOID'].str.startswith('72')]\n",
    "    \n",
    "    # P1: Eliminate counties with changed boundaries\n",
    "    counties_detailed = counties_detailed.loc[~counties_detailed[\"GEOID\"].isin(changes_list)]\n",
    "\n",
    "    # Adjust the GEOID of counties with changed GEOIDs\n",
    "    counties_detailed.GEOID.loc[counties_detailed[\"GEOID\"] == \"02158\"] = \"02270\"\n",
    "    counties_detailed.GEOID.loc[counties_detailed[\"GEOID\"] == \"46102\"] = \"46113\"\n",
    "    \n",
    "    # P3: Join to geometry\n",
    "    data[\"counties\"+str(i)] = counties_shp.merge(counties_detailed, how = \"inner\", on = \"GEOID\")\n",
    "    \n",
    "    # Print out the length of the dataset as a rough check that the data matches\n",
    "    print(\"Length of\", \"counties\"+str(i), \":\", len(data[\"counties\"+str(i)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab63c8bc-0007-46e4-9681-09bbb1450a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing data\n",
    "# Loop over datasets to deal with missing data individually\n",
    "for year in data:\n",
    "    # Loop over columns to deal with missing data individually\n",
    "    for i in data[year].columns:\n",
    "        x = data[year][i].isnull().sum() # Compute the number of missing values\n",
    "        if (x > 0) & (i != 'LSAD') & (i != \"B25077_001E\") & (i != \"B25064_001E\"): # Print out number of missing values\n",
    "            print(\"Dataset\", year, \"column\", i, \"contains\", x, \"missing value(s).\")\n",
    "        \n",
    "        # Impute median home value with its spatial lag as in original study\n",
    "        if (x > 0) & (i == 'B25077_001E'):\n",
    "            # Calculate spatial weights matrix\n",
    "            w = lps.weights.Queen.from_dataframe(data[year], silence_warnings = True)\n",
    "            w.transform = 'R'\n",
    "            # Calculate spatial lag\n",
    "            data[year]['B25077_001E_LAG'] = lps.weights.lag_spatial(w, data[year].B25077_001E)\n",
    "            # Impute for the missing value\n",
    "            data[year].B25077_001E[np.isnan(data[year]['B25077_001E'])] = data[year].B25077_001E_LAG[pd.isna(data[year]['B25077_001E'])]\n",
    "            x = data[year][i].isnull().sum()\n",
    "            # print(\"Now, dataset\", year, \"column\", i, \"contains\", x, \"missing value(s).\")\n",
    "            \n",
    "        # Impute median gross rent with its spatial lag (check with Joe)\n",
    "        if (x > 0) & (i == 'B25064_001E'):\n",
    "            # Calculate spatial weights matrix\n",
    "            w = lps.weights.Queen.from_dataframe(data[year], silence_warnings = True)\n",
    "            w.transform = 'R'\n",
    "            # Calculate spatial lag\n",
    "            data[year]['B25064_001E_LAG'] = lps.weights.lag_spatial(w, data[year].B25064_001E)\n",
    "            # Impute for the missing value\n",
    "            data[year].B25064_001E[np.isnan(data[year]['B25064_001E'])] = data[year].B25064_001E_LAG[pd.isna(data[year]['B25064_001E'])]\n",
    "            x = data[year][i].isnull().sum()\n",
    "            # print(\"Now, dataset\", year, \"column\", i, \"contains\", x, \"missing value(s).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82ca44bf-4bd4-432f-9493-cab8f42088c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counties2012  has  0  missing values.\n",
      "counties2013  has  0  missing values.\n",
      "counties2014  has  0  missing values.\n",
      "counties2015  has  0  missing values.\n",
      "counties2016  has  0  missing values.\n",
      "counties2017  has  0  missing values.\n",
      "counties2018  has  0  missing values.\n",
      "counties2019  has  0  missing values.\n",
      "counties2020  has  0  missing values.\n",
      "counties2021  has  0  missing values.\n"
     ]
    }
   ],
   "source": [
    "# One last check for missing data\n",
    "for year in data:\n",
    "    x = data[year].drop([\"LSAD\", \"B25064_001E_LAG\", \"B25077_001E_LAG\"], axis = 1, errors = \"ignore\").isnull().sum().sum()\n",
    "    print(year, \" has \", x, \" missing values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4efdb3-432f-4f95-9c20-c1b9d5207a20",
   "metadata": {},
   "source": [
    "We have no missing data, so we can now calculate the variables for input into SoVI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da69c993-8e31-4998-ac8a-8c2f2c56696a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step P4\n",
    "# Calculating the variables used in SoVI\n",
    "for year in data:\n",
    "    data[year]['MEDAGE_ACS'] = data[year].B01002_001E\n",
    "    data[year]['BLACK_ACS'] = data[year].B03002_004E / (data[year].B03002_001E)\n",
    "    data[year]['QNATAM_ACS'] = data[year].B03002_005E / (data[year].B03002_001E)\n",
    "    data[year]['QASIAN_ACS'] = data[year].B03002_006E / (data[year].B03002_001E)\n",
    "    data[year]['QHISP_ACS'] = data[year].B03002_012E / (data[year].B03002_001E)\n",
    "    data[year]['QAGEDEP_ACS'] = (data[year].B06001_002E + data[year].B09020_001E) / (data[year].B01003_001E)\n",
    "    data[year]['QPUNIT_ACS'] = data[year].B25008_001E / (data[year].B25002_002E)\n",
    "    data[year]['PRENTER_ACS'] = data[year].B25003_003E / (data[year].B25002_001E)\n",
    "    data[year]['QNRRES_ACS'] = data[year].B09020_021E / (data[year].B01003_001E)\n",
    "    data[year]['QFEMALE_ACS'] = data[year].B01001_026E / (data[year].B01003_001E)\n",
    "    data[year]['QFHH_ACS'] = data[year].B11001_006E / (data[year].B11001_001E)\n",
    "    data[year]['QUNOCCHU_ACS'] = data[year].B25002_003E / (data[year].B25002_001E)\n",
    "    data[year]['QCVLUN'] = (data[year].B23022_025E + data[year].B23022_049E) / \\\n",
    "                    data[year].B23022_001E\n",
    "    data[year]['QPOVTY'] = (data[year].B17021_002E) / data[year].B17021_001E\n",
    "    data[year]['QMOHO'] = (data[year].B25024_010E) / data[year].B25024_001E\n",
    "    data[year]['QFEMLBR'] = (data[year].C24010_038E) / data[year].C24010_001E\n",
    "    data[year]['QSSBEN'] = (data[year].B19055_002E) / data[year].B19055_001E\n",
    "    data[year]['QFAM'] = (data[year].B09002_002E) / data[year].B09002_001E\n",
    "    data[year]['QRICH200K'] = (data[year].B19001_017E) / data[year].B11001_001E\n",
    "    data[year]['PERCAP_ALT'] = data[year].B19025_001E / (data[year].B25008_001E)\n",
    "    data[year]['QESL_ALT'] = (data[year].B06007_005E + data[year].B06007_008E) / \\\n",
    "                    data[year].B06007_001E\n",
    "    data[year]['QED12LES_ALT'] = (data[year].B16010_002E) / data[year].B16010_001E\n",
    "    data[year]['QEXTRCT_ALT'] = (data[year].C24050_002E) / data[year].C24050_001E\n",
    "    data[year]['QSERV_ALT'] = (data[year].C24050_029E) / data[year].C24050_001E\n",
    "    data[year]['QNOAUTO_ALT'] = (data[year].B08201_002E) / data[year].B08201_001E\n",
    "    data[year]['MDGRENT_ALT'] = data[year].B25064_001E\n",
    "    data[year]['MHSEVAL_ALT'] = data[year].B25077_001E\n",
    "    data[year]['POPDENS'] = data[year].B01003_001E / (data[year].CENSUSAREA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f0fe3d59-ef04-4bb6-9b3b-9b77b124a5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step P5: Check for and impute missing data\n",
    "# Loop over datasets to deal with missing data individually\n",
    "for year in data:\n",
    "    # Loop over columns to deal with missing data individually\n",
    "    for i in data[year].columns:\n",
    "        x = data[year][i].isnull().sum()\n",
    "        if (x > 0) & (i != 'LSAD') & (i != 'B25064_001E_LAG') & (i != 'B25077_001E_LAG') & (i != 'QFAM'):\n",
    "            print(\"Dataset\", year, \"column\", i, \"contains\", x, \"missing value(s).\")\n",
    "        \n",
    "        # Replace missing QFAM data with 0\n",
    "        if (x > 0) & (i == 'QFAM'):\n",
    "            data[year].QFAM = data[year].QFAM.replace([np.inf, -np.inf, np.nan], 0)\n",
    "        \n",
    "# Check for infinities\n",
    "for year in data:\n",
    "    pos_inf = (data[year].select_dtypes(include=['int64','float64']) == np.inf).sum().sum()\n",
    "    neg_inf = (data[year].select_dtypes(include=['int64','float64']) == -np.inf).sum().sum()\n",
    "    if pos_inf > 0:\n",
    "        print(\"There are\", pos_inf, \"instances of infinity in dataset\", year)\n",
    "    if neg_inf > 0:\n",
    "        print(\"There are\", neg_inf, \"instances of negative infinity in dataset\", year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f3c57085-8f11-4d0e-b36d-5b401287cef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counties2012  has  0  missing values.\n",
      "counties2013  has  0  missing values.\n",
      "counties2014  has  0  missing values.\n",
      "counties2015  has  0  missing values.\n",
      "counties2016  has  0  missing values.\n",
      "counties2017  has  0  missing values.\n",
      "counties2018  has  0  missing values.\n",
      "counties2019  has  0  missing values.\n",
      "counties2020  has  0  missing values.\n",
      "counties2021  has  0  missing values.\n"
     ]
    }
   ],
   "source": [
    "# One last check for missing data\n",
    "for year in data:\n",
    "    x = data[year].drop([\"LSAD\", \"B25064_001E_LAG\", \"B25077_001E_LAG\"], axis = 1, errors = \"ignore\").isnull().sum().sum()\n",
    "    print(year, \" has \", x, \" missing values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709c8700",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "#### Principal Component Analysis\n",
    "\n",
    "Spielman et al. constructed a class to conduct SPSS-style PCA with varimax rotation in Python and validated their procedure against Cutter et al.'s SPSS workflow used to calculate SoVI.\n",
    "Below we include a workflow diagram that shows the main operations and important outputs of their SPSS_PCA class. \n",
    "After that, we include their relevant code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9124c3b",
   "metadata": {},
   "source": [
    "![PCA Workflow](../../results/figures/RPl_PCA_Workflow.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01ca35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPSS_PCA:\n",
    "\t'''\n",
    "\tA class that integrates most (all?) of the assumptions SPSS imbeds in their\n",
    "    implimnetation of principal components analysis (PCA), which can be found in\n",
    "    thier GUI under Analyze > Dimension Reduction > Factor. This class is not\n",
    "\tintended to be a full blown recreation of the SPSS Factor Analysis GUI, but\n",
    "\tit does replicate (possibly) the most common use cases. Note that this class\n",
    "\twill not produce exactly the same results as SPSS, probably due to differences\n",
    "\tin how eigenvectors/eigenvalues and/or singular values are computed. However,\n",
    "\tthis class does seem to get all the signs to match, which is not really necessary\n",
    "\tbut kinda nice. Most of the approach came from the official SPSS documentation.\n",
    "\n",
    "\tReferences\n",
    "\t----------\n",
    "\tftp://public.dhe.ibm.com/software/analytics/spss/documentation/statistics/20.0/en/\n",
    "    client/Manuals/IBM_SPSS_Statistics_Algorithms.pdf\n",
    "\thttp://spssx-discussion.1045642.n5.nabble.com/Interpretation-of-PCA-td1074350.html\n",
    "\thttp://mdp-toolkit.sourceforge.net/api/mdp.nodes.WhiteningNode-class.html\n",
    "\thttps://github.com/mdp-toolkit/mdp-toolkit/blob/master/mdp/nodes/pca_nodes.py\n",
    "\n",
    "\tParameters\n",
    "\t----------\n",
    "\tinputs:  numpy array\n",
    "\t\t\t n x k numpy array; n observations and k variables on each observation\n",
    "\treduce:  boolean (default=False)\n",
    "\t\t\t If True, then use eigenvalues to determine which factors to keep; all\n",
    "\t\t\t results will be based on just these factors. If False use all factors.\n",
    "\tmin_eig: float (default=1.0)\n",
    "\t\t\t If reduce=True, then keep all factors with an eigenvalue greater than\n",
    "\t\t\t min_eig. SPSS default is 1.0. If reduce=False, then min_eig is ignored.\n",
    "\tvarimax: boolean (default=False)\n",
    "\t\t\t If True, then apply a varimax rotation to the results. If False, then\n",
    "\t\t\t return the unrotated results only.\n",
    "\n",
    "\tAttributes\n",
    "\t----------\n",
    "\tz_inputs:\tnumpy array\n",
    "\t\t\t\tz-scores of the input array.\n",
    "\tcomp_mat:\tnumpy array\n",
    "\t\t\t\tComponent matrix (a.k.a, \"loadings\").\n",
    "\tscores:\t\tnumpy array\n",
    "\t\t\t\tNew uncorrelated vectors associated with each observation.\n",
    "\teigenvals_all:\tnumpy array\n",
    "\t\t\t\tEigenvalues associated with each factor.\n",
    "\teigenvals:\tnumpy array\n",
    "\t\t\t\tSubset of eigenvalues_all reflecting only those that meet the\n",
    "\t\t\t\tcriterion defined by parameters reduce and min_eig.\n",
    "\tweights:    numpy array\n",
    "\t\t\t\tValues applied to the input data (after z-scores) to get the PCA\n",
    "\t\t\t\tscores. \"Component score coefficient matrix\" in SPSS or\n",
    "\t\t\t\t\"projection matrix\" in the MDP library.\n",
    "\tcomms: \t\tnumpy array\n",
    "\t\t\t\tCommunalities\n",
    "\tsum_sq_load: numpy array\n",
    "\t\t\t\t Sum of squared loadings.\n",
    "\tcomp_mat_rot: numpy array or None\n",
    "\t\t\t\t  Component matrix after rotation. Ordered from highest to lowest\n",
    "\t\t\t\t  variance explained based on sum_sq_load_rot. None if varimax=False.\n",
    "\tscores_rot:\tnumpy array or None\n",
    "\t\t\t\tUncorrelated vectors associated with each observation, after\n",
    "\t\t\t\trotation. None if varimax=False.\n",
    "\tweights_rot: numpy array or None\n",
    "\t\t\t\tRotated values applied to the input data (after z-scores) to get\n",
    "\t\t\t\tthe PCA\tscores. None if varimax=False.\n",
    "\tsum_sq_load_rot: numpy array or None\n",
    "\t\t\t\t Sum of squared loadings for rotated results. None if\n",
    "\t\t\t\t varimax=False.\n",
    "\n",
    "\t'''\n",
    "\n",
    "\tdef __init__(self, inputs, reduce=False, min_eig=1.0, varimax=False):\n",
    "        \n",
    "        # Step S1: Standardize inputs\n",
    "\t\tz_inputs = ZSCORE(inputs)  # necessary for SPSS \"correlation matrix\" setting\n",
    "        \n",
    "        # Step S2: Unrotated PCA\n",
    "\t\t# run base SPSS-style PCA to get all eigenvalues\n",
    "\t\tpca_node = MDP.nodes.WhiteningNode()  # settings for the PCA\n",
    "\t\tscores = pca_node.execute(z_inputs)  # base run PCA\n",
    "\t\teigenvalues_all = pca_node.d   # rename PCA results\n",
    "\n",
    "\t\t# run SPSS-style PCA based on user settings\n",
    "\t\t# settings for the PCA\n",
    "        pca_node = MDP.nodes.WhiteningNode(reduce=reduce, var_abs=min_eig)  \n",
    "        # run PCA  (these have mean=0, std_dev=1)\n",
    "\t\tscores = pca_node.execute(z_inputs)  \n",
    "        weights = pca_node.v # save weights from PCA results\n",
    "        eigenvalues = pca_node.d # save eigenvalues from PCA results\n",
    "\t\tcomponent_matrix = weights * eigenvalues  # compute the loadings\n",
    "\t\t# invert signs for components with mostly negative loadings\n",
    "        component_matrix = self._reflect(component_matrix)\n",
    "\t\tcommunalities = (component_matrix**2).sum(1) # compute the communalities\n",
    "        # compute sum of the squares of loadings, same as eigenvalues\n",
    "\t\tsum_sq_loadings = (component_matrix**2).sum(0) \n",
    "        # divide matrix by eigenvalues\n",
    "\t\tweights_reflected = component_matrix/eigenvalues\n",
    "        # calculate scores, where abs(scores)=abs(scores_reflected)\n",
    "\t\tscores_reflected = np.dot(z_inputs, weights_reflected) \n",
    "\n",
    "        # Step S3: Varimax rotation\n",
    "\t\tif varimax:\n",
    "\t\t\t# SPSS-style varimax rotation prep\n",
    "            # normalize inputs to varimax\n",
    "\t\t\tc_normalizer = 1. / MDP.numx.sqrt(communalities)  \n",
    "            # reshape to vectorize normalization\n",
    "\t\t\tc_normalizer.shape = (component_matrix.shape[0],1)  \n",
    "            # normalize component matrix for varimax\n",
    "\t\t\tcm_normalized = c_normalizer * component_matrix \n",
    "\n",
    "\t\t\t# varimax rotation\n",
    "\t\t\tcm_normalized_varimax = self._varimax(cm_normalized)  # run varimax\n",
    "\t\t\t# denormalize varimax output\n",
    "            c_normalizer2 = MDP.numx.sqrt(communalities)  \n",
    "            # reshape to vectorize denormalization\n",
    "\t\t\tc_normalizer2.shape = (component_matrix.shape[0],1) \n",
    "            # denormalize varimax output\n",
    "\t\t\tcm_varimax = c_normalizer2 * cm_normalized_varimax  \n",
    "\n",
    "\t\t\t# reorder varimax component matrix\n",
    "\t\t\t# base the ordering on sum of squared loadings\n",
    "            sorter = (cm_varimax**2).sum(0)  \n",
    "            # add index to denote current order\n",
    "\t\t\tsorter = zip(sorter.tolist(), range(sorter.shape[0]))  \n",
    "            # sort from largest to smallest\n",
    "\t\t\tsorter = sorted(sorter, key=itemgetter(0), reverse=True) \n",
    "            # unzip the sorted list\n",
    "\t\t\tsum_sq_loadings_varimax, reorderer = zip(*sorter)  \n",
    "            # convert to array\n",
    "\t\t\tsum_sq_loadings_varimax = np.array(sum_sq_loadings_varimax)  \n",
    "            # reorder component matrix\n",
    "\t\t\tcm_varimax = cm_varimax[:,reorderer]  \n",
    "\n",
    "\t\t\t# varimax scores\n",
    "            # invert signs for factors with mostly negative loadings\n",
    "\t\t\tcm_varimax_reflected = self._reflect(cm_varimax)  \n",
    "\t\t\tvarimax_weights = np.dot(cm_varimax_reflected,\n",
    "\t\t\t\t\t\t\t  np.linalg.inv(np.dot(cm_varimax_reflected.T,\n",
    "\t\t\t\t\t\t\t  cm_varimax_reflected))) # CM(CM'CM)^-1\n",
    "\t\t\tscores_varimax = np.dot(z_inputs, varimax_weights)\n",
    "\t\telse:\n",
    "\t\t\tcomp_mat_rot = None\n",
    "\t\t\tscores_rot = None\n",
    "\t\t\tweights_rot = None\n",
    "\n",
    "\t\t# assign output variables\n",
    "\t\tself.z_inputs = z_inputs\n",
    "\t\tself.scores = scores_reflected\n",
    "\t\tself.comp_mat = component_matrix\n",
    "\t\tself.eigenvals_all = eigenvalues_all\n",
    "\t\tself.eigenvals = eigenvalues\n",
    "\t\tself.weights = weights_reflected\n",
    "\t\tself.comms = communalities\n",
    "\t\tself.sum_sq_load = sum_sq_loadings\n",
    "\t\tself.comp_mat_rot = cm_varimax_reflected\n",
    "\t\tself.scores_rot = scores_varimax # PCA scores output\n",
    "\t\tself.weights_rot = varimax_weights # PCA weights output\n",
    "\t\tself.sum_sq_load_rot = sum_sq_loadings_varimax\n",
    "\n",
    "\tdef _reflect(self, cm):\n",
    "\t\t# reflect factors with negative sums; SPSS default\n",
    "\t\tcm = copy.deepcopy(cm)\n",
    "\t\treflector = cm.sum(0)\n",
    "\t\tfor column, measure in enumerate(reflector):\n",
    "\t\t\tif measure < 0:\n",
    "\t\t\t\tcm[:,column] = -cm[:,column]\n",
    "\t\treturn cm\n",
    "\n",
    "\tdef _varimax(self, Phi, gamma = 1.0, q = 100, tol = 1e-6):\n",
    "\t\t# see http://en.wikipedia.org/wiki/Talk%3aVarimax_rotation\n",
    "\t\t# and http://stackoverflow.com/questions/17628589/\n",
    "        # perform-varimax-rotation-in-python-using-numpy\n",
    "\t\tp,k = Phi.shape\n",
    "\t\tR = np.eye(k)\n",
    "\t\td=0\n",
    "\t\tfor i in range(q):\n",
    "\t\t\td_old = d\n",
    "\t\t\tLambda = np.dot(Phi, R)\n",
    "\t\t\tu,s,vh = np.linalg.svd(\n",
    "                np.dot(Phi.T, np.asarray(Lambda)**3 - (gamma/p) *\n",
    "                       np.dot(Lambda,\n",
    "                              np.diag(np.diag(np.dot(Lambda.T,\n",
    "                                                     Lambda))))))\n",
    "\t\t\tR = np.dot(u,vh)\n",
    "\t\t\td = np.sum(s)\n",
    "\t\t\tif d_old!=0 and d/d_old < 1 + tol:\n",
    "\t\t\t\tbreak\n",
    "\t\treturn np.dot(Phi, R)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62582a64",
   "metadata": {},
   "source": [
    "#### Calculating SoVI\n",
    "At this stage, we seek to calculate the z-score standardized SoVI and variable weightings for each dataset. Below is our workflow for calculating SoVI, adapted from the workflow in our reproduction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7aceb54",
   "metadata": {},
   "source": [
    "![PCA Workflow](../../results/figures/RPl_SoVI.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a084cfe6",
   "metadata": {},
   "source": [
    "#### Internal consistency analysis\n",
    "\n",
    "We will develop a simple linear regression model for each county in the analysis, where the independent variable is time and the dependent variable is the county's z-score standardized SoVI score.\n",
    "To do this, we will need to manipulate our output data to generate a dataset with a row for each year, a column documenting the year, and a column for each county's z-score standardized SoVI scores.\n",
    "We will use the statsmodels package and its [regression.linear_model.OLS¶](https://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.OLS.html) class to conduct the regression analysis and generate standard errors.\n",
    "The standard error of our year variable's regression coefficient will illustrate how consistent SoVI scores are in a particular location when controlling for time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96947d34",
   "metadata": {},
   "source": [
    "#### Theoretical consistency analysis\n",
    "\n",
    "Similar to Spielman et al.'s analysis, we will sum together all of the components for each model in order to determine the net effect of each variable on the final SoVI score.\n",
    "We will summarize this information in a dataframe with a row for each variable and a column for each model.\n",
    "For each model, we will then rank the variables with positive coefficients and negative coefficients separately, by magnitude."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c9f67b",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "### RPl-H1\n",
    "\n",
    "We plan to present our results of our internal consistency analysis by producing a country-wide map of standard error coefficients.\n",
    "We will also calculate summary statistics of standard error coefficients, including minimum, mean, and maximum.\n",
    "\n",
    "### RPl-H2\n",
    "\n",
    "Will will present our theoretical consistency results by producing a rank chart displaying the rank of each variable over time.\n",
    "We will place the ranks of positive coefficients above the X-axis and the ranks of negative coefficients below the X-axis.\n",
    "In this manner, the number of variables on either side of the X-axis may vary between models, but we will present information regarding both the magnitude and sign of variables in one graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f0b306",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "### RPl-H1\n",
    "\n",
    "Standard errors in simple linear regression represent the average distance between the actual data and the line of best fit.\n",
    "The smaller the standard errors, the better the SoVI scores fit a linear trend over time.\n",
    "Since standard errors use the units of the response variable and the response variable is z-score normalized, the standard errors can be interpreted as the the average deviance in SoVI scores from a linear trend over time, where the unit of measurement is one standard deviation of the SoVI score.\n",
    "In this manner, our standard errors will illustrate the degree to which SoVI scores are consistent when controlling for time.\n",
    "We would expect standard errors within one standard deviation if the SoVI model is internally consistent over time, especially when we consider the temporal dependency in the data. \n",
    "\n",
    "### RPl-H2\n",
    "\n",
    "If SoVI is theoretically consistent, we would anticipate variable rankings and signs to be fairly consistent over time.\n",
    "On our figure, this would be illustrated with few lines crossing each other, few lines crossing the zero line, and narrow ranges of each variable's rankings.\n",
    "The more that variables switch signs, and the greater the range of each variable's ranks, the less theoretically consistent SoVI is over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a25da3b",
   "metadata": {},
   "source": [
    "## Integrity Statement\n",
    "\n",
    "The authors of this preregistration state that they completed this preregistration to the best of their knowledge and that no other preregistration exists pertaining to the same hypotheses and research."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9a39ef",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "\n",
    "- `Funding Name`: NSF Directorate for Social, Behavioral and Economic Sciences\n",
    "- `Funding Title`: Transforming theory-building and STEM education through reproductions and replications in the geographical sciences\n",
    "- `Award info URI`: https://www.nsf.gov/awardsearch/showAward?AWD_ID=2049837\n",
    "- `Award number`: BCS-2049837\n",
    "\n",
    "This report is based upon the template for Reproducible and Replicable Research in Human-Environment and Geographical Sciences, DOI:[10.17605/OSF.IO/W29MQ](https://doi.org/10.17605/OSF.IO/W29MQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f96fc2d",
   "metadata": {},
   "source": [
    "## References \n",
    "- Cutter, S. L., Boruff, B. J., & Shirley, W. L. (2003). Social Vulnerability to Environmental Hazards. Social Science Quarterly, 84(2), 242–261. https://doi.org/10.1111/1540-6237.8402002\n",
    "- Spielman, S. E., Tuccillo, J., Folch, D. C., Schweikert, A., Davies, R., Wood, N., & Tate, E. (2020). Evaluating Social Vulnerability Indicators: Criteria and their Application to the Social Vulnerability Index. Natural Hazards, 100(1), 417–436. https://doi.org/10.1007/s11069-019-03820-z\n",
    "- Tate, E. (2012). Social vulnerability indices: A comparative assessment using uncertainty and sensitivity analysis. Natural Hazards, 63(2), 325–347. https://doi.org/10.1007/s11069-012-0152-2\n",
    "- United States Census Bureau. (2021, October 8). Substantial Changes to Counties and County Equivalent Entities: 1970-Present. https://www.census.gov/programs-surveys/geography/technical-documentation/county-changes.2010.html#list-tab-957819518"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:RPl-Spielman-2020]",
   "language": "python",
   "name": "conda-env-RPl-Spielman-2020-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
